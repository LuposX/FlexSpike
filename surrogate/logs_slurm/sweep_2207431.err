/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
wandb: Agent Starting Run: lgdn0ijx with config:
wandb: 	alpha: 0.85
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.0002979192085015855
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 2
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 0.9
wandb: Currently logged in as: lupos to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run lgdn0ijx
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_003646-lgdn0ijx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-1
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/lgdn0ijx
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 248 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
248 K     Trainable params
0         Non-trainable params
248 K     Total params
0.995     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–â–â–„â–ˆ
wandb:  grad_mean/lif_layers.1 â–â–â–…â–ˆ
wandb: grad_mean/lif_layers.10 â–â–…â–ˆâ–ˆ
wandb: grad_mean/lif_layers.11 â–ˆâ–‚â–‚â–
wandb: grad_mean/lif_layers.12 â–ˆâ–ƒâ–‚â–
wandb: grad_mean/lif_layers.13 â–ˆâ–ƒâ–â–
wandb: grad_mean/lif_layers.14 â–ˆâ–ƒâ–â–
wandb: grad_mean/lif_layers.15 â–ˆâ–‚â–â–
wandb:  grad_mean/lif_layers.2 â–â–‚â–…â–ˆ
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 137545.09375
wandb:  grad_mean/lif_layers.1 12483.04688
wandb: grad_mean/lif_layers.10 0.00497
wandb: grad_mean/lif_layers.11 0.00197
wandb: grad_mean/lif_layers.12 0.001
wandb: grad_mean/lif_layers.13 0.00059
wandb: grad_mean/lif_layers.14 0.00038
wandb: grad_mean/lif_layers.15 0.00042
wandb:  grad_mean/lif_layers.2 1629.41187
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run distinctive-sweep-1 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/lgdn0ijx
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_003646-lgdn0ijx/logs
wandb: Agent Starting Run: i0o60n1u with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.0001367637496110311
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 1
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0.1
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_030713-i0o60n1u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-2
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/i0o60n1u
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 62.9 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
63.0 K    Trainable params
0         Non-trainable params
63.0 K    Total params
0.252     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–â–‚â–…â–ˆ
wandb:  grad_mean/lif_layers.1 â–â–‚â–…â–ˆ
wandb: grad_mean/lif_layers.10 â–ˆâ–ƒâ–â–
wandb: grad_mean/lif_layers.11 â–ˆâ–„â–‚â–
wandb: grad_mean/lif_layers.12 â–ˆâ–„â–‚â–
wandb: grad_mean/lif_layers.13 â–ˆâ–„â–‚â–
wandb: grad_mean/lif_layers.14 â–ˆâ–„â–â–
wandb: grad_mean/lif_layers.15 â–ˆâ–„â–â–
wandb:  grad_mean/lif_layers.2 â–â–‚â–…â–ˆ
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.10563
wandb:  grad_mean/lif_layers.1 0.02711
wandb: grad_mean/lif_layers.10 0.00201
wandb: grad_mean/lif_layers.11 0.00217
wandb: grad_mean/lif_layers.12 0.0025
wandb: grad_mean/lif_layers.13 0.00295
wandb: grad_mean/lif_layers.14 0.00293
wandb: grad_mean/lif_layers.15 0.00336
wandb:  grad_mean/lif_layers.2 0.00945
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run autumn-sweep-2 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/i0o60n1u
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_030713-i0o60n1u/logs
wandb: Agent Starting Run: 56ep8ia2 with config:
wandb: 	alpha: 0.85
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 8.698798223078832e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0.1
wandb: 	zhyp_s: 0.9
wandb: setting up run 56ep8ia2
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_040026-56ep8ia2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scarlet-sweep-3
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/56ep8ia2
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 29.6 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
29.7 K    Trainable params
0         Non-trainable params
29.7 K    Total params
0.119     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–â–ƒâ–†â–ˆ
wandb: grad_mean/lif_layers.1 â–â–ƒâ–†â–ˆ
wandb: grad_mean/lif_layers.2 â–â–ƒâ–†â–ˆ
wandb: grad_mean/lif_layers.3 â–â–‚â–…â–ˆ
wandb: grad_mean/lif_layers.4 â–â–‚â–„â–ˆ
wandb: grad_mean/lif_layers.5 â–â–ƒâ–ƒâ–ˆ
wandb: grad_mean/lif_layers.6 â–â–ƒâ–ƒâ–ˆ
wandb: grad_mean/lif_layers.7 â–â–‚â–ƒâ–ˆ
wandb: grad_mean/output_layer â–â–‚â–ƒâ–ˆ
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.0283
wandb: grad_mean/lif_layers.1 0.00687
wandb: grad_mean/lif_layers.2 0.00326
wandb: grad_mean/lif_layers.3 0.00238
wandb: grad_mean/lif_layers.4 0.00227
wandb: grad_mean/lif_layers.5 0.00241
wandb: grad_mean/lif_layers.6 0.00256
wandb: grad_mean/lif_layers.7 0.00297
wandb: grad_mean/output_layer 0.2867
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run scarlet-sweep-3 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/56ep8ia2
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_040026-56ep8ia2/logs
wandb: Agent Starting Run: pdiawe80 with config:
wandb: 	alpha: 0.85
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 9.952422544440756e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.8
wandb: setting up run pdiawe80
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_042606-pdiawe80
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-4
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/pdiawe80
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 17.5 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
17.7 K    Trainable params
0         Non-trainable params
17.7 K    Total params
0.071     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:   grad_mean/lif_layers.0 â–…â–ˆâ–â–‡
wandb:   grad_mean/lif_layers.1 â–ˆâ–†â–â–‚
wandb:   grad_mean/output_layer â–ˆâ–‡â–â–‚
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–†â–†â–†â–‡â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–ˆâ–ˆâ–‡â–†â–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.00702
wandb:   grad_mean/lif_layers.1 0.003
wandb:   grad_mean/output_layer 0.35157
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01902
wandb: spikes/train_avg_layer_1 0.00109
wandb:               train_loss 0.28654
wandb:      trainer/global_step 279
wandb:                 val_loss 0.28398
wandb: 
wandb: ğŸš€ View run iconic-sweep-4 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/pdiawe80
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_042606-pdiawe80/logs
wandb: Agent Starting Run: od0wjk7z with config:
wandb: 	alpha: 0.85
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.00015522723737498815
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_043904-od0wjk7z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-5
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/od0wjk7z
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 4.7 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
4.8 K     Trainable params
0         Non-trainable params
4.8 K     Total params
0.019     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 80-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–†â–ˆâ–ˆ
wandb:   grad_mean/lif_layers.1 â–ˆâ–‡â–„â–
wandb:   grad_mean/output_layer â–ˆâ–„â–‚â–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               train_loss â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.03279
wandb:   grad_mean/lif_layers.1 0.00499
wandb:   grad_mean/output_layer 0.34409
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.0257
wandb: spikes/train_avg_layer_1 0.00826
wandb:               train_loss 0.27599
wandb:      trainer/global_step 279
wandb:                 val_loss 0.27359
wandb: 
wandb: ğŸš€ View run prime-sweep-5 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/od0wjk7z
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_043904-od0wjk7z/logs
wandb: Agent Starting Run: kvivgr5w with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 6.121311067384071e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 2
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_044418-kvivgr5w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-6
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/kvivgr5w
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 4.2 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
4.2 K     Trainable params
0         Non-trainable params
4.2 K     Total params
0.017     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–…â–‚â–
wandb:  grad_mean/lif_layers.1 â–â–ˆâ–‡â–
wandb: grad_mean/lif_layers.10 â–â–ˆâ–†â–ƒ
wandb: grad_mean/lif_layers.11 â–â–ˆâ–…â–ƒ
wandb: grad_mean/lif_layers.12 â–â–ˆâ–…â–ƒ
wandb: grad_mean/lif_layers.13 â–â–ˆâ–…â–ƒ
wandb: grad_mean/lif_layers.14 â–â–ˆâ–…â–ƒ
wandb: grad_mean/lif_layers.15 â–â–ˆâ–†â–ƒ
wandb:  grad_mean/lif_layers.2 â–â–ˆâ–‡â–
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.00015
wandb:  grad_mean/lif_layers.1 7e-05
wandb: grad_mean/lif_layers.10 0.0018
wandb: grad_mean/lif_layers.11 0.00216
wandb: grad_mean/lif_layers.12 0.00229
wandb: grad_mean/lif_layers.13 0.00242
wandb: grad_mean/lif_layers.14 0.0024
wandb: grad_mean/lif_layers.15 0.00332
wandb:  grad_mean/lif_layers.2 0.0001
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run honest-sweep-6 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/kvivgr5w
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_044418-kvivgr5w/logs
wandb: Agent Starting Run: 6vd76ab4 with config:
wandb: 	alpha: 0.95
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 3.481974395676387e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_045528-6vd76ab4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-sweep-7
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/6vd76ab4
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 92     | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
111       Trainable params
0         Non-trainable params
111       Total params
0.000     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–‡â–ˆâ–‚â–
wandb:   grad_mean/lif_layers.1 â–ˆâ–…â–ƒâ–
wandb:   grad_mean/lif_layers.2 â–ˆâ–„â–ƒâ–
wandb:   grad_mean/lif_layers.3 â–ˆâ–…â–ƒâ–
wandb:   grad_mean/output_layer â–ˆâ–…â–ƒâ–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–ƒâ–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–ƒâ–‚â–‚â–â–†â–ˆâ–ˆâ–‡â–‡â–†â–…â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–ƒâ–â–â–â–â–
wandb: spikes/train_avg_layer_2 â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–ƒâ–†â–†â–†â–†â–†
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.00362
wandb:   grad_mean/lif_layers.1 0.00222
wandb:   grad_mean/lif_layers.2 0.00832
wandb:   grad_mean/lif_layers.3 0.00857
wandb:   grad_mean/output_layer 0.26098
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.02028
wandb: spikes/train_avg_layer_1 0.00666
wandb: spikes/train_avg_layer_2 0.00026
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run helpful-sweep-7 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/6vd76ab4
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_045528-6vd76ab4/logs
wandb: Agent Starting Run: dqvigx8m with config:
wandb: 	alpha: 0.85
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 4.532115415544528e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 1
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0.1
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_045655-dqvigx8m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-8
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/dqvigx8m
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 172    | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
191       Trainable params
0         Non-trainable params
191       Total params
0.001     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 80-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–â–ˆâ–ƒâ–†
wandb: grad_mean/lif_layers.1 â–â–ˆâ–„â–ˆ
wandb: grad_mean/lif_layers.2 â–â–ˆâ–„â–‡
wandb: grad_mean/lif_layers.3 â–â–ˆâ–ƒâ–‡
wandb: grad_mean/lif_layers.4 â–â–ˆâ–ƒâ–†
wandb: grad_mean/lif_layers.5 â–â–ˆâ–‚â–†
wandb: grad_mean/lif_layers.6 â–â–ˆâ–‚â–ˆ
wandb: grad_mean/lif_layers.7 â–â–‡â–â–ˆ
wandb: grad_mean/output_layer â–â–ˆâ–‚â–†
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.00227
wandb: grad_mean/lif_layers.1 0.00106
wandb: grad_mean/lif_layers.2 0.0005
wandb: grad_mean/lif_layers.3 0.00102
wandb: grad_mean/lif_layers.4 0.00233
wandb: grad_mean/lif_layers.5 0.0018
wandb: grad_mean/lif_layers.6 0.00232
wandb: grad_mean/lif_layers.7 0.00407
wandb: grad_mean/output_layer 0.11552
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run chocolate-sweep-8 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/dqvigx8m
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_045655-dqvigx8m/logs
wandb: Agent Starting Run: v61nf5fc with config:
wandb: 	alpha: 0.85
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 2.733173186088086e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_045920-v61nf5fc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-9
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/v61nf5fc
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 13.0 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
13.1 K    Trainable params
0         Non-trainable params
13.1 K    Total params
0.052     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–„â–ˆâ–„â–
wandb:   grad_mean/lif_layers.1 â–„â–ˆâ–ƒâ–
wandb:   grad_mean/lif_layers.2 â–„â–ˆâ–ƒâ–
wandb:   grad_mean/lif_layers.3 â–…â–ˆâ–ƒâ–
wandb:   grad_mean/output_layer â–„â–ˆâ–ƒâ–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–ƒâ–„â–„â–…â–…â–„â–ƒâ–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–…â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–â–‚â–ƒâ–„â–„â–„â–…â–„â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_2 â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–†
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.00657
wandb:   grad_mean/lif_layers.1 0.00271
wandb:   grad_mean/lif_layers.2 0.00309
wandb:   grad_mean/lif_layers.3 0.00381
wandb:   grad_mean/output_layer 0.37265
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01866
wandb: spikes/train_avg_layer_1 0.00143
wandb: spikes/train_avg_layer_2 8e-05
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run astral-sweep-9 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/v61nf5fc
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_045920-v61nf5fc/logs
wandb: Agent Starting Run: bpdtt4xs with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 1.0500009744335397e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: True
wandb: 	rho: 1
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.8
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_051126-bpdtt4xs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-10
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/bpdtt4xs
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 4.7 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
4.8 K     Trainable params
0         Non-trainable params
4.8 K     Total params
0.019     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–ˆâ–…â–â–
wandb:   grad_mean/lif_layers.1 â–ˆâ–„â–â–
wandb:   grad_mean/output_layer â–ˆâ–…â–â–‚
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–…â–ˆâ–‡â–†â–ˆâ–‡â–ˆâ–ˆâ–‡â–†â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„
wandb: spikes/train_avg_layer_1 â–ˆâ–ˆâ–ˆâ–‡â–†â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.00549
wandb:   grad_mean/lif_layers.1 0.00316
wandb:   grad_mean/output_layer 0.25502
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01842
wandb: spikes/train_avg_layer_1 0.00192
wandb:               train_loss 0.22981
wandb:      trainer/global_step 279
wandb:                 val_loss 0.22765
wandb: 
wandb: ğŸš€ View run dry-sweep-10 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/bpdtt4xs
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_051126-bpdtt4xs/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: hgbv5hs3 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 3.909290966367851e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_051700-hgbv5hs3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-11
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/hgbv5hs3
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 29.6 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
29.7 K    Trainable params
0         Non-trainable params
29.7 K    Total params
0.119     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–â–ˆâ–…â–„
wandb: grad_mean/lif_layers.1 â–â–ˆâ–‚â–
wandb: grad_mean/lif_layers.2 â–â–ˆâ–â–
wandb: grad_mean/lif_layers.3 â–‚â–ˆâ–â–
wandb: grad_mean/lif_layers.4 â–â–ˆâ–â–
wandb: grad_mean/lif_layers.5 â–â–ˆâ–â–
wandb: grad_mean/lif_layers.6 â–â–ˆâ–â–
wandb: grad_mean/lif_layers.7 â–â–ˆâ–â–‚
wandb: grad_mean/output_layer â–‚â–ˆâ–â–ƒ
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.00188
wandb: grad_mean/lif_layers.1 0.00093
wandb: grad_mean/lif_layers.2 0.00081
wandb: grad_mean/lif_layers.3 0.00114
wandb: grad_mean/lif_layers.4 0.0013
wandb: grad_mean/lif_layers.5 0.0017
wandb: grad_mean/lif_layers.6 0.00162
wandb: grad_mean/lif_layers.7 0.00211
wandb: grad_mean/output_layer 0.21984
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run cerulean-sweep-11 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/hgbv5hs3
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_051700-hgbv5hs3/logs
wandb: Agent Starting Run: 5ywpc8o2 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 2.607063318712745e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: True
wandb: 	rho: 1
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0.1
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_054251-5ywpc8o2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-12
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/5ywpc8o2
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 172    | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
191       Trainable params
0         Non-trainable params
191       Total params
0.001     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–ƒâ–â–‡â–ˆ
wandb: grad_mean/lif_layers.1 â–„â–â–‡â–ˆ
wandb: grad_mean/lif_layers.2 â–„â–â–†â–ˆ
wandb: grad_mean/lif_layers.3 â–„â–â–†â–ˆ
wandb: grad_mean/lif_layers.4 â–ƒâ–â–†â–ˆ
wandb: grad_mean/lif_layers.5 â–ƒâ–â–†â–ˆ
wandb: grad_mean/lif_layers.6 â–ƒâ–â–†â–ˆ
wandb: grad_mean/lif_layers.7 â–„â–â–†â–ˆ
wandb: grad_mean/output_layer â–ƒâ–â–†â–ˆ
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.00098
wandb: grad_mean/lif_layers.1 0.00138
wandb: grad_mean/lif_layers.2 0.00132
wandb: grad_mean/lif_layers.3 0.00204
wandb: grad_mean/lif_layers.4 0.00249
wandb: grad_mean/lif_layers.5 0.00407
wandb: grad_mean/lif_layers.6 0.00322
wandb: grad_mean/lif_layers.7 0.00388
wandb: grad_mean/output_layer 0.12011
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run different-sweep-12 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/5ywpc8o2
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_054251-5ywpc8o2/logs
wandb: Agent Starting Run: 0vgqqwpd with config:
wandb: 	alpha: 0.95
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.00013351289972318643
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: True
wandb: 	rho: 2
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_054521-0vgqqwpd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-13
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/0vgqqwpd
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 2.0 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
2.1 K     Trainable params
0         Non-trainable params
2.1 K     Total params
0.008     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–â–„â–†â–ˆ
wandb: grad_mean/lif_layers.1 â–â–…â–‡â–ˆ
wandb: grad_mean/lif_layers.2 â–â–…â–‡â–ˆ
wandb: grad_mean/lif_layers.3 â–â–†â–†â–ˆ
wandb: grad_mean/lif_layers.4 â–ˆâ–„â–â–„
wandb: grad_mean/lif_layers.5 â–ˆâ–…â–â–ƒ
wandb: grad_mean/lif_layers.6 â–ˆâ–…â–â–‚
wandb: grad_mean/lif_layers.7 â–ˆâ–†â–â–‚
wandb: grad_mean/output_layer â–ˆâ–†â–â–‚
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.01652
wandb: grad_mean/lif_layers.1 0.00561
wandb: grad_mean/lif_layers.2 0.00508
wandb: grad_mean/lif_layers.3 0.00441
wandb: grad_mean/lif_layers.4 0.00588
wandb: grad_mean/lif_layers.5 0.00573
wandb: grad_mean/lif_layers.6 0.00562
wandb: grad_mean/lif_layers.7 0.00616
wandb: grad_mean/output_layer 0.34303
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run fiery-sweep-13 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/0vgqqwpd
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_054521-0vgqqwpd/logs
wandb: Agent Starting Run: eea4z2ta with config:
wandb: 	alpha: 0.95
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 6.546741528874915e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_055051-eea4z2ta
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-14
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/eea4z2ta
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 116 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
116 K     Trainable params
0         Non-trainable params
116 K     Total params
0.467     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–â–„â–‡â–ˆ
wandb: grad_mean/lif_layers.1 â–â–„â–‡â–ˆ
wandb: grad_mean/lif_layers.2 â–â–…â–‡â–ˆ
wandb: grad_mean/lif_layers.3 â–â–…â–ˆâ–ˆ
wandb: grad_mean/lif_layers.4 â–â–†â–ˆâ–ˆ
wandb: grad_mean/lif_layers.5 â–â–†â–ˆâ–ˆ
wandb: grad_mean/lif_layers.6 â–â–ˆâ–ˆâ–†
wandb: grad_mean/lif_layers.7 â–ˆâ–ˆâ–ƒâ–
wandb: grad_mean/output_layer â–ˆâ–†â–â–
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.14016
wandb: grad_mean/lif_layers.1 0.02719
wandb: grad_mean/lif_layers.2 0.00816
wandb: grad_mean/lif_layers.3 0.00367
wandb: grad_mean/lif_layers.4 0.00229
wandb: grad_mean/lif_layers.5 0.00163
wandb: grad_mean/lif_layers.6 0.00132
wandb: grad_mean/lif_layers.7 0.00143
wandb: grad_mean/output_layer 0.22302
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run sleek-sweep-14 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/eea4z2ta
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_055051-eea4z2ta/logs
wandb: Agent Starting Run: 0mzq5ne3 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 7.03201536139886e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 1
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.8
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_070220-0mzq5ne3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sweep-15
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/0mzq5ne3
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 13.0 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
13.1 K    Trainable params
0         Non-trainable params
13.1 K    Total params
0.052     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:   grad_mean/lif_layers.0 â–â–…â–‡â–ˆ
wandb:   grad_mean/lif_layers.1 â–â–…â–†â–ˆ
wandb:   grad_mean/lif_layers.2 â–â–„â–„â–ˆ
wandb:   grad_mean/lif_layers.3 â–ˆâ–†â–â–„
wandb:   grad_mean/output_layer â–ˆâ–†â–â–ƒ
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_2 â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.01054
wandb:   grad_mean/lif_layers.1 0.00333
wandb:   grad_mean/lif_layers.2 0.00156
wandb:   grad_mean/lif_layers.3 0.00143
wandb:   grad_mean/output_layer 0.14659
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.02058
wandb: spikes/train_avg_layer_1 0.00247
wandb: spikes/train_avg_layer_2 0.0008
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run neat-sweep-15 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/0mzq5ne3
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_070220-0mzq5ne3/logs
wandb: Agent Starting Run: 3qw36ocb with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 1.2363137481542587e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 1
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_071431-3qw36ocb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-16
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/3qw36ocb
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 13.0 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
13.1 K    Trainable params
0         Non-trainable params
13.1 K    Total params
0.052     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–ƒâ–â–ˆâ–ƒ
wandb:   grad_mean/lif_layers.1 â–†â–â–ˆâ–‡
wandb:   grad_mean/lif_layers.2 â–ˆâ–â–‡â–†
wandb:   grad_mean/lif_layers.3 â–ˆâ–â–‡â–…
wandb:   grad_mean/output_layer â–ˆâ–â–ˆâ–…
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–ˆâ–‡â–‡â–‡â–…â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–…â–…â–„â–„â–„â–„â–‚â–‚â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–†â–‡â–†â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_2 â–â–â–â–â–â–â–â–ƒâ–†â–‚â–â–â–â–â–â–â–â–â–„â–…â–ƒâ–â–â–ˆâ–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.00391
wandb:   grad_mean/lif_layers.1 0.00176
wandb:   grad_mean/lif_layers.2 0.00157
wandb:   grad_mean/lif_layers.3 0.00197
wandb:   grad_mean/output_layer 0.20586
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01869
wandb: spikes/train_avg_layer_1 0.00078
wandb: spikes/train_avg_layer_2 0
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run twilight-sweep-16 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/3qw36ocb
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_071431-3qw36ocb/logs
wandb: Agent Starting Run: 9yjwjze2 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 2.2170503515523408e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: False
wandb: 	rho: 1
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0.1
wandb: 	zhyp_s: 0.8
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_072637-9yjwjze2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-17
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/9yjwjze2
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 248 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
248 K     Trainable params
0         Non-trainable params
248 K     Total params
0.995     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–…â–â–‚
wandb:  grad_mean/lif_layers.1 â–‡â–†â–â–ˆ
wandb: grad_mean/lif_layers.10 â–†â–†â–â–ˆ
wandb: grad_mean/lif_layers.11 â–†â–†â–â–ˆ
wandb: grad_mean/lif_layers.12 â–†â–†â–â–ˆ
wandb: grad_mean/lif_layers.13 â–†â–†â–â–ˆ
wandb: grad_mean/lif_layers.14 â–†â–†â–â–ˆ
wandb: grad_mean/lif_layers.15 â–†â–†â–â–ˆ
wandb:  grad_mean/lif_layers.2 â–ˆâ–…â–â–ˆ
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.00016
wandb:  grad_mean/lif_layers.1 6e-05
wandb: grad_mean/lif_layers.10 0.00121
wandb: grad_mean/lif_layers.11 0.00142
wandb: grad_mean/lif_layers.12 0.00166
wandb: grad_mean/lif_layers.13 0.00213
wandb: grad_mean/lif_layers.14 0.00215
wandb: grad_mean/lif_layers.15 0.00228
wandb:  grad_mean/lif_layers.2 8e-05
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run smooth-sweep-17 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/9yjwjze2
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_072637-9yjwjze2/logs
wandb: Agent Starting Run: rzblq03z with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 3.230259281477831e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0.1
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_095752-rzblq03z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-18
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/rzblq03z
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 332    | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
351       Trainable params
0         Non-trainable params
351       Total params
0.001     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 80-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:  grad_mean/lif_layers.0 â–†â–â–ƒâ–ˆ
wandb:  grad_mean/lif_layers.1 â–ˆâ–â–‚â–†
wandb: grad_mean/lif_layers.10 â–ˆâ–‚â–ƒâ–
wandb: grad_mean/lif_layers.11 â–ˆâ–‚â–ƒâ–
wandb: grad_mean/lif_layers.12 â–ˆâ–‚â–ƒâ–
wandb: grad_mean/lif_layers.13 â–ˆâ–â–ƒâ–
wandb: grad_mean/lif_layers.14 â–ˆâ–‚â–ƒâ–
wandb: grad_mean/lif_layers.15 â–ˆâ–ƒâ–„â–
wandb:  grad_mean/lif_layers.2 â–ˆâ–â–ƒâ–…
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.00021
wandb:  grad_mean/lif_layers.1 0.00017
wandb: grad_mean/lif_layers.10 0.00164
wandb: grad_mean/lif_layers.11 0.00172
wandb: grad_mean/lif_layers.12 0.0042
wandb: grad_mean/lif_layers.13 0.00576
wandb: grad_mean/lif_layers.14 0.01059
wandb: grad_mean/lif_layers.15 0.01341
wandb:  grad_mean/lif_layers.2 0.00017
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run snowy-sweep-18 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/rzblq03z
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_095752-rzblq03z/logs
wandb: Agent Starting Run: dzif88f3 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 1.6490101693597717e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_100241-dzif88f3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-19
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/dzif88f3
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 2.0 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
2.1 K     Trainable params
0         Non-trainable params
2.1 K     Total params
0.008     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–ˆâ–â–‚â–ƒ
wandb: grad_mean/lif_layers.1 â–ˆâ–„â–ƒâ–
wandb: grad_mean/lif_layers.2 â–ˆâ–„â–…â–
wandb: grad_mean/lif_layers.3 â–ˆâ–…â–…â–
wandb: grad_mean/lif_layers.4 â–ˆâ–„â–…â–
wandb: grad_mean/lif_layers.5 â–ˆâ–„â–…â–
wandb: grad_mean/lif_layers.6 â–ˆâ–„â–…â–
wandb: grad_mean/lif_layers.7 â–ˆâ–„â–…â–
wandb: grad_mean/output_layer â–ˆâ–„â–…â–
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.00331
wandb: grad_mean/lif_layers.1 0.00159
wandb: grad_mean/lif_layers.2 0.00174
wandb: grad_mean/lif_layers.3 0.00281
wandb: grad_mean/lif_layers.4 0.00376
wandb: grad_mean/lif_layers.5 0.00354
wandb: grad_mean/lif_layers.6 0.00447
wandb: grad_mean/lif_layers.7 0.00728
wandb: grad_mean/output_layer 0.45443
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run ruby-sweep-19 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/dzif88f3
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_100241-dzif88f3/logs
wandb: Agent Starting Run: qnkjb61p with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.00021879085012128585
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 1
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.8
wandb: setting up run qnkjb61p
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_100821-qnkjb61p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-20
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/qnkjb61p
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 248 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
248 K     Trainable params
0         Non-trainable params
248 K     Total params
0.995     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–â–â–„â–ˆ
wandb:  grad_mean/lif_layers.1 â–â–â–„â–ˆ
wandb: grad_mean/lif_layers.10 â–â–…â–‡â–ˆ
wandb: grad_mean/lif_layers.11 â–â–ˆâ–‡â–ˆ
wandb: grad_mean/lif_layers.12 â–â–ˆâ–…â–„
wandb: grad_mean/lif_layers.13 â–‚â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.14 â–†â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.15 â–ˆâ–†â–‚â–
wandb:  grad_mean/lif_layers.2 â–â–â–„â–ˆ
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 6902.13574
wandb:  grad_mean/lif_layers.1 915.89532
wandb: grad_mean/lif_layers.10 0.01068
wandb: grad_mean/lif_layers.11 0.00477
wandb: grad_mean/lif_layers.12 0.00251
wandb: grad_mean/lif_layers.13 0.00152
wandb: grad_mean/lif_layers.14 0.00113
wandb: grad_mean/lif_layers.15 0.00113
wandb:  grad_mean/lif_layers.2 177.29947
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run laced-sweep-20 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/qnkjb61p
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_100821-qnkjb61p/logs
wandb: Agent Starting Run: uk93i1he with config:
wandb: 	alpha: 0.85
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 1.722814099778019e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.8
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_124001-uk93i1he
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-sweep-21
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/uk93i1he
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 332    | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
351       Trainable params
0         Non-trainable params
351       Total params
0.001     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–â–‡â–ˆâ–‚
wandb:  grad_mean/lif_layers.1 â–â–ˆâ–…â–
wandb: grad_mean/lif_layers.10 â–ƒâ–ˆâ–„â–
wandb: grad_mean/lif_layers.11 â–ƒâ–ˆâ–„â–
wandb: grad_mean/lif_layers.12 â–ƒâ–ˆâ–„â–
wandb: grad_mean/lif_layers.13 â–‚â–ˆâ–„â–
wandb: grad_mean/lif_layers.14 â–‚â–ˆâ–„â–
wandb: grad_mean/lif_layers.15 â–‚â–ˆâ–„â–
wandb:  grad_mean/lif_layers.2 â–â–ˆâ–„â–
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.00019
wandb:  grad_mean/lif_layers.1 3e-05
wandb: grad_mean/lif_layers.10 0.00382
wandb: grad_mean/lif_layers.11 0.00752
wandb: grad_mean/lif_layers.12 0.01266
wandb: grad_mean/lif_layers.13 0.01479
wandb: grad_mean/lif_layers.14 0.01963
wandb: grad_mean/lif_layers.15 0.0151
wandb:  grad_mean/lif_layers.2 7e-05
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run pious-sweep-21 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/uk93i1he
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_124001-uk93i1he/logs
wandb: Agent Starting Run: p97kmga8 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.00034211534999223966
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: True
wandb: 	rho: 1
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0.1
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_124450-p97kmga8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-22
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/p97kmga8
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 62.9 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
63.0 K    Trainable params
0         Non-trainable params
63.0 K    Total params
0.252     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:  grad_mean/lif_layers.0 â–â–â–„â–ˆ
wandb:  grad_mean/lif_layers.1 â–â–â–„â–ˆ
wandb: grad_mean/lif_layers.10 â–â–†â–ˆâ–‡
wandb: grad_mean/lif_layers.11 â–â–†â–ˆâ–‡
wandb: grad_mean/lif_layers.12 â–â–‡â–ˆâ–ˆ
wandb: grad_mean/lif_layers.13 â–â–‡â–‡â–ˆ
wandb: grad_mean/lif_layers.14 â–â–ˆâ–…â–„
wandb: grad_mean/lif_layers.15 â–ˆâ–ˆâ–‚â–
wandb:  grad_mean/lif_layers.2 â–â–â–„â–ˆ
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 970.12122
wandb:  grad_mean/lif_layers.1 128.74013
wandb: grad_mean/lif_layers.10 0.00857
wandb: grad_mean/lif_layers.11 0.00554
wandb: grad_mean/lif_layers.12 0.00381
wandb: grad_mean/lif_layers.13 0.00275
wandb: grad_mean/lif_layers.14 0.0021
wandb: grad_mean/lif_layers.15 0.00164
wandb:  grad_mean/lif_layers.2 28.84802
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run misunderstood-sweep-22 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/p97kmga8
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_124450-p97kmga8/logs
wandb: Agent Starting Run: b0undei6 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 3.056502157566975e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 1
wandb: setting up run b0undei6
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_133827-b0undei6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-23
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/b0undei6
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 2.0 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
2.1 K     Trainable params
0         Non-trainable params
2.1 K     Total params
0.008     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–†â–…â–â–ˆ
wandb: grad_mean/lif_layers.1 â–‡â–…â–â–ˆ
wandb: grad_mean/lif_layers.2 â–ˆâ–…â–â–ˆ
wandb: grad_mean/lif_layers.3 â–ˆâ–…â–â–‡
wandb: grad_mean/lif_layers.4 â–ˆâ–…â–â–‡
wandb: grad_mean/lif_layers.5 â–ˆâ–…â–â–‡
wandb: grad_mean/lif_layers.6 â–ˆâ–…â–â–‡
wandb: grad_mean/lif_layers.7 â–ˆâ–…â–â–‡
wandb: grad_mean/output_layer â–ˆâ–…â–â–†
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.00284
wandb: grad_mean/lif_layers.1 0.00169
wandb: grad_mean/lif_layers.2 0.00192
wandb: grad_mean/lif_layers.3 0.00175
wandb: grad_mean/lif_layers.4 0.00181
wandb: grad_mean/lif_layers.5 0.00201
wandb: grad_mean/lif_layers.6 0.00238
wandb: grad_mean/lif_layers.7 0.00309
wandb: grad_mean/output_layer 0.22436
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run silvery-sweep-23 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/b0undei6
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_133827-b0undei6/logs
wandb: Agent Starting Run: miacldek with config:
wandb: 	alpha: 0.95
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 2.621873265943679e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_134418-miacldek
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-24
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/miacldek
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 4.2 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
4.2 K     Trainable params
0         Non-trainable params
4.2 K     Total params
0.017     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–‚â–â–ˆ
wandb:  grad_mean/lif_layers.1 â–ˆâ–â–‚â–ƒ
wandb: grad_mean/lif_layers.10 â–ˆâ–„â–â–‚
wandb: grad_mean/lif_layers.11 â–ˆâ–„â–â–ƒ
wandb: grad_mean/lif_layers.12 â–ˆâ–…â–â–ƒ
wandb: grad_mean/lif_layers.13 â–ˆâ–…â–â–ƒ
wandb: grad_mean/lif_layers.14 â–ˆâ–…â–â–ƒ
wandb: grad_mean/lif_layers.15 â–ˆâ–…â–â–ƒ
wandb:  grad_mean/lif_layers.2 â–ˆâ–â–â–
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.00011
wandb:  grad_mean/lif_layers.1 5e-05
wandb: grad_mean/lif_layers.10 0.00061
wandb: grad_mean/lif_layers.11 0.00084
wandb: grad_mean/lif_layers.12 0.00082
wandb: grad_mean/lif_layers.13 0.00085
wandb: grad_mean/lif_layers.14 0.00087
wandb: grad_mean/lif_layers.15 0.00101
wandb:  grad_mean/lif_layers.2 5e-05
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run tough-sweep-24 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/miacldek
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_134418-miacldek/logs
wandb: Agent Starting Run: cwnhl8c4 with config:
wandb: 	alpha: 0.95
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 5.111513292220632e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: True
wandb: 	rho: 1
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_135527-cwnhl8c4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-25
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/cwnhl8c4
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 400    | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
431       Trainable params
0         Non-trainable params
431       Total params
0.002     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 80-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–ˆâ–…â–‡
wandb:   grad_mean/lif_layers.1 â–†â–ˆâ–â–‚
wandb:   grad_mean/output_layer â–„â–ˆâ–â–ƒ
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–ˆâ–†â–†â–‡â–ˆâ–‡â–‡â–ˆâ–†â–†â–ˆâ–‡â–†â–†â–†â–…â–„â–„â–„â–ƒâ–‚â–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.02461
wandb:   grad_mean/lif_layers.1 0.00895
wandb:   grad_mean/output_layer 0.47743
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01669
wandb: spikes/train_avg_layer_1 0.00044
wandb:               train_loss 0.39063
wandb:      trainer/global_step 279
wandb:                 val_loss 0.38743
wandb: 
wandb: ğŸš€ View run distinctive-sweep-25 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/cwnhl8c4
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_135527-cwnhl8c4/logs
wandb: Agent Starting Run: or2n7eix with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 7.241467120425332e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0.1
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_135705-or2n7eix
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-26
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/or2n7eix
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 50.6 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
50.7 K    Trainable params
0         Non-trainable params
50.7 K    Total params
0.203     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–…â–‡â–ˆ
wandb:   grad_mean/lif_layers.1 â–â–…â–‡â–ˆ
wandb:   grad_mean/lif_layers.2 â–â–„â–ˆâ–ˆ
wandb:   grad_mean/lif_layers.3 â–â–ƒâ–ˆâ–ˆ
wandb:   grad_mean/output_layer â–â–„â–ˆâ–ˆ
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_2 â–â–â–â–â–‚â–‚â–‚â–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.00443
wandb:   grad_mean/lif_layers.1 0.00212
wandb:   grad_mean/lif_layers.2 0.00195
wandb:   grad_mean/lif_layers.3 0.0021
wandb:   grad_mean/output_layer 0.3125
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01921
wandb: spikes/train_avg_layer_1 0.00136
wandb: spikes/train_avg_layer_2 7e-05
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run easy-sweep-26 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/or2n7eix
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_135705-or2n7eix/logs
wandb: Agent Starting Run: 7up5dzyz with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.0004789280252329972
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: True
wandb: 	rho: 1
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_142947-7up5dzyz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clean-sweep-27
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/7up5dzyz
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 332    | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
351       Trainable params
0         Non-trainable params
351       Total params
0.001     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 80-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–‡â–ƒâ–
wandb:  grad_mean/lif_layers.1 â–ˆâ–„â–ƒâ–
wandb: grad_mean/lif_layers.10 â–ˆâ–ƒâ–ƒâ–
wandb: grad_mean/lif_layers.11 â–ˆâ–ƒâ–ƒâ–
wandb: grad_mean/lif_layers.12 â–ˆâ–„â–ƒâ–
wandb: grad_mean/lif_layers.13 â–ˆâ–„â–‚â–
wandb: grad_mean/lif_layers.14 â–ˆâ–„â–ƒâ–
wandb: grad_mean/lif_layers.15 â–ˆâ–„â–ƒâ–
wandb:  grad_mean/lif_layers.2 â–ˆâ–‚â–‚â–
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.00018
wandb:  grad_mean/lif_layers.1 0.00022
wandb: grad_mean/lif_layers.10 0.00788
wandb: grad_mean/lif_layers.11 0.00892
wandb: grad_mean/lif_layers.12 0.00742
wandb: grad_mean/lif_layers.13 0.00673
wandb: grad_mean/lif_layers.14 0.00446
wandb: grad_mean/lif_layers.15 0.00411
wandb:  grad_mean/lif_layers.2 0.00041
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run clean-sweep-27 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/7up5dzyz
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_142947-7up5dzyz/logs
wandb: Agent Starting Run: 55xgpxv7 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 6.026861452303461e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_143431-55xgpxv7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-28
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/55xgpxv7
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 50.6 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
50.7 K    Trainable params
0         Non-trainable params
50.7 K    Total params
0.203     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–ˆâ–†â–‡
wandb:   grad_mean/lif_layers.1 â–â–ˆâ–†â–‡
wandb:   grad_mean/lif_layers.2 â–â–ˆâ–ƒâ–…
wandb:   grad_mean/lif_layers.3 â–â–ˆâ–ƒâ–„
wandb:   grad_mean/output_layer â–â–ˆâ–ƒâ–„
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_2 â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.00312
wandb:   grad_mean/lif_layers.1 0.00145
wandb:   grad_mean/lif_layers.2 0.00116
wandb:   grad_mean/lif_layers.3 0.00146
wandb:   grad_mean/output_layer 0.23515
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01921
wandb: spikes/train_avg_layer_1 0.00143
wandb: spikes/train_avg_layer_2 0.0001
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run atomic-sweep-28 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/55xgpxv7
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_143431-55xgpxv7/logs
wandb: Agent Starting Run: fp7249rc with config:
wandb: 	alpha: 0.85
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 3.467564400645936e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 2
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_150718-fp7249rc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-29
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/fp7249rc
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 4.7 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
4.8 K     Trainable params
0         Non-trainable params
4.8 K     Total params
0.019     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–ˆâ–†â–„
wandb:   grad_mean/lif_layers.1 â–â–ˆâ–‡â–†
wandb:   grad_mean/output_layer â–â–ˆâ–‡â–„
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–…â–…â–„â–ƒâ–„â–ƒâ–„â–ƒâ–ƒâ–‚â–â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–‚â–‚â–â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               train_loss â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.00909
wandb:   grad_mean/lif_layers.1 0.00465
wandb:   grad_mean/output_layer 0.36233
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.0233
wandb: spikes/train_avg_layer_1 0.00198
wandb:               train_loss 0.29089
wandb:      trainer/global_step 279
wandb:                 val_loss 0.28832
wandb: 
wandb: ğŸš€ View run trim-sweep-29 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/fp7249rc
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_150718-fp7249rc/logs
wandb: Agent Starting Run: 4o1qtmjh with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.00032653706982409134
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0.1
wandb: 	zhyp_s: 0.9
wandb: setting up run 4o1qtmjh
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_151243-4o1qtmjh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-30
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/4o1qtmjh
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 944    | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
975       Trainable params
0         Non-trainable params
975       Total params
0.004     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 80-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–‡â–ˆâ–ˆ
wandb:   grad_mean/lif_layers.1 â–â–‡â–ˆâ–ˆ
wandb:   grad_mean/lif_layers.2 â–â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.3 â–ƒâ–ˆâ–‚â–
wandb:   grad_mean/output_layer â–ˆâ–†â–â–‚
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.03442
wandb:   grad_mean/lif_layers.1 0.01314
wandb:   grad_mean/lif_layers.2 0.00559
wandb:   grad_mean/lif_layers.3 0.00355
wandb:   grad_mean/output_layer 0.19363
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.03992
wandb: spikes/train_avg_layer_1 0.01483
wandb: spikes/train_avg_layer_2 0.00811
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run wild-sweep-30 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/4o1qtmjh
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_151243-4o1qtmjh/logs
wandb: Agent Starting Run: 7pjybm1m with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 2.644933604881764e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 2
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_151544-7pjybm1m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-31
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/7pjybm1m
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 13.0 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
13.1 K    Trainable params
0         Non-trainable params
13.1 K    Total params
0.052     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–‡â–ˆâ–…
wandb:   grad_mean/lif_layers.1 â–â–†â–ˆâ–‚
wandb:   grad_mean/lif_layers.2 â–â–…â–ˆâ–
wandb:   grad_mean/lif_layers.3 â–â–†â–ˆâ–
wandb:   grad_mean/output_layer â–‚â–†â–ˆâ–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–‚â–‚â–â–ƒâ–„â–ƒâ–„â–„â–„â–…â–„â–…â–„â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb: spikes/train_avg_layer_1 â–„â–…â–„â–„â–„â–‚â–â–â–â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–‡â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.00742
wandb:   grad_mean/lif_layers.1 0.00352
wandb:   grad_mean/lif_layers.2 0.00265
wandb:   grad_mean/lif_layers.3 0.00298
wandb:   grad_mean/output_layer 0.33561
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.0181
wandb: spikes/train_avg_layer_1 0.00085
wandb: spikes/train_avg_layer_2 0
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run fragrant-sweep-31 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/7pjybm1m
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_151544-7pjybm1m/logs
wandb: Agent Starting Run: lqdcff6m with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.00020655624031388543
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 2
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0.1
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_152805-lqdcff6m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-32
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/lqdcff6m
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 116 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
116 K     Trainable params
0         Non-trainable params
116 K     Total params
0.467     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: uploading console lines 40-40; updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–â–ƒâ–‡â–ˆ
wandb: grad_mean/lif_layers.1 â–â–ƒâ–‡â–ˆ
wandb: grad_mean/lif_layers.2 â–â–ƒâ–‡â–ˆ
wandb: grad_mean/lif_layers.3 â–â–ƒâ–‡â–ˆ
wandb: grad_mean/lif_layers.4 â–ƒâ–â–†â–ˆ
wandb: grad_mean/lif_layers.5 â–ˆâ–â–â–‚
wandb: grad_mean/lif_layers.6 â–ˆâ–ƒâ–â–
wandb: grad_mean/lif_layers.7 â–ˆâ–ƒâ–â–
wandb: grad_mean/output_layer â–ˆâ–„â–â–
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 2.12822
wandb: grad_mean/lif_layers.1 0.2106
wandb: grad_mean/lif_layers.2 0.03674
wandb: grad_mean/lif_layers.3 0.00857
wandb: grad_mean/lif_layers.4 0.00186
wandb: grad_mean/lif_layers.5 0.00031
wandb: grad_mean/lif_layers.6 0.00022
wandb: grad_mean/lif_layers.7 0.00036
wandb: grad_mean/output_layer 0.16171
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run revived-sweep-32 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/lqdcff6m
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_152805-lqdcff6m/logs
wandb: Agent Starting Run: yb63ta1e with config:
wandb: 	alpha: 0.95
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 1.8714651724114351e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_164009-yb63ta1e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-33
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/yb63ta1e
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 52     | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
71        Trainable params
0         Non-trainable params
71        Total params
0.000     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 73-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–†â–ˆâ–‡â–
wandb:   grad_mean/lif_layers.1 â–ˆâ–„â–„â–
wandb:   grad_mean/output_layer â–ˆâ–„â–‚â–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†
wandb: spikes/train_avg_layer_1 â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:      trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0902
wandb:   grad_mean/lif_layers.1 0.01544
wandb:   grad_mean/output_layer 0.53942
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01462
wandb: spikes/train_avg_layer_1 0.00124
wandb:               train_loss 0.46387
wandb:      trainer/global_step 279
wandb:                 val_loss 0.46029
wandb: 
wandb: ğŸš€ View run driven-sweep-33 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/yb63ta1e
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_164009-yb63ta1e/logs
wandb: Agent Starting Run: csg49279 with config:
wandb: 	alpha: 0.95
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 1.016685120891322e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 0.8
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_164106-csg49279
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-34
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/csg49279
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 116 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
116 K     Trainable params
0         Non-trainable params
116 K     Total params
0.467     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–ˆâ–â–â–…
wandb: grad_mean/lif_layers.1 â–ˆâ–ƒâ–â–…
wandb: grad_mean/lif_layers.2 â–ˆâ–ƒâ–â–…
wandb: grad_mean/lif_layers.3 â–ˆâ–ƒâ–â–†
wandb: grad_mean/lif_layers.4 â–ˆâ–ƒâ–â–†
wandb: grad_mean/lif_layers.5 â–ˆâ–ƒâ–â–†
wandb: grad_mean/lif_layers.6 â–ˆâ–‚â–â–…
wandb: grad_mean/lif_layers.7 â–ˆâ–‚â–â–…
wandb: grad_mean/output_layer â–ˆâ–‚â–â–†
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.0016
wandb: grad_mean/lif_layers.1 0.00076
wandb: grad_mean/lif_layers.2 0.00071
wandb: grad_mean/lif_layers.3 0.00084
wandb: grad_mean/lif_layers.4 0.00092
wandb: grad_mean/lif_layers.5 0.00105
wandb: grad_mean/lif_layers.6 0.00113
wandb: grad_mean/lif_layers.7 0.00129
wandb: grad_mean/output_layer 0.20394
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run olive-sweep-34 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/csg49279
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_164106-csg49279/logs
wandb: Agent Starting Run: x4wuh0v2 with config:
wandb: 	alpha: 0.95
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.00012869295838626978
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 2
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_175245-x4wuh0v2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-35
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/x4wuh0v2
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 13.0 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
13.1 K    Trainable params
0         Non-trainable params
13.1 K    Total params
0.052     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:   grad_mean/lif_layers.0 â–â–…â–‡â–ˆ
wandb:   grad_mean/lif_layers.1 â–â–†â–‡â–ˆ
wandb:   grad_mean/lif_layers.2 â–â–ˆâ–…â–†
wandb:   grad_mean/lif_layers.3 â–ˆâ–„â–â–
wandb:   grad_mean/output_layer â–ˆâ–…â–‚â–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_2 â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.06789
wandb:   grad_mean/lif_layers.1 0.01427
wandb:   grad_mean/lif_layers.2 0.00431
wandb:   grad_mean/lif_layers.3 0.0023
wandb:   grad_mean/output_layer 0.30968
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.02932
wandb: spikes/train_avg_layer_1 0.01229
wandb: spikes/train_avg_layer_2 0.01077
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run happy-sweep-35 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/x4wuh0v2
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_175245-x4wuh0v2/logs
wandb: Agent Starting Run: 7rs56pis with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 1.5328215974030265e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0.1
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_180451-7rs56pis
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-36
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/7rs56pis
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 116 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
116 K     Trainable params
0         Non-trainable params
116 K     Total params
0.467     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–â–„â–…â–ˆ
wandb: grad_mean/lif_layers.1 â–â–„â–…â–ˆ
wandb: grad_mean/lif_layers.2 â–â–‚â–‚â–ˆ
wandb: grad_mean/lif_layers.3 â–ˆâ–…â–â–ˆ
wandb: grad_mean/lif_layers.4 â–ˆâ–…â–â–ˆ
wandb: grad_mean/lif_layers.5 â–ˆâ–„â–â–ˆ
wandb: grad_mean/lif_layers.6 â–ˆâ–„â–â–ˆ
wandb: grad_mean/lif_layers.7 â–‡â–„â–â–ˆ
wandb: grad_mean/output_layer â–ˆâ–…â–â–ˆ
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.00226
wandb: grad_mean/lif_layers.1 0.00116
wandb: grad_mean/lif_layers.2 0.00094
wandb: grad_mean/lif_layers.3 0.00093
wandb: grad_mean/lif_layers.4 0.00107
wandb: grad_mean/lif_layers.5 0.00106
wandb: grad_mean/lif_layers.6 0.00111
wandb: grad_mean/lif_layers.7 0.00162
wandb: grad_mean/output_layer 0.26213
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run jolly-sweep-36 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/7rs56pis
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_180451-7rs56pis/logs
wandb: Agent Starting Run: 2vrudxyh with config:
wandb: 	alpha: 0.95
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 1.0669821344022375e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 1
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 1
wandb: setting up run 2vrudxyh
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_191636-2vrudxyh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-37
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/2vrudxyh
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 50.6 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
50.7 K    Trainable params
0         Non-trainable params
50.7 K    Total params
0.203     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–†â–…â–ˆ
wandb:   grad_mean/lif_layers.1 â–â–ˆâ–†â–ˆ
wandb:   grad_mean/lif_layers.2 â–â–ˆâ–‚â–ƒ
wandb:   grad_mean/lif_layers.3 â–ƒâ–ˆâ–â–‚
wandb:   grad_mean/output_layer â–„â–ˆâ–â–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–‚â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_2 â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.00323
wandb:   grad_mean/lif_layers.1 0.0015
wandb:   grad_mean/lif_layers.2 0.00126
wandb:   grad_mean/lif_layers.3 0.00152
wandb:   grad_mean/output_layer 0.23178
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01781
wandb: spikes/train_avg_layer_1 0.00105
wandb: spikes/train_avg_layer_2 1e-05
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run misunderstood-sweep-37 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/2vrudxyh
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_191636-2vrudxyh/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: qk0yvh4n with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 8.236479714984952e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 0.9
wandb: setting up run qk0yvh4n
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_194906-qk0yvh4n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-sweep-38
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/qk0yvh4n
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 52     | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
71        Trainable params
0         Non-trainable params
71        Total params
0.000     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–ˆâ–‡â–ƒ
wandb:   grad_mean/lif_layers.1 â–†â–â–‡â–ˆ
wandb:   grad_mean/output_layer â–ˆâ–â–‡â–ˆ
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–„â–…â–„â–…â–†â–„â–…â–…â–†â–‡â–ˆâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–â–â–‚â–‚â–‚â–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–‡â–‡â–‡â–†â–†â–†â–…â–…â–„â–„â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–„â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.18015
wandb:   grad_mean/lif_layers.1 0.03974
wandb:   grad_mean/output_layer 0.6772
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01564
wandb: spikes/train_avg_layer_1 0.00395
wandb:               train_loss 0.62061
wandb:      trainer/global_step 279
wandb:                 val_loss 0.6163
wandb: 
wandb: ğŸš€ View run stilted-sweep-38 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/qk0yvh4n
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_194906-qk0yvh4n/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: x4jmxkv5 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 2.468601196672923e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: False
wandb: 	rho: 2
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0.1
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_195003-x4jmxkv5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-39
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/x4jmxkv5
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 17.5 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
17.7 K    Trainable params
0         Non-trainable params
17.7 K    Total params
0.071     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–ˆâ–â–‚â–
wandb:   grad_mean/lif_layers.1 â–ˆâ–â–‡â–
wandb:   grad_mean/output_layer â–ˆâ–â–ˆâ–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–…â–…â–…â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–†â–…â–…â–…â–…â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–‚â–‚â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚
wandb: spikes/train_avg_layer_1 â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–†â–†â–„â–‚â–ƒâ–‚â–ƒâ–„â–„â–‚â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„
wandb:               train_loss â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.00548
wandb:   grad_mean/lif_layers.1 0.00262
wandb:   grad_mean/output_layer 0.3272
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01965
wandb: spikes/train_avg_layer_1 0.00136
wandb:               train_loss 0.27315
wandb:      trainer/global_step 279
wandb:                 val_loss 0.27067
wandb: 
wandb: ğŸš€ View run lively-sweep-39 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/x4jmxkv5
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_195003-x4jmxkv5/logs
wandb: Agent Starting Run: l0vrohes with config:
wandb: 	alpha: 0.85
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 1.2061782697977474e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0.1
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_200301-l0vrohes
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-40
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/l0vrohes
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 13.0 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
13.1 K    Trainable params
0         Non-trainable params
13.1 K    Total params
0.052     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–â–ˆâ–…
wandb:   grad_mean/lif_layers.1 â–ƒâ–â–ˆâ–‡
wandb:   grad_mean/lif_layers.2 â–ˆâ–â–…â–„
wandb:   grad_mean/lif_layers.3 â–ˆâ–â–ƒâ–„
wandb:   grad_mean/output_layer â–ˆâ–â–„â–„
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–ˆâ–ˆâ–ˆâ–‡â–‡â–…â–†â–…â–…â–†â–‡â–‡â–‡â–†â–†â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb: spikes/train_avg_layer_1 â–â–â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_2 â–„â–ƒâ–„â–‡â–‡â–‡â–‡â–…â–„â–„â–„â–„â–…â–ƒâ–‚â–‚â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–…â–†â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–…â–…â–…â–…â–…â–…â–…
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.00466
wandb:   grad_mean/lif_layers.1 0.00219
wandb:   grad_mean/lif_layers.2 0.00174
wandb:   grad_mean/lif_layers.3 0.00203
wandb:   grad_mean/output_layer 0.21672
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.02041
wandb: spikes/train_avg_layer_1 0.00168
wandb: spikes/train_avg_layer_2 2e-05
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run summer-sweep-40 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/l0vrohes
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_200301-l0vrohes/logs
wandb: Agent Starting Run: d84yvjm3 with config:
wandb: 	alpha: 0.95
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 8.750138652739507e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: False
wandb: 	rho: 2
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 0.8
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_201518-d84yvjm3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-41
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/d84yvjm3
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 62.9 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
63.0 K    Trainable params
0         Non-trainable params
63.0 K    Total params
0.252     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–â–‚â–†â–ˆ
wandb:  grad_mean/lif_layers.1 â–â–ƒâ–†â–ˆ
wandb: grad_mean/lif_layers.10 â–ˆâ–‚â–‡â–
wandb: grad_mean/lif_layers.11 â–ˆâ–‚â–‡â–
wandb: grad_mean/lif_layers.12 â–ˆâ–‚â–ˆâ–
wandb: grad_mean/lif_layers.13 â–ˆâ–‚â–ˆâ–
wandb: grad_mean/lif_layers.14 â–ˆâ–‚â–‡â–
wandb: grad_mean/lif_layers.15 â–ˆâ–‚â–‡â–
wandb:  grad_mean/lif_layers.2 â–â–ƒâ–†â–ˆ
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.01034
wandb:  grad_mean/lif_layers.1 0.00355
wandb: grad_mean/lif_layers.10 0.0015
wandb: grad_mean/lif_layers.11 0.00186
wandb: grad_mean/lif_layers.12 0.00215
wandb: grad_mean/lif_layers.13 0.00246
wandb: grad_mean/lif_layers.14 0.00282
wandb: grad_mean/lif_layers.15 0.00331
wandb:  grad_mean/lif_layers.2 0.00145
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run daily-sweep-41 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/d84yvjm3
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_201518-d84yvjm3/logs
wandb: Agent Starting Run: 62w1ikxq with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.00015974712344976697
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0.1
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_210758-62w1ikxq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-42
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/62w1ikxq
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 2.0 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
2.1 K     Trainable params
0         Non-trainable params
2.1 K     Total params
0.008     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–â–…â–ˆâ–ˆ
wandb: grad_mean/lif_layers.1 â–â–„â–ˆâ–ˆ
wandb: grad_mean/lif_layers.2 â–â–„â–ˆâ–ˆ
wandb: grad_mean/lif_layers.3 â–â–„â–ˆâ–ˆ
wandb: grad_mean/lif_layers.4 â–â–„â–ˆâ–‡
wandb: grad_mean/lif_layers.5 â–â–‚â–‡â–ˆ
wandb: grad_mean/lif_layers.6 â–„â–â–ˆâ–†
wandb: grad_mean/lif_layers.7 â–ˆâ–…â–‡â–
wandb: grad_mean/output_layer â–ˆâ–…â–†â–
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.00719
wandb: grad_mean/lif_layers.1 0.0024
wandb: grad_mean/lif_layers.2 0.00193
wandb: grad_mean/lif_layers.3 0.00141
wandb: grad_mean/lif_layers.4 0.00137
wandb: grad_mean/lif_layers.5 0.00186
wandb: grad_mean/lif_layers.6 0.00149
wandb: grad_mean/lif_layers.7 0.00234
wandb: grad_mean/output_layer 0.14203
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run sleek-sweep-42 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/62w1ikxq
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_210758-62w1ikxq/logs
wandb: Agent Starting Run: lh9qjnbc with config:
wandb: 	alpha: 0.95
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 2.355233802075125e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: False
wandb: 	rho: 1
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0.1
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_211344-lh9qjnbc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-43
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/lh9qjnbc
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 62.9 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
63.0 K    Trainable params
0         Non-trainable params
63.0 K    Total params
0.252     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–â–ˆâ–„â–‡
wandb:  grad_mean/lif_layers.1 â–â–ˆâ–„â–†
wandb: grad_mean/lif_layers.10 â–â–ˆâ–ƒâ–†
wandb: grad_mean/lif_layers.11 â–â–ˆâ–ƒâ–…
wandb: grad_mean/lif_layers.12 â–â–ˆâ–ƒâ–…
wandb: grad_mean/lif_layers.13 â–â–ˆâ–ƒâ–…
wandb: grad_mean/lif_layers.14 â–â–ˆâ–‚â–…
wandb: grad_mean/lif_layers.15 â–â–ˆâ–‚â–…
wandb:  grad_mean/lif_layers.2 â–â–ˆâ–„â–†
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 7e-05
wandb:  grad_mean/lif_layers.1 4e-05
wandb: grad_mean/lif_layers.10 0.00132
wandb: grad_mean/lif_layers.11 0.00142
wandb: grad_mean/lif_layers.12 0.00168
wandb: grad_mean/lif_layers.13 0.00169
wandb: grad_mean/lif_layers.14 0.00176
wandb: grad_mean/lif_layers.15 0.00187
wandb:  grad_mean/lif_layers.2 6e-05
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run wandering-sweep-43 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/lh9qjnbc
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_211344-lh9qjnbc/logs
wandb: Agent Starting Run: 4nq8x0eq with config:
wandb: 	alpha: 0.85
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.0004006370966642723
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_220625-4nq8x0eq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-44
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/4nq8x0eq
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 4.7 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
4.8 K     Trainable params
0         Non-trainable params
4.8 K     Total params
0.019     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–ˆâ–ƒâ–â–
wandb:   grad_mean/lif_layers.1 â–ˆâ–ƒâ–â–
wandb:   grad_mean/output_layer â–ˆâ–…â–â–‚
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               train_loss â–ˆâ–ˆâ–‡â–‡â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:                 val_loss â–ˆâ–ˆâ–‡â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.00253
wandb:   grad_mean/lif_layers.1 0.0004
wandb:   grad_mean/output_layer 0.10418
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.03046
wandb: spikes/train_avg_layer_1 0.01937
wandb:               train_loss 0.16832
wandb:      trainer/global_step 279
wandb:                 val_loss 0.16662
wandb: 
wandb: ğŸš€ View run lemon-sweep-44 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/4nq8x0eq
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_220625-4nq8x0eq/logs
wandb: Agent Starting Run: 6oyfsa80 with config:
wandb: 	alpha: 0.85
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 8.053173448643449e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: True
wandb: 	rho: 1
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_221150-6oyfsa80
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-sweep-45
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/6oyfsa80
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 248 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
248 K     Trainable params
0         Non-trainable params
248 K     Total params
0.995     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–â–‚â–…â–ˆ
wandb:  grad_mean/lif_layers.1 â–â–‚â–…â–ˆ
wandb: grad_mean/lif_layers.10 â–ˆâ–„â–â–„
wandb: grad_mean/lif_layers.11 â–ˆâ–„â–â–‚
wandb: grad_mean/lif_layers.12 â–ˆâ–…â–â–‚
wandb: grad_mean/lif_layers.13 â–ˆâ–…â–â–‚
wandb: grad_mean/lif_layers.14 â–ˆâ–…â–â–‚
wandb: grad_mean/lif_layers.15 â–ˆâ–…â–â–‚
wandb:  grad_mean/lif_layers.2 â–â–‚â–…â–ˆ
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.15244
wandb:  grad_mean/lif_layers.1 0.03026
wandb: grad_mean/lif_layers.10 0.00099
wandb: grad_mean/lif_layers.11 0.00116
wandb: grad_mean/lif_layers.12 0.00132
wandb: grad_mean/lif_layers.13 0.00143
wandb: grad_mean/lif_layers.14 0.00151
wandb: grad_mean/lif_layers.15 0.00194
wandb:  grad_mean/lif_layers.2 0.01038
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run apricot-sweep-45 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/6oyfsa80
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251122_221150-6oyfsa80/logs
wandb: Agent Starting Run: y2bmpk9v with config:
wandb: 	alpha: 0.95
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 1.2532382256360734e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_004220-y2bmpk9v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-sweep-46
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/y2bmpk9v
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 92     | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
111       Trainable params
0         Non-trainable params
111       Total params
0.000     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 71-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–ˆâ–â–ƒâ–ƒ
wandb:   grad_mean/lif_layers.1 â–‡â–…â–ˆâ–
wandb:   grad_mean/lif_layers.2 â–ˆâ–â–ƒâ–‚
wandb:   grad_mean/lif_layers.3 â–ˆâ–‚â–‚â–
wandb:   grad_mean/output_layer â–ˆâ–‚â–‚â–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–„â–„â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_2 â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–†â–†â–‡â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0363
wandb:   grad_mean/lif_layers.1 0.02056
wandb:   grad_mean/lif_layers.2 0.01296
wandb:   grad_mean/lif_layers.3 0.00867
wandb:   grad_mean/output_layer 0.29368
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01923
wandb: spikes/train_avg_layer_1 0.00495
wandb: spikes/train_avg_layer_2 6e-05
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run blooming-sweep-46 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/y2bmpk9v
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_004220-y2bmpk9v/logs
wandb: Agent Starting Run: 5fnbnn4q with config:
wandb: 	alpha: 0.95
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 1.9538728611778153e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: True
wandb: 	rho: 1
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_004353-5fnbnn4q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-sweep-47
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/5fnbnn4q
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 116 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
116 K     Trainable params
0         Non-trainable params
116 K     Total params
0.467     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–â–„â–‡â–ˆ
wandb: grad_mean/lif_layers.1 â–â–…â–ˆâ–†
wandb: grad_mean/lif_layers.2 â–â–†â–ˆâ–†
wandb: grad_mean/lif_layers.3 â–â–†â–ˆâ–†
wandb: grad_mean/lif_layers.4 â–â–†â–ˆâ–†
wandb: grad_mean/lif_layers.5 â–â–†â–ˆâ–†
wandb: grad_mean/lif_layers.6 â–â–…â–ˆâ–†
wandb: grad_mean/lif_layers.7 â–â–†â–ˆâ–†
wandb: grad_mean/output_layer â–â–†â–ˆâ–‡
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.00151
wandb: grad_mean/lif_layers.1 0.00076
wandb: grad_mean/lif_layers.2 0.00069
wandb: grad_mean/lif_layers.3 0.00077
wandb: grad_mean/lif_layers.4 0.00081
wandb: grad_mean/lif_layers.5 0.00088
wandb: grad_mean/lif_layers.6 0.00094
wandb: grad_mean/lif_layers.7 0.00118
wandb: grad_mean/output_layer 0.20103
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run morning-sweep-47 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/5fnbnn4q
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_004353-5fnbnn4q/logs
wandb: Agent Starting Run: m4hujhm0 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 1.2923931193801745e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 2
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_015546-m4hujhm0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-48
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/m4hujhm0
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 52     | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
71        Trainable params
0         Non-trainable params
71        Total params
0.000     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:   grad_mean/lif_layers.0 â–‚â–ˆâ–â–„
wandb:   grad_mean/lif_layers.1 â–â–ˆâ–â–„
wandb:   grad_mean/output_layer â–â–ˆâ–â–„
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–ƒâ–ƒâ–„â–…â–…â–†â–†â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–†â–„â–„â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒ
wandb: spikes/train_avg_layer_1 â–â–‚â–ƒâ–„â–‚â–„â–‚â–â–ƒâ–„â–…â–…â–‡â–‡â–…â–„â–ˆâ–‡â–…â–„â–…â–…â–†â–†â–…â–…â–„â–†â–†â–‡â–†â–‡â–‡â–†â–†â–…â–†â–†â–‡â–‡
wandb:               train_loss â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.01065
wandb:   grad_mean/lif_layers.1 0.00955
wandb:   grad_mean/output_layer 0.18647
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01
wandb: spikes/train_avg_layer_1 8e-05
wandb:               train_loss 0.19805
wandb:      trainer/global_step 279
wandb:                 val_loss 0.19807
wandb: 
wandb: ğŸš€ View run vocal-sweep-48 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/m4hujhm0
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_015546-m4hujhm0/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 98vzbpay with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 6.22939206063586e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_015653-98vzbpay
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-49
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/98vzbpay
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 2.0 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
2.1 K     Trainable params
0         Non-trainable params
2.1 K     Total params
0.008     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–â–â–ˆâ–…
wandb: grad_mean/lif_layers.1 â–â–ƒâ–ˆâ–‡
wandb: grad_mean/lif_layers.2 â–‡â–…â–ˆâ–
wandb: grad_mean/lif_layers.3 â–ˆâ–…â–†â–
wandb: grad_mean/lif_layers.4 â–ˆâ–…â–†â–
wandb: grad_mean/lif_layers.5 â–ˆâ–…â–‡â–
wandb: grad_mean/lif_layers.6 â–ˆâ–…â–†â–
wandb: grad_mean/lif_layers.7 â–ˆâ–…â–…â–
wandb: grad_mean/output_layer â–ˆâ–…â–†â–
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.00387
wandb: grad_mean/lif_layers.1 0.00319
wandb: grad_mean/lif_layers.2 0.00334
wandb: grad_mean/lif_layers.3 0.00403
wandb: grad_mean/lif_layers.4 0.00523
wandb: grad_mean/lif_layers.5 0.00545
wandb: grad_mean/lif_layers.6 0.00499
wandb: grad_mean/lif_layers.7 0.00628
wandb: grad_mean/output_layer 0.31961
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run fresh-sweep-49 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/98vzbpay
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_015653-98vzbpay/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: vg1evqrs with config:
wandb: 	alpha: 0.95
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 9.220767580233252e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_020239-vg1evqrs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-50
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/vg1evqrs
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 52     | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
71        Trainable params
0         Non-trainable params
71        Total params
0.000     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 80-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–ˆâ–…â–â–ƒ
wandb:   grad_mean/lif_layers.1 â–ˆâ–„â–‚â–
wandb:   grad_mean/output_layer â–ˆâ–…â–‚â–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–‚â–â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–â–‚â–‚â–„â–†â–†â–‡â–ˆâ–‡â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:               train_loss â–ˆâ–ˆâ–‡â–‡â–†â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–ˆâ–ˆâ–‡â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.00251
wandb:   grad_mean/lif_layers.1 0.00133
wandb:   grad_mean/output_layer 0.00932
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.03575
wandb: spikes/train_avg_layer_1 0.00284
wandb:               train_loss 0.16441
wandb:      trainer/global_step 279
wandb:                 val_loss 0.16358
wandb: 
wandb: ğŸš€ View run distinctive-sweep-50 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/vg1evqrs
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_020239-vg1evqrs/logs
wandb: Agent Starting Run: ulcrmrv1 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 1.7014323601469043e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_020331-ulcrmrv1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-51
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/ulcrmrv1
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 50.6 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
50.7 K    Trainable params
0         Non-trainable params
50.7 K    Total params
0.203     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–„â–‡â–ˆ
wandb:   grad_mean/lif_layers.1 â–â–„â–‡â–ˆ
wandb:   grad_mean/lif_layers.2 â–ƒâ–â–†â–ˆ
wandb:   grad_mean/lif_layers.3 â–ˆâ–â–â–‚
wandb:   grad_mean/output_layer â–ˆâ–â–â–‚
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_2 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.00634
wandb:   grad_mean/lif_layers.1 0.0022
wandb:   grad_mean/lif_layers.2 0.00143
wandb:   grad_mean/lif_layers.3 0.0017
wandb:   grad_mean/output_layer 0.25405
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.0186
wandb: spikes/train_avg_layer_1 0.00169
wandb: spikes/train_avg_layer_2 6e-05
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run fallen-sweep-51 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/ulcrmrv1
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_020331-ulcrmrv1/logs
wandb: Agent Starting Run: 19zbn8g0 with config:
wandb: 	alpha: 0.85
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.00019931925099752717
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 2
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.8
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_023557-19zbn8g0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-52
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/19zbn8g0
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 248 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
248 K     Trainable params
0         Non-trainable params
248 K     Total params
0.995     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: Network error (HTTPError), entering retry loop.
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                   epoch â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–â–â–…â–ˆ
wandb:  grad_mean/lif_layers.1 â–â–â–…â–ˆ
wandb: grad_mean/lif_layers.10 â–â–†â–‡â–ˆ
wandb: grad_mean/lif_layers.11 â–â–ˆâ–‡â–‡
wandb: grad_mean/lif_layers.12 â–â–ˆâ–„â–„
wandb: grad_mean/lif_layers.13 â–…â–ˆâ–‚â–
wandb: grad_mean/lif_layers.14 â–ˆâ–†â–‚â–
wandb: grad_mean/lif_layers.15 â–ˆâ–„â–‚â–
wandb:  grad_mean/lif_layers.2 â–â–â–†â–ˆ
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 3186.57764
wandb:  grad_mean/lif_layers.1 470.53333
wandb: grad_mean/lif_layers.10 0.00907
wandb: grad_mean/lif_layers.11 0.00426
wandb: grad_mean/lif_layers.12 0.00233
wandb: grad_mean/lif_layers.13 0.00142
wandb: grad_mean/lif_layers.14 0.00091
wandb: grad_mean/lif_layers.15 0.00102
wandb:  grad_mean/lif_layers.2 105.61135
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run grateful-sweep-52 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/19zbn8g0
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_023557-19zbn8g0/logs
wandb: Agent Starting Run: gd3gzfme with config:
wandb: 	alpha: 0.85
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 8.538795761173245e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 1
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_050657-gd3gzfme
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-53
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/gd3gzfme
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 50.6 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
50.7 K    Trainable params
0         Non-trainable params
50.7 K    Total params
0.203     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 82-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–…â–‡â–ˆ
wandb:   grad_mean/lif_layers.1 â–â–…â–‡â–ˆ
wandb:   grad_mean/lif_layers.2 â–ˆâ–„â–â–‚
wandb:   grad_mean/lif_layers.3 â–ˆâ–ƒâ–â–
wandb:   grad_mean/output_layer â–ˆâ–‚â–â–‚
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_2 â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.02389
wandb:   grad_mean/lif_layers.1 0.00469
wandb:   grad_mean/lif_layers.2 0.0013
wandb:   grad_mean/lif_layers.3 0.00076
wandb:   grad_mean/output_layer 0.16384
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.02522
wandb: spikes/train_avg_layer_1 0.01321
wandb: spikes/train_avg_layer_2 0.01306
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run vocal-sweep-53 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/gd3gzfme
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_050657-gd3gzfme/logs
wandb: Agent Starting Run: wfqz8nzr with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 5.862734904758551e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 2
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_053927-wfqz8nzr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-54
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/wfqz8nzr
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 172    | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
191       Trainable params
0         Non-trainable params
191       Total params
0.001     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 76-83, summary, console lines 40-40
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb: grad_mean/lif_layers.0 â–â–ˆâ–‡â–†
wandb: grad_mean/lif_layers.1 â–â–â–ˆâ–…
wandb: grad_mean/lif_layers.2 â–…â–ˆâ–ˆâ–
wandb: grad_mean/lif_layers.3 â–†â–ˆâ–ˆâ–
wandb: grad_mean/lif_layers.4 â–†â–ˆâ–ˆâ–
wandb: grad_mean/lif_layers.5 â–†â–ˆâ–ˆâ–
wandb: grad_mean/lif_layers.6 â–†â–ˆâ–ˆâ–
wandb: grad_mean/lif_layers.7 â–…â–‡â–ˆâ–
wandb: grad_mean/output_layer â–…â–‡â–ˆâ–
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.00381
wandb: grad_mean/lif_layers.1 0.00595
wandb: grad_mean/lif_layers.2 0.00443
wandb: grad_mean/lif_layers.3 0.00615
wandb: grad_mean/lif_layers.4 0.0065
wandb: grad_mean/lif_layers.5 0.00755
wandb: grad_mean/lif_layers.6 0.00561
wandb: grad_mean/lif_layers.7 0.00789
wandb: grad_mean/output_layer 0.19407
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run logical-sweep-54 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/wfqz8nzr
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_053927-wfqz8nzr/logs
wandb: Agent Starting Run: 1sf9yryh with config:
wandb: 	alpha: 0.95
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.0009413889813915722
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 1
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 1
wandb: setting up run 1sf9yryh
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_054202-1sf9yryh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clean-sweep-55
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/1sf9yryh
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 2.0 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
2.1 K     Trainable params
0         Non-trainable params
2.1 K     Total params
0.008     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 42-43
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–â–„â–†â–ˆ
wandb: grad_mean/lif_layers.1 â–â–„â–‡â–ˆ
wandb: grad_mean/lif_layers.2 â–â–„â–‡â–ˆ
wandb: grad_mean/lif_layers.3 â–â–…â–‡â–ˆ
wandb: grad_mean/lif_layers.4 â–â–…â–‡â–ˆ
wandb: grad_mean/lif_layers.5 â–â–…â–‡â–ˆ
wandb: grad_mean/lif_layers.6 â–â–…â–‡â–ˆ
wandb: grad_mean/lif_layers.7 â–ˆâ–â–„â–‡
wandb: grad_mean/output_layer â–ˆâ–…â–ƒâ–
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 1.49449
wandb: grad_mean/lif_layers.1 0.42565
wandb: grad_mean/lif_layers.2 0.13647
wandb: grad_mean/lif_layers.3 0.03792
wandb: grad_mean/lif_layers.4 0.0175
wandb: grad_mean/lif_layers.5 0.00454
wandb: grad_mean/lif_layers.6 0.00167
wandb: grad_mean/lif_layers.7 0.0003
wandb: grad_mean/output_layer 0.00998
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run clean-sweep-55 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/1sf9yryh
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_054202-1sf9yryh/logs
wandb: Agent Starting Run: hl8irws6 with config:
wandb: 	alpha: 0.85
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 1.8489224524447894e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 4
wandb: 	relu_bypass: False
wandb: 	rho: 1
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.1
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 0.8
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_054737-hl8irws6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-sweep-56
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/hl8irws6
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 52     | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
71        Trainable params
0         Non-trainable params
71        Total params
0.000     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–‚â–…â–ˆ
wandb:   grad_mean/lif_layers.1 â–‚â–…â–â–ˆ
wandb:   grad_mean/output_layer â–ƒâ–…â–â–ˆ
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–‚â–‚â–ƒâ–„â–…â–†â–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:               train_loss â–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.02707
wandb:   grad_mean/lif_layers.1 0.0227
wandb:   grad_mean/output_layer 0.46166
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.02487
wandb: spikes/train_avg_layer_1 0.00232
wandb:               train_loss 0.36658
wandb:      trainer/global_step 279
wandb:                 val_loss 0.36349
wandb: 
wandb: ğŸš€ View run northern-sweep-56 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/hl8irws6
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_054737-hl8irws6/logs
wandb: Agent Starting Run: qcs6c22d with config:
wandb: 	alpha: 0.95
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.00012814529229736055
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -5
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0.2
wandb: 	zhyp_s: 1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_054829-qcs6c22d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-sweep-57
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/qcs6c22d
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 13.0 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
13.1 K    Trainable params
0         Non-trainable params
13.1 K    Total params
0.052     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–†â–ˆâ–ˆ
wandb:   grad_mean/lif_layers.1 â–â–‡â–ˆâ–‡
wandb:   grad_mean/lif_layers.2 â–â–ˆâ–†â–
wandb:   grad_mean/lif_layers.3 â–ˆâ–„â–ƒâ–
wandb:   grad_mean/output_layer â–ˆâ–„â–„â–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: spikes/train_avg_layer_2 â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.02305
wandb:   grad_mean/lif_layers.1 0.00514
wandb:   grad_mean/lif_layers.2 0.00156
wandb:   grad_mean/lif_layers.3 0.00113
wandb:   grad_mean/output_layer 0.14871
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.02607
wandb: spikes/train_avg_layer_1 0.01015
wandb: spikes/train_avg_layer_2 0.00753
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run youthful-sweep-57 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/qcs6c22d
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_054829-qcs6c22d/logs
wandb: Agent Starting Run: qplt37i3 with config:
wandb: 	alpha: 0.85
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -5
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.0002526275083159912
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 1
wandb: 	rs: -3
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0.2
wandb: 	zdep_s: 0.1
wandb: 	zhyp_s: 0.8
wandb: setting up run qplt37i3
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251123_060056-qplt37i3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-58
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/ur9eoqq0
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/qplt37i3
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 17.5 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
17.7 K    Trainable params
0         Non-trainable params
17.7 K    Total params
0.071     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
slurmstepd: error: *** JOB 2207431 ON uc3n062 CANCELLED AT 2025-11-23T06:08:42 DUE TO TIME LIMIT ***
