{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f0baaac-eee8-4fa6-b539-5267bfe7aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class SRC(nn.Module):\n",
    "    \"\"\"\n",
    "    Spike-based Recurrent Cell (SRC) layer.\n",
    "\n",
    "    Defaults set to values:\n",
    "      alpha_init = 0.99\n",
    "      rho = 3.0\n",
    "      r = 2.0\n",
    "      rs = -7.0\n",
    "      bh_init = -6.0 (clamped to <= bh_max = -4.0 during forward)\n",
    "      z = 0.0\n",
    "      zhyp_s = 0.9\n",
    "      zdep_s = 0.0\n",
    "\n",
    "    Input shapes:\n",
    "      - seq-first: (seq_len, batch, input_size) (default)\n",
    "      - if batch_first=True: (batch, seq_len, input_size)\n",
    "    Output:\n",
    "      - s_out_seq: (seq_len, batch, hidden_size)\n",
    "      - final states: (h_T, hs_T, i_T) each (batch, hidden_size)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        batch_first: bool = False,\n",
    "        # Table 1 defaults:\n",
    "        rho: float = 3.0,\n",
    "        r: float = 2.0,\n",
    "        rs: float = -7.0,\n",
    "        z: float = 0.0,\n",
    "        zhyp_s: float = 0.9,\n",
    "        zdep_s: float = 0.0,\n",
    "        bh_init: float = -6.0,\n",
    "        bh_max: float = -4.0,\n",
    "        alpha_init: float = 0.99,\n",
    "        learnable_alpha: bool = False,\n",
    "        learnable_bh: bool = True,\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        # constants and buffers (per-neuron vectors for broadcasting)\n",
    "        self.register_buffer(\"_r\", torch.full((hidden_size,), float(r), **factory_kwargs))\n",
    "        self.register_buffer(\"_rs\", torch.full((hidden_size,), float(rs), **factory_kwargs))\n",
    "        self.register_buffer(\"_z\", torch.full((hidden_size,), float(z), **factory_kwargs))\n",
    "\n",
    "        # zs constants (scalars)\n",
    "        self._zhyp_s = float(zhyp_s)\n",
    "        self._zdep_s = float(zdep_s)\n",
    "\n",
    "        self.rho = float(rho)\n",
    "        self.bh_max = float(bh_max)\n",
    "\n",
    "        # input projection weights Ws: shape (hidden_size, input_size)\n",
    "        self.Ws = nn.Parameter(torch.empty(hidden_size, input_size, **factory_kwargs))\n",
    "\n",
    "        # bias bh per neuron (learnable optionally)\n",
    "        if learnable_bh:\n",
    "            self.bh = nn.Parameter(torch.full((hidden_size,), float(bh_init), **factory_kwargs))\n",
    "        else:\n",
    "            self.register_buffer(\"bh\", torch.full((hidden_size,), float(bh_init), **factory_kwargs))\n",
    "\n",
    "        # alpha: leaky integrator coefficient (learnable optional)\n",
    "        self.learnable_alpha = bool(learnable_alpha)\n",
    "        if self.learnable_alpha:\n",
    "            # parameterize via logit so sigmoid(alpha_raw) in (0,1)\n",
    "            alpha_raw_init = torch.logit(torch.tensor(alpha_init, **factory_kwargs).clamp(1e-6, 1 - 1e-6))\n",
    "            self.alpha_raw = nn.Parameter(torch.full((hidden_size,), float(alpha_raw_init), **factory_kwargs))\n",
    "        else:\n",
    "            self.register_buffer(\"_alpha_scalar\", torch.tensor(float(alpha_init), **factory_kwargs))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # initialize Ws with a Kaiming-like uniform; small random init is good\n",
    "        nn.init.kaiming_uniform_(self.Ws, a=5**0.5)\n",
    "        # other params/buffers already set through init\n",
    "\n",
    "    @property\n",
    "    def alpha(self) -> torch.Tensor:\n",
    "        \"\"\"Return alpha in (0,1) shape (hidden_size,)\"\"\"\n",
    "        if self.learnable_alpha:\n",
    "            return torch.sigmoid(self.alpha_raw)\n",
    "        else:\n",
    "            return self._alpha_scalar.expand(self.hidden_size)\n",
    "\n",
    "    def _compute_zs(self, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        zs[h] = zhyp_s + (zdep_s - zhyp_s) * H(h - 0.5)\n",
    "        h shape: (batch, hidden_size)\n",
    "        returns zs shape: (batch, hidden_size)\n",
    "        \"\"\"\n",
    "        step = (h >= 0.5).to(h.dtype)\n",
    "        zs_val = self._zhyp_s + (self._zdep_s - self._zhyp_s) * step\n",
    "        return zs_val\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        s_in: torch.Tensor,\n",
    "        hx: Optional[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = None\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        s_in: input pulses, shape (seq_len, batch, input_size) or (batch, seq_len, input_size) if batch_first\n",
    "        hx: optional tuple (h0, hs0, i0)\n",
    "        \"\"\"\n",
    "        if self.batch_first:\n",
    "            s_in = s_in.transpose(0, 1)  # -> (seq, batch, in)\n",
    "\n",
    "        seq_len, batch_size, in_size = s_in.shape\n",
    "        assert in_size == self.input_size, f\"input_size mismatch: got {in_size}, expected {self.input_size}\"\n",
    "\n",
    "        device = s_in.device\n",
    "        dtype = s_in.dtype\n",
    "\n",
    "        # initial states\n",
    "        if hx is None:\n",
    "            h = torch.zeros(batch_size, self.hidden_size, device=device, dtype=dtype)\n",
    "            hs = torch.zeros_like(h)\n",
    "            i = torch.zeros_like(h)\n",
    "        else:\n",
    "            h, hs, i = hx\n",
    "            if h is None:\n",
    "                h = torch.zeros(batch_size, self.hidden_size, device=device, dtype=dtype)\n",
    "            if hs is None:\n",
    "                hs = torch.zeros_like(h)\n",
    "            if i is None:\n",
    "                i = torch.zeros_like(h)\n",
    "\n",
    "        # constant vectors\n",
    "        r = self._r.to(device=device, dtype=dtype)\n",
    "        rs = self._rs.to(device=device, dtype=dtype)\n",
    "        z = self._z.to(device=device, dtype=dtype)\n",
    "        alpha = self.alpha.to(device=device, dtype=dtype)\n",
    "\n",
    "        s_out_seq = []\n",
    "        bh_clamped = torch.clamp(self.bh, max=self.bh_max).to(device=device, dtype=dtype)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            s_in_t = s_in[t]  # (batch, input_size)\n",
    "            projected = F.linear(s_in_t, self.Ws)  # (batch, hidden_size)\n",
    "\n",
    "            alpha_b = alpha.unsqueeze(0)  # (1, hidden_size)\n",
    "            i = alpha_b * i + projected  # i[t] = alpha * i[t-1] + Ws * s_in[t]\n",
    "\n",
    "            # x[t] = rho * tanh(i / rho)\n",
    "            x = self.rho * torch.tanh(i / self.rho)\n",
    "\n",
    "            # candidate hidden state\n",
    "            h_cand = torch.tanh(x + h * r.unsqueeze(0) + hs * rs.unsqueeze(0) + bh_clamped.unsqueeze(0))\n",
    "\n",
    "            # update h: h[t] = z * h_prev + (1-z) * h_cand (z default 0 -> h = h_cand)\n",
    "            z_b = z.unsqueeze(0)\n",
    "            h_new = z_b * h + (1.0 - z_b) * h_cand\n",
    "\n",
    "            # compute zs from updated h (matches Eq.5c's use of h[t] in the step)\n",
    "            zs = self._compute_zs(h_new)  # (batch, hidden_size)\n",
    "\n",
    "            # update hs: hs[t] = zs * hs_prev + (1-zs) * h_prev (uses previous h)\n",
    "            hs_new = zs * hs + (1.0 - zs) * h\n",
    "\n",
    "            # output spikes: ReLU(h[t])\n",
    "            s_out_t = F.relu(h_new)\n",
    "\n",
    "            # save state\n",
    "            h = h_new\n",
    "            hs = hs_new\n",
    "\n",
    "            s_out_seq.append(s_out_t.unsqueeze(0))\n",
    "\n",
    "        s_out_seq = torch.cat(s_out_seq, dim=0)  # (seq_len, batch, hidden_size)\n",
    "        return s_out_seq, (h, hs, i)\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        s_in_t: torch.Tensor,\n",
    "        hx: Optional[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = None\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"Single-step convenience wrapper. s_in_t shape: (batch, input_size)\"\"\"\n",
    "        seq_in = s_in_t.unsqueeze(0)\n",
    "        out_seq, states = self.forward(seq_in, hx=hx)\n",
    "        return out_seq[0], states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3814a5cf-62a1-4692-8f1d-817c2d7e2c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2, 16])\n",
      "spiking timesteps: 0.0\n"
     ]
    }
   ],
   "source": [
    "# smoke\n",
    "batch = 2\n",
    "seq = 20\n",
    "inp = 5\n",
    "hid = 16\n",
    "cell = SRC(input_size=inp, hidden_size=hid, batch_first=False)\n",
    "s_in = (torch.rand(seq, batch, inp) > 0.97).float()  # very sparse pulses\n",
    "s_out, (h_T, hs_T, i_T) = cell(s_in)\n",
    "print(s_out.shape)  # (seq, batch, hid)\n",
    "print(\"spiking timesteps:\", (s_out > 0).float().sum().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5e5c9-2dc2-4116-a427-86f6cdcb6855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# ---- SRC cell (paste the previous SRC class here) ----\n",
    "# Make sure the SRC class code is already defined in your environment.\n",
    "\n",
    "# ---- 1. Generate synthetic spiking dataset ----\n",
    "def generate_spike_data(seq_len=20, batch_size=64, input_size=5):\n",
    "    \"\"\"\n",
    "    Generates random sparse spike inputs and a binary label based on a simple rule:\n",
    "    label = 1 if sum of spikes in first 3 input neurons at last timestep > 1\n",
    "    \"\"\"\n",
    "    X = (torch.rand(seq_len, batch_size, input_size) > 0.9).float()\n",
    "    # simple rule: sum of first 3 inputs at last timestep > 1\n",
    "    y = (X[-1, :, :3].sum(dim=1) > 1).float()\n",
    "    return X, y\n",
    "\n",
    "# ---- 2. Define a small network with SRC ----\n",
    "class SRCNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.src = SRC(input_size=input_size, hidden_size=hidden_size)\n",
    "        self.readout = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (seq_len, batch, input_size)\n",
    "        s_out, _ = self.src(x)  # s_out: (seq, batch, hidden)\n",
    "        last_h = s_out[-1]      # use last timestep for readout\n",
    "        out = self.readout(last_h)\n",
    "        return out\n",
    "\n",
    "# ---- 3. Surrogate gradient helper ----\n",
    "# We'll use a simple straight-through approximation for ReLU spikes\n",
    "class SurrogateSpike(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return (input > 0).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # simple surrogate gradient: gradient of fast sigmoid\n",
    "        return grad_output * torch.exp(-input.abs())  # decays for large inputs\n",
    "\n",
    "# Replace F.relu with SurrogateSpike if you want gradients through spikes\n",
    "# For simplicity, weâ€™ll continue with normal ReLU in this toy example\n",
    "\n",
    "# ---- 4. Training loop ----\n",
    "# Hyperparameters\n",
    "input_size = 5\n",
    "hidden_size = 16\n",
    "output_size = 1  # binary classification\n",
    "seq_len = 20\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "lr = 1e-2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# model, loss, optimizer\n",
    "model = SRCNetwork(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# training\n",
    "for epoch in range(epochs):\n",
    "    X, y = generate_spike_data(seq_len, batch_size, input_size)\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X)  # (batch, 1)\n",
    "    loss = criterion(logits.squeeze(), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = (torch.sigmoid(logits.squeeze()) > 0.5).float()\n",
    "        acc = (pred == y).float().mean().item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}, Loss: {loss.item():.4f}, Acc: {acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a94db3-e7a2-4175-9b4d-78f5ff31866a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
