/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
wandb: Agent Starting Run: jxhri08c with config:
wandb: 	alpha: 8
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Currently logged in as: lupos to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_154525-jxhri08c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-sweep-1
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/jxhri08c
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 517 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
517 K     Trainable params
0         Non-trainable params
517 K     Total params
2.072     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–â–‚
wandb:  grad_mean/lif_layers.1 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.10 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.11 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.12 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.13 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.14 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.15 â–ˆâ–â–ƒ
wandb:  grad_mean/lif_layers.2 â–ˆâ–â–ƒ
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 0.0
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run eternal-sweep-1 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/jxhri08c
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_154525-jxhri08c/logs
wandb: Agent Starting Run: e04xytix with config:
wandb: 	alpha: 6
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 32
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: setting up run e04xytix
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_165131-e04xytix
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-2
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/e04xytix
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 13.7 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 33     | train
--------------------------------------------------------
13.7 K    Trainable params
0         Non-trainable params
13.7 K    Total params
0.055     Total estimated model params size (MB)
13        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–‚â–ˆ
wandb:   grad_mean/lif_layers.1 â–â–â–ˆ
wandb:   grad_mean/output_layer â–â–â–ˆ
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–†â–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/output_layer 0.00578
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb:               train_loss 0.16247
wandb:      trainer/global_step 279
wandb:                 val_loss 0.16158
wandb: 
wandb: ğŸš€ View run electric-sweep-2 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/e04xytix
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_165131-e04xytix/logs
wandb: Agent Starting Run: fj9t18ye with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_165538-fj9t18ye
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-3
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/fj9t18ye
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 16.8 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
16.9 K    Trainable params
0         Non-trainable params
16.9 K    Total params
0.067     Total estimated model params size (MB)
31        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 81-82, summary, console lines 38-38
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–ˆâ–â–†
wandb: grad_mean/lif_layers.1 â–ˆâ–‡â–
wandb: grad_mean/lif_layers.2 â–ˆâ–‡â–
wandb: grad_mean/lif_layers.3 â–ˆâ–‡â–
wandb: grad_mean/lif_layers.4 â–ˆâ–‡â–
wandb: grad_mean/lif_layers.5 â–ˆâ–‡â–
wandb: grad_mean/lif_layers.6 â–ˆâ–‡â–
wandb: grad_mean/lif_layers.7 â–ˆâ–‡â–
wandb: grad_mean/output_layer â–ˆâ–‡â–
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.0
wandb: grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.2 0.0
wandb: grad_mean/lif_layers.3 0.0
wandb: grad_mean/lif_layers.4 0.0
wandb: grad_mean/lif_layers.5 0.0
wandb: grad_mean/lif_layers.6 0.0
wandb: grad_mean/lif_layers.7 0.0
wandb: grad_mean/output_layer 0.00148
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run wild-sweep-3 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/fj9t18ye
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_165538-fj9t18ye/logs
wandb: Agent Starting Run: uo3ct3ur with config:
wandb: 	alpha: 6
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_170403-uo3ct3ur
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-4
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/uo3ct3ur
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 466 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
466 K     Trainable params
0         Non-trainable params
466 K     Total params
1.866     Total estimated model params size (MB)
19        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–ˆâ–‡â–
wandb:   grad_mean/lif_layers.1 â–‡â–ˆâ–
wandb:   grad_mean/lif_layers.2 â–‡â–ˆâ–
wandb:   grad_mean/lif_layers.3 â–‡â–ˆâ–
wandb:   grad_mean/output_layer â–‡â–ˆâ–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/lif_layers.2 0.0
wandb:   grad_mean/lif_layers.3 0.0
wandb:   grad_mean/output_layer 0.00019
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb: spikes/train_avg_layer_2 0
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run rose-sweep-4 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/uo3ct3ur
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_170403-uo3ct3ur/logs
wandb: Agent Starting Run: vgsv5xjc with config:
wandb: 	alpha: 2
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 32
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_174929-vgsv5xjc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-5
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/vgsv5xjc
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 13.7 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 33     | train
--------------------------------------------------------
13.7 K    Trainable params
0         Non-trainable params
13.7 K    Total params
0.055     Total estimated model params size (MB)
13        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–†â–â–ˆ
wandb:   grad_mean/lif_layers.1 â–â–„â–ˆ
wandb:   grad_mean/output_layer â–â–„â–ˆ
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–†â–…â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/output_layer 0.0003
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb:               train_loss 0.16247
wandb:      trainer/global_step 279
wandb:                 val_loss 0.16158
wandb: 
wandb: ğŸš€ View run glowing-sweep-5 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/vgsv5xjc
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_174929-vgsv5xjc/logs
wandb: Agent Starting Run: gdizyqau with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_175327-gdizyqau
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-6
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/gdizyqau
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 994 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
994 K     Trainable params
0         Non-trainable params
994 K     Total params
3.980     Total estimated model params size (MB)
31        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–â–…â–ˆ
wandb: grad_mean/lif_layers.1 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.2 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.3 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.4 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.5 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.6 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.7 â–ˆâ–ƒâ–
wandb: grad_mean/output_layer â–ˆâ–ƒâ–
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.0
wandb: grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.2 0.0
wandb: grad_mean/lif_layers.3 0.0
wandb: grad_mean/lif_layers.4 0.0
wandb: grad_mean/lif_layers.5 0.0
wandb: grad_mean/lif_layers.6 0.0
wandb: grad_mean/lif_layers.7 0.0
wandb: grad_mean/output_layer 0.00075
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run logical-sweep-6 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/gdizyqau
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_175327-gdizyqau/logs
wandb: Agent Starting Run: 2xz97wan with config:
wandb: 	alpha: 2
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 32
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_192729-2xz97wan
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-sweep-7
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/2xz97wan
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 64.4 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 33     | train
--------------------------------------------------------
64.4 K    Trainable params
0         Non-trainable params
64.4 K    Total params
0.258     Total estimated model params size (MB)
31        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–ˆâ–…â–
wandb: grad_mean/lif_layers.1 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.2 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.3 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.4 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.5 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.6 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.7 â–ˆâ–â–ƒ
wandb: grad_mean/output_layer â–ˆâ–â–ƒ
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.0
wandb: grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.2 0.0
wandb: grad_mean/lif_layers.3 0.0
wandb: grad_mean/lif_layers.4 0.0
wandb: grad_mean/lif_layers.5 0.0
wandb: grad_mean/lif_layers.6 0.0
wandb: grad_mean/lif_layers.7 0.0
wandb: grad_mean/output_layer 0.00174
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run apricot-sweep-7 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/2xz97wan
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_192729-2xz97wan/logs
wandb: Agent Starting Run: xodfk8ly with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 32
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: setting up run xodfk8ly
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_194119-xodfk8ly
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-sweep-8
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/xodfk8ly
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 131 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 33     | train
--------------------------------------------------------
132 K     Trainable params
0         Non-trainable params
132 K     Total params
0.528     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–â–ˆâ–‚
wandb:  grad_mean/lif_layers.1 â–ˆâ–â–‚
wandb: grad_mean/lif_layers.10 â–ˆâ–â–‚
wandb: grad_mean/lif_layers.11 â–ˆâ–â–‚
wandb: grad_mean/lif_layers.12 â–ˆâ–â–‚
wandb: grad_mean/lif_layers.13 â–ˆâ–â–‚
wandb: grad_mean/lif_layers.14 â–ˆâ–â–‚
wandb: grad_mean/lif_layers.15 â–ˆâ–â–‚
wandb:  grad_mean/lif_layers.2 â–ˆâ–â–‚
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 0.0
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run fast-sweep-8 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/xodfk8ly
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_194119-xodfk8ly/logs
wandb: Agent Starting Run: fd23wc1p with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 8
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_200906-fd23wc1p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-sweep-9
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/fd23wc1p
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 4.6 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 9      | train
--------------------------------------------------------
4.6 K     Trainable params
0         Non-trainable params
4.6 K     Total params
0.018     Total estimated model params size (MB)
31        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.1 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.2 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.3 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.4 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.5 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.6 â–ˆâ–„â–
wandb: grad_mean/lif_layers.7 â–ˆâ–„â–
wandb: grad_mean/output_layer â–ˆâ–„â–
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.0
wandb: grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.2 0.0
wandb: grad_mean/lif_layers.3 0.0
wandb: grad_mean/lif_layers.4 0.0
wandb: grad_mean/lif_layers.5 0.0
wandb: grad_mean/lif_layers.6 0.0
wandb: grad_mean/lif_layers.7 1e-05
wandb: grad_mean/output_layer 0.00646
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run dashing-sweep-9 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/fd23wc1p
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_200906-fd23wc1p/logs
wandb: Agent Starting Run: co43cwp7 with config:
wandb: 	alpha: 6
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: setting up run co43cwp7
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_201520-co43cwp7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-10
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/co43cwp7
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 202 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
202 K     Trainable params
0         Non-trainable params
202 K     Total params
0.810     Total estimated model params size (MB)
13        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–„â–ˆ
wandb:   grad_mean/lif_layers.1 â–ˆâ–„â–
wandb:   grad_mean/output_layer â–ˆâ–„â–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–†â–…â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/output_layer 0.0003
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb:               train_loss 0.16247
wandb:      trainer/global_step 279
wandb:                 val_loss 0.16158
wandb: 
wandb: ğŸš€ View run deep-sweep-10 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/co43cwp7
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_201520-co43cwp7/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: nfed3rl2 with config:
wandb: 	alpha: 8
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 32
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: setting up run nfed3rl2
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_203608-nfed3rl2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-11
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/nfed3rl2
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 13.7 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 33     | train
--------------------------------------------------------
13.7 K    Trainable params
0         Non-trainable params
13.7 K    Total params
0.055     Total estimated model params size (MB)
13        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 79-82, summary, console lines 38-38
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–ˆâ–â–‚
wandb:   grad_mean/lif_layers.1 â–â–ˆâ–…
wandb:   grad_mean/output_layer â–â–ˆâ–…
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/output_layer 0.00382
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb:               train_loss 0.16247
wandb:      trainer/global_step 279
wandb:                 val_loss 0.16159
wandb: 
wandb: ğŸš€ View run eager-sweep-11 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/nfed3rl2
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_203608-nfed3rl2/logs
wandb: Agent Starting Run: mgc6nqpa with config:
wandb: 	alpha: 6
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_203950-mgc6nqpa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-sweep-12
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/mgc6nqpa
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 517 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
517 K     Trainable params
0         Non-trainable params
517 K     Total params
2.072     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 81-82, summary, console lines 38-38
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–â–ˆâ–…
wandb:  grad_mean/lif_layers.1 â–â–ˆâ–ƒ
wandb: grad_mean/lif_layers.10 â–â–ˆâ–ƒ
wandb: grad_mean/lif_layers.11 â–â–ˆâ–ƒ
wandb: grad_mean/lif_layers.12 â–â–ˆâ–ƒ
wandb: grad_mean/lif_layers.13 â–â–ˆâ–ƒ
wandb: grad_mean/lif_layers.14 â–â–ˆâ–ƒ
wandb: grad_mean/lif_layers.15 â–â–ˆâ–ƒ
wandb:  grad_mean/lif_layers.2 â–â–ˆâ–ƒ
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 0.0
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run youthful-sweep-12 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/mgc6nqpa
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_203950-mgc6nqpa/logs
wandb: Agent Starting Run: tqorduvk with config:
wandb: 	alpha: 8
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: setting up run tqorduvk
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_214505-tqorduvk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-13
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/tqorduvk
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 2.6 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
2.6 K     Trainable params
0         Non-trainable params
2.6 K     Total params
0.011     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–â–ˆâ–…
wandb:  grad_mean/lif_layers.1 â–â–ˆâ–…
wandb: grad_mean/lif_layers.10 â–â–ˆâ–„
wandb: grad_mean/lif_layers.11 â–â–ˆâ–„
wandb: grad_mean/lif_layers.12 â–â–ˆâ–†
wandb: grad_mean/lif_layers.13 â–â–‡â–ˆ
wandb: grad_mean/lif_layers.14 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.15 â–ˆâ–ƒâ–
wandb:  grad_mean/lif_layers.2 â–â–ˆâ–‡
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 0.0001
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run resilient-sweep-13 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/tqorduvk
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_214505-tqorduvk/logs
wandb: Agent Starting Run: 6asqvvgx with config:
wandb: 	alpha: 8
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: setting up run 6asqvvgx
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_215402-6asqvvgx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-14
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/6asqvvgx
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 52.0 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
52.0 K    Trainable params
0         Non-trainable params
52.0 K    Total params
0.208     Total estimated model params size (MB)
13        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–‚â–â–ˆ
wandb:   grad_mean/lif_layers.1 â–‚â–ˆâ–
wandb:   grad_mean/output_layer â–‚â–ˆâ–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/output_layer 0.00198
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb:               train_loss 0.16247
wandb:      trainer/global_step 279
wandb:                 val_loss 0.16158
wandb: 
wandb: ğŸš€ View run dandy-sweep-14 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/6asqvvgx
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_215402-6asqvvgx/logs
wandb: Agent Starting Run: dlan22hb with config:
wandb: 	alpha: 2
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: setting up run dlan22hb
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_220156-dlan22hb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-15
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/dlan22hb
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 34.2 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
34.3 K    Trainable params
0         Non-trainable params
34.3 K    Total params
0.137     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–ƒâ–
wandb:  grad_mean/lif_layers.1 â–â–ˆâ–‚
wandb: grad_mean/lif_layers.10 â–â–ˆâ–‚
wandb: grad_mean/lif_layers.11 â–â–ˆâ–‚
wandb: grad_mean/lif_layers.12 â–â–ˆâ–‚
wandb: grad_mean/lif_layers.13 â–â–ˆâ–‚
wandb: grad_mean/lif_layers.14 â–â–ˆâ–‚
wandb: grad_mean/lif_layers.15 â–â–ˆâ–‚
wandb:  grad_mean/lif_layers.2 â–â–ˆâ–‚
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 0.0
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run winter-sweep-15 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/dlan22hb
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_220156-dlan22hb/logs
wandb: Agent Starting Run: g2x2blli with config:
wandb: 	alpha: 8
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: setting up run g2x2blli
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_221832-g2x2blli
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-16
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/g2x2blli
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 52.0 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
52.0 K    Trainable params
0         Non-trainable params
52.0 K    Total params
0.208     Total estimated model params size (MB)
13        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–…â–ˆ
wandb:   grad_mean/lif_layers.1 â–„â–ˆâ–
wandb:   grad_mean/output_layer â–„â–ˆâ–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–‡â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/output_layer 0.00153
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb:               train_loss 0.16247
wandb:      trainer/global_step 279
wandb:                 val_loss 0.16158
wandb: 
wandb: ğŸš€ View run light-sweep-16 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/g2x2blli
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_221832-g2x2blli/logs
wandb: Agent Starting Run: m4iqth6x with config:
wandb: 	alpha: 2
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_222622-m4iqth6x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-sweep-17
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/m4iqth6x
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 2.6 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
2.6 K     Trainable params
0         Non-trainable params
2.6 K     Total params
0.011     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–â–‚
wandb:  grad_mean/lif_layers.1 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.10 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.11 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.12 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.13 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.14 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.15 â–ˆâ–ƒâ–
wandb:  grad_mean/lif_layers.2 â–ˆâ–ƒâ–
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 0.0
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run helpful-sweep-17 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/m4iqth6x
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_222622-m4iqth6x/logs
wandb: Agent Starting Run: r6emif40 with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_223447-r6emif40
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-sweep-18
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/r6emif40
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 368    | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
387       Trainable params
0         Non-trainable params
387       Total params
0.002     Total estimated model params size (MB)
13        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–ˆâ–‚
wandb:   grad_mean/lif_layers.1 â–ˆâ–â–ˆ
wandb:   grad_mean/output_layer â–ˆâ–â–ˆ
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–†â–‚â–â–‚â–‚â–‚â–â–„â–„â–ƒâ–‚â–â–‚â–ƒâ–„â–†â–ƒâ–ƒâ–‚â–â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–â–â–‚â–ƒâ–‚â–‚â–ƒâ–â–â–†â–â–â–‚â–‚â–â–â–ˆâ–â–â–‚â–‚â–â–‚â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 1e-05
wandb:   grad_mean/output_layer 0.00402
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb:               train_loss 0.16247
wandb:      trainer/global_step 279
wandb:                 val_loss 0.16159
wandb: 
wandb: ğŸš€ View run denim-sweep-18 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/r6emif40
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_223447-r6emif40/logs
wandb: Agent Starting Run: ze727bkf with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: setting up run ze727bkf
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_223612-ze727bkf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-19
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/ze727bkf
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 994 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
994 K     Trainable params
0         Non-trainable params
994 K     Total params
3.980     Total estimated model params size (MB)
31        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: uploading history steps 81-82, summary, console lines 38-38
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–ˆâ–â–…
wandb: grad_mean/lif_layers.1 â–†â–ˆâ–
wandb: grad_mean/lif_layers.2 â–†â–ˆâ–
wandb: grad_mean/lif_layers.3 â–†â–ˆâ–
wandb: grad_mean/lif_layers.4 â–†â–ˆâ–
wandb: grad_mean/lif_layers.5 â–†â–ˆâ–
wandb: grad_mean/lif_layers.6 â–†â–ˆâ–
wandb: grad_mean/lif_layers.7 â–†â–ˆâ–
wandb: grad_mean/output_layer â–†â–ˆâ–
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.0
wandb: grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.2 0.0
wandb: grad_mean/lif_layers.3 0.0
wandb: grad_mean/lif_layers.4 0.0
wandb: grad_mean/lif_layers.5 0.0
wandb: grad_mean/lif_layers.6 0.0
wandb: grad_mean/lif_layers.7 0.0
wandb: grad_mean/output_layer 0.00244
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run exalted-sweep-19 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/ze727bkf
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_223612-ze727bkf/logs
wandb: Agent Starting Run: t5ovfm7z with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_000935-t5ovfm7z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-sweep-20
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/t5ovfm7z
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 34.2 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
34.3 K    Trainable params
0         Non-trainable params
34.3 K    Total params
0.137     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: uploading history steps 81-82, summary, console lines 38-38
wandb: uploading summary, console lines 40-41
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–ƒâ–
wandb:  grad_mean/lif_layers.1 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.10 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.11 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.12 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.13 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.14 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.15 â–ˆâ–ƒâ–
wandb:  grad_mean/lif_layers.2 â–ˆâ–ƒâ–
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 1e-05
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run desert-sweep-20 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/t5ovfm7z
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_000935-t5ovfm7z/logs
wandb: Agent Starting Run: m2q6wv84 with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_002609-m2q6wv84
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-sweep-21
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/m2q6wv84
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 3.8 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
3.8 K     Trainable params
0         Non-trainable params
3.8 K     Total params
0.015     Total estimated model params size (MB)
13        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:   grad_mean/lif_layers.0 â–ˆâ–â–…
wandb:   grad_mean/lif_layers.1 â–ˆâ–â–…
wandb:   grad_mean/output_layer â–ˆâ–â–…
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–‡â–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–‡â–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/output_layer 0.01646
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb:               train_loss 0.16254
wandb:      trainer/global_step 279
wandb:                 val_loss 0.16162
wandb: 
wandb: ğŸš€ View run denim-sweep-21 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/m2q6wv84
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_002609-m2q6wv84/logs
wandb: Agent Starting Run: 7490lrd1 with config:
wandb: 	alpha: 6
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_002843-7490lrd1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-22
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/7490lrd1
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 2.6 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
2.6 K     Trainable params
0         Non-trainable params
2.6 K     Total params
0.011     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–â–ˆâ–‚
wandb:  grad_mean/lif_layers.1 â–ˆâ–„â–
wandb: grad_mean/lif_layers.10 â–ˆâ–„â–
wandb: grad_mean/lif_layers.11 â–ˆâ–„â–
wandb: grad_mean/lif_layers.12 â–ˆâ–„â–
wandb: grad_mean/lif_layers.13 â–ˆâ–„â–
wandb: grad_mean/lif_layers.14 â–ˆâ–„â–
wandb: grad_mean/lif_layers.15 â–ˆâ–„â–
wandb:  grad_mean/lif_layers.2 â–ˆâ–„â–
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 0.0
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run iconic-sweep-22 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/7490lrd1
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_002843-7490lrd1/logs
wandb: Agent Starting Run: ktm3yli5 with config:
wandb: 	alpha: 6
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_003714-ktm3yli5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-sweep-23
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/ktm3yli5
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 2.1 M  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
2.1 M     Trainable params
0         Non-trainable params
2.1 M     Total params
8.207     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–â–ƒ
wandb:  grad_mean/lif_layers.1 â–ˆâ–â–‚
wandb: grad_mean/lif_layers.10 â–ˆâ–â–‚
wandb: grad_mean/lif_layers.11 â–ˆâ–â–‚
wandb: grad_mean/lif_layers.12 â–ˆâ–â–‚
wandb: grad_mean/lif_layers.13 â–ˆâ–â–‚
wandb: grad_mean/lif_layers.14 â–ˆâ–â–‚
wandb: grad_mean/lif_layers.15 â–ˆâ–â–‚
wandb:  grad_mean/lif_layers.2 â–ˆâ–â–‚
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 0.0
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run clear-sweep-23 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/ktm3yli5
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_003714-ktm3yli5/logs
wandb: Agent Starting Run: qzyvoeqq with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 8
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_035124-qzyvoeqq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-sweep-24
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/qzyvoeqq
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 2.3 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 9      | train
--------------------------------------------------------
2.3 K     Trainable params
0         Non-trainable params
2.3 K     Total params
0.009     Total estimated model params size (MB)
19        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–ˆâ–ƒâ–
wandb:   grad_mean/lif_layers.1 â–ˆâ–„â–
wandb:   grad_mean/lif_layers.2 â–ˆâ–„â–
wandb:   grad_mean/lif_layers.3 â–ˆâ–„â–
wandb:   grad_mean/output_layer â–ˆâ–…â–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/lif_layers.2 0.0
wandb:   grad_mean/lif_layers.3 1e-05
wandb:   grad_mean/output_layer 0.02421
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb: spikes/train_avg_layer_2 0
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run clear-sweep-24 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/qzyvoeqq
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_035124-qzyvoeqq/logs
wandb: Agent Starting Run: mnum5zzh with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_035450-mnum5zzh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-sweep-25
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/mnum5zzh
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 251 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
251 K     Trainable params
0         Non-trainable params
251 K     Total params
1.007     Total estimated model params size (MB)
31        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–ƒâ–ˆâ–
wandb: grad_mean/lif_layers.1 â–ˆâ–â–„
wandb: grad_mean/lif_layers.2 â–ˆâ–â–„
wandb: grad_mean/lif_layers.3 â–ˆâ–â–„
wandb: grad_mean/lif_layers.4 â–ˆâ–â–„
wandb: grad_mean/lif_layers.5 â–ˆâ–â–…
wandb: grad_mean/lif_layers.6 â–ˆâ–â–„
wandb: grad_mean/lif_layers.7 â–ˆâ–â–„
wandb: grad_mean/output_layer â–ˆâ–â–„
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.0
wandb: grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.2 0.0
wandb: grad_mean/lif_layers.3 0.0
wandb: grad_mean/lif_layers.4 0.0
wandb: grad_mean/lif_layers.5 0.0
wandb: grad_mean/lif_layers.6 0.0
wandb: grad_mean/lif_layers.7 0.0
wandb: grad_mean/output_layer 0.00222
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run blooming-sweep-25 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/mnum5zzh
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_035450-mnum5zzh/logs
wandb: Agent Starting Run: giu9hzzk with config:
wandb: 	alpha: 6
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_042726-giu9hzzk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sweep-26
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/giu9hzzk
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 34.2 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
34.3 K    Trainable params
0         Non-trainable params
34.3 K    Total params
0.137     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–‚â–
wandb:  grad_mean/lif_layers.1 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.10 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.11 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.12 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.13 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.14 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.15 â–ˆâ–ƒâ–
wandb:  grad_mean/lif_layers.2 â–ˆâ–‚â–
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 0.0
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run comic-sweep-26 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/giu9hzzk
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_042726-giu9hzzk/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: l96r2nwt with config:
wandb: 	alpha: 2
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_044420-l96r2nwt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-sweep-27
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/l96r2nwt
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 1.3 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
1.3 K     Trainable params
0         Non-trainable params
1.3 K     Total params
0.005     Total estimated model params size (MB)
31        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb: grad_mean/lif_layers.0 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.1 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.2 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.3 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.4 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.5 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.6 â–ˆâ–â–ƒ
wandb: grad_mean/lif_layers.7 â–ˆâ–â–ƒ
wandb: grad_mean/output_layer â–ˆâ–â–ƒ
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.0
wandb: grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.2 0.0
wandb: grad_mean/lif_layers.3 0.0
wandb: grad_mean/lif_layers.4 0.0
wandb: grad_mean/lif_layers.5 0.0
wandb: grad_mean/lif_layers.6 0.0
wandb: grad_mean/lif_layers.7 2e-05
wandb: grad_mean/output_layer 0.00463
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run smart-sweep-27 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/l96r2nwt
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_044420-l96r2nwt/logs
wandb: Agent Starting Run: sru46l5k with config:
wandb: 	alpha: 6
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_044848-sru46l5k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-28
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/sru46l5k
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 251 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
251 K     Trainable params
0         Non-trainable params
251 K     Total params
1.007     Total estimated model params size (MB)
31        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 81-82, summary, console lines 38-38
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: grad_mean/lif_layers.0 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.1 â–ˆâ–…â–
wandb: grad_mean/lif_layers.2 â–ˆâ–…â–
wandb: grad_mean/lif_layers.3 â–ˆâ–…â–
wandb: grad_mean/lif_layers.4 â–ˆâ–…â–
wandb: grad_mean/lif_layers.5 â–ˆâ–…â–
wandb: grad_mean/lif_layers.6 â–ˆâ–…â–
wandb: grad_mean/lif_layers.7 â–ˆâ–…â–
wandb: grad_mean/output_layer â–ˆâ–…â–
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.0
wandb: grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.2 0.0
wandb: grad_mean/lif_layers.3 0.0
wandb: grad_mean/lif_layers.4 0.0
wandb: grad_mean/lif_layers.5 0.0
wandb: grad_mean/lif_layers.6 0.0
wandb: grad_mean/lif_layers.7 0.0
wandb: grad_mean/output_layer 0.00741
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run silver-sweep-28 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/sru46l5k
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_044848-sru46l5k/logs
wandb: Agent Starting Run: r55ywr7v with config:
wandb: 	alpha: 6
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: setting up run r55ywr7v
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_052058-r55ywr7v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-29
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/r55ywr7v
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 368    | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
387       Trainable params
0         Non-trainable params
387       Total params
0.002     Total estimated model params size (MB)
13        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:   grad_mean/lif_layers.0 â–â–†â–ˆ
wandb:   grad_mean/lif_layers.1 â–„â–â–ˆ
wandb:   grad_mean/output_layer â–„â–â–ˆ
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–†â–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–†â–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 2e-05
wandb:   grad_mean/output_layer 0.00471
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb:               train_loss 0.16247
wandb:      trainer/global_step 279
wandb:                 val_loss 0.16159
wandb: 
wandb: ğŸš€ View run sparkling-sweep-29 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/r55ywr7v
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_052058-r55ywr7v/logs
wandb: Agent Starting Run: d2ggdgz2 with config:
wandb: 	alpha: 8
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_052242-d2ggdgz2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-30
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/d2ggdgz2
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 16.8 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
16.9 K    Trainable params
0         Non-trainable params
16.9 K    Total params
0.067     Total estimated model params size (MB)
31        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 81-82, summary, console lines 38-38
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.1 â–ˆâ–…â–
wandb: grad_mean/lif_layers.2 â–ˆâ–…â–
wandb: grad_mean/lif_layers.3 â–ˆâ–…â–
wandb: grad_mean/lif_layers.4 â–ˆâ–…â–
wandb: grad_mean/lif_layers.5 â–ˆâ–…â–
wandb: grad_mean/lif_layers.6 â–ˆâ–…â–
wandb: grad_mean/lif_layers.7 â–ˆâ–†â–
wandb: grad_mean/output_layer â–ˆâ–†â–
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.0
wandb: grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.2 0.0
wandb: grad_mean/lif_layers.3 0.0
wandb: grad_mean/lif_layers.4 0.0
wandb: grad_mean/lif_layers.5 0.0
wandb: grad_mean/lif_layers.6 0.0
wandb: grad_mean/lif_layers.7 0.0
wandb: grad_mean/output_layer 0.01129
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run honest-sweep-30 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/d2ggdgz2
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_052242-d2ggdgz2/logs
wandb: Agent Starting Run: 1p0juugt with config:
wandb: 	alpha: 6
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 8
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_053108-1p0juugt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-31
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/1p0juugt
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 9.2 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 9      | train
--------------------------------------------------------
9.2 K     Trainable params
0         Non-trainable params
9.2 K     Total params
0.037     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–ˆâ–
wandb:  grad_mean/lif_layers.1 â–ˆâ–†â–
wandb: grad_mean/lif_layers.10 â–ˆâ–†â–
wandb: grad_mean/lif_layers.11 â–ˆâ–†â–
wandb: grad_mean/lif_layers.12 â–ˆâ–†â–
wandb: grad_mean/lif_layers.13 â–ˆâ–†â–
wandb: grad_mean/lif_layers.14 â–ˆâ–†â–
wandb: grad_mean/lif_layers.15 â–ˆâ–†â–
wandb:  grad_mean/lif_layers.2 â–ˆâ–†â–
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 0.0
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run kind-sweep-31 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/1p0juugt
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_053108-1p0juugt/logs
wandb: Agent Starting Run: eef82qvg with config:
wandb: 	alpha: 2
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 32
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_054313-eef82qvg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-32
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/eef82qvg
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 13.7 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 33     | train
--------------------------------------------------------
13.7 K    Trainable params
0         Non-trainable params
13.7 K    Total params
0.055     Total estimated model params size (MB)
13        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 77-82, summary, console lines 38-38
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–ˆâ–
wandb:   grad_mean/lif_layers.1 â–â–ˆâ–†
wandb:   grad_mean/output_layer â–â–ˆâ–†
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/output_layer 0.0024
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb:               train_loss 0.16247
wandb:      trainer/global_step 279
wandb:                 val_loss 0.16158
wandb: 
wandb: ğŸš€ View run light-sweep-32 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/eef82qvg
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_054313-eef82qvg/logs
wandb: Agent Starting Run: seit9q1o with config:
wandb: 	alpha: 8
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: setting up run seit9q1o
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_054659-seit9q1o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-33
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/seit9q1o
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 16.8 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
16.9 K    Trainable params
0         Non-trainable params
16.9 K    Total params
0.067     Total estimated model params size (MB)
31        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.1 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.2 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.3 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.4 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.5 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.6 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.7 â–ˆâ–‚â–
wandb: grad_mean/output_layer â–ˆâ–‚â–
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.0
wandb: grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.2 0.0
wandb: grad_mean/lif_layers.3 0.0
wandb: grad_mean/lif_layers.4 0.0
wandb: grad_mean/lif_layers.5 0.0
wandb: grad_mean/lif_layers.6 0.0
wandb: grad_mean/lif_layers.7 1e-05
wandb: grad_mean/output_layer 0.05218
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run sleek-sweep-33 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/seit9q1o
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_054659-seit9q1o/logs
wandb: Agent Starting Run: 4vbkgx4w with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_055529-4vbkgx4w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-sweep-34
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/4vbkgx4w
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 3.8 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
3.8 K     Trainable params
0         Non-trainable params
3.8 K     Total params
0.015     Total estimated model params size (MB)
13        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–ˆâ–„
wandb:   grad_mean/lif_layers.1 â–â–ˆâ–ƒ
wandb:   grad_mean/output_layer â–â–ˆâ–ƒ
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:                 val_loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 1e-05
wandb:   grad_mean/output_layer 0.00394
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb:               train_loss 0.16247
wandb:      trainer/global_step 279
wandb:                 val_loss 0.16159
wandb: 
wandb: ğŸš€ View run dulcet-sweep-34 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/4vbkgx4w
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_055529-4vbkgx4w/logs
wandb: Agent Starting Run: hrlbn0pf with config:
wandb: 	alpha: 8
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_055758-hrlbn0pf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-35
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/hrlbn0pf
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 688    | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
707       Trainable params
0         Non-trainable params
707       Total params
0.003     Total estimated model params size (MB)
19        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–ˆâ–‚â–
wandb:   grad_mean/lif_layers.1 â–ˆâ–â–
wandb:   grad_mean/lif_layers.2 â–ˆâ–â–
wandb:   grad_mean/lif_layers.3 â–ˆâ–â–‚
wandb:   grad_mean/output_layer â–ˆâ–â–ƒ
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/lif_layers.2 0.0
wandb:   grad_mean/lif_layers.3 6e-05
wandb:   grad_mean/output_layer 0.03467
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb: spikes/train_avg_layer_2 0
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run wandering-sweep-35 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/hrlbn0pf
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_055758-hrlbn0pf/logs
wandb: Agent Starting Run: 7y1wy0di with config:
wandb: 	alpha: 6
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 32
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_060028-7y1wy0di
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sweep-36
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/7y1wy0di
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 30.6 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 33     | train
--------------------------------------------------------
30.6 K    Trainable params
0         Non-trainable params
30.6 K    Total params
0.123     Total estimated model params size (MB)
19        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–ˆâ–
wandb:   grad_mean/lif_layers.1 â–„â–â–ˆ
wandb:   grad_mean/lif_layers.2 â–„â–â–ˆ
wandb:   grad_mean/lif_layers.3 â–„â–â–ˆ
wandb:   grad_mean/output_layer â–„â–â–ˆ
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/lif_layers.2 0.0
wandb:   grad_mean/lif_layers.3 0.0
wandb:   grad_mean/output_layer 0.00526
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb: spikes/train_avg_layer_2 0
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run comic-sweep-36 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/7y1wy0di
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_060028-7y1wy0di/logs
wandb: Agent Starting Run: s3b9i500 with config:
wandb: 	alpha: 2
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 8
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_060738-s3b9i500
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-37
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/s3b9i500
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 2.3 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 9      | train
--------------------------------------------------------
2.3 K     Trainable params
0         Non-trainable params
2.3 K     Total params
0.009     Total estimated model params size (MB)
19        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–ˆâ–‚â–
wandb:   grad_mean/lif_layers.1 â–ˆâ–‚â–
wandb:   grad_mean/lif_layers.2 â–ˆâ–‚â–
wandb:   grad_mean/lif_layers.3 â–ˆâ–ƒâ–
wandb:   grad_mean/output_layer â–ˆâ–„â–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/lif_layers.2 0.0
wandb:   grad_mean/lif_layers.3 3e-05
wandb:   grad_mean/output_layer 0.03491
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb: spikes/train_avg_layer_2 0
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run magic-sweep-37 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/s3b9i500
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_060738-s3b9i500/logs
wandb: Agent Starting Run: mjkdp5lx with config:
wandb: 	alpha: 8
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_061059-mjkdp5lx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-sweep-38
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/mjkdp5lx
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 2.1 M  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
2.1 M     Trainable params
0         Non-trainable params
2.1 M     Total params
8.207     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)
wandb: Network error (HTTPError), entering retry loop.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: uploading console lines 40-40
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–ƒâ–â–ˆ
wandb:  grad_mean/lif_layers.1 â–„â–â–ˆ
wandb: grad_mean/lif_layers.10 â–„â–â–ˆ
wandb: grad_mean/lif_layers.11 â–„â–â–ˆ
wandb: grad_mean/lif_layers.12 â–„â–â–ˆ
wandb: grad_mean/lif_layers.13 â–„â–â–ˆ
wandb: grad_mean/lif_layers.14 â–„â–â–ˆ
wandb: grad_mean/lif_layers.15 â–„â–â–ˆ
wandb:  grad_mean/lif_layers.2 â–„â–â–ˆ
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 0.0
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run dashing-sweep-38 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/mjkdp5lx
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_061059-mjkdp5lx/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 3iuws10h with config:
wandb: 	alpha: 6
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_092513-3iuws10h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-sweep-39
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/3iuws10h
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 52.0 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
52.0 K    Trainable params
0         Non-trainable params
52.0 K    Total params
0.208     Total estimated model params size (MB)
13        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 81-82, summary, console lines 38-38
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–ƒâ–â–ˆ
wandb:   grad_mean/lif_layers.1 â–â–ˆâ–ˆ
wandb:   grad_mean/output_layer â–â–ˆâ–ˆ
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/output_layer 0.00223
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb:               train_loss 0.16247
wandb:      trainer/global_step 279
wandb:                 val_loss 0.16158
wandb: 
wandb: ğŸš€ View run jumping-sweep-39 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/3iuws10h
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_092513-3iuws10h/logs
wandb: Agent Starting Run: utgld4ua with config:
wandb: 	alpha: 2
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 8
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_093345-utgld4ua
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-40
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/utgld4ua
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 4.6 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 9      | train
--------------------------------------------------------
4.6 K     Trainable params
0         Non-trainable params
4.6 K     Total params
0.018     Total estimated model params size (MB)
31        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.1 â–ˆâ–â–
wandb: grad_mean/lif_layers.2 â–ˆâ–â–
wandb: grad_mean/lif_layers.3 â–ˆâ–â–
wandb: grad_mean/lif_layers.4 â–ˆâ–â–
wandb: grad_mean/lif_layers.5 â–ˆâ–â–
wandb: grad_mean/lif_layers.6 â–ˆâ–â–
wandb: grad_mean/lif_layers.7 â–ˆâ–â–
wandb: grad_mean/output_layer â–ˆâ–â–
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.0
wandb: grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.2 0.0
wandb: grad_mean/lif_layers.3 0.0
wandb: grad_mean/lif_layers.4 0.0
wandb: grad_mean/lif_layers.5 0.0
wandb: grad_mean/lif_layers.6 0.0
wandb: grad_mean/lif_layers.7 1e-05
wandb: grad_mean/output_layer 0.00195
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run distinctive-sweep-40 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/utgld4ua
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_093345-utgld4ua/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 3lydw8bj with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_093958-3lydw8bj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-sweep-41
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/3lydw8bj
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 2.6 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
2.6 K     Trainable params
0         Non-trainable params
2.6 K     Total params
0.011     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–ƒâ–
wandb:  grad_mean/lif_layers.1 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.10 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.11 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.12 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.13 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.14 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.15 â–ˆâ–ƒâ–
wandb:  grad_mean/lif_layers.2 â–ˆâ–‚â–
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 0.00012
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run desert-sweep-41 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/3lydw8bj
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_093958-3lydw8bj/logs
wandb: Agent Starting Run: b6lzqxh3 with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 8
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: setting up run b6lzqxh3
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_094828-b6lzqxh3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-sweep-42
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/b6lzqxh3
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 1.1 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 9      | train
--------------------------------------------------------
1.1 K     Trainable params
0         Non-trainable params
1.1 K     Total params
0.005     Total estimated model params size (MB)
13        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–ˆâ–ƒâ–
wandb:   grad_mean/lif_layers.1 â–ˆâ–ƒâ–
wandb:   grad_mean/output_layer â–ˆâ–ƒâ–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–‡â–†â–†â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–‡â–†â–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 5e-05
wandb:   grad_mean/output_layer 0.09494
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb:               train_loss 0.17155
wandb:      trainer/global_step 279
wandb:                 val_loss 0.1702
wandb: 
wandb: ğŸš€ View run eternal-sweep-42 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/b6lzqxh3
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_094828-b6lzqxh3/logs
wandb: Agent Starting Run: nqg05uqc with config:
wandb: 	alpha: 8
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 8
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_095022-nqg05uqc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-43
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/nqg05uqc
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 9.2 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 9      | train
--------------------------------------------------------
9.2 K     Trainable params
0         Non-trainable params
9.2 K     Total params
0.037     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–‚â–
wandb:  grad_mean/lif_layers.1 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.10 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.11 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.12 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.13 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.14 â–ˆâ–„â–
wandb: grad_mean/lif_layers.15 â–ˆâ–…â–
wandb:  grad_mean/lif_layers.2 â–ˆâ–‚â–
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 1e-05
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run whole-sweep-43 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/nqg05uqc
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_095022-nqg05uqc/logs
wandb: Agent Starting Run: 52nq6lhk with config:
wandb: 	alpha: 2
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 8
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_100222-52nq6lhk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-44
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/52nq6lhk
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 9.2 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 9      | train
--------------------------------------------------------
9.2 K     Trainable params
0         Non-trainable params
9.2 K     Total params
0.037     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                   epoch â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–‚â–
wandb:  grad_mean/lif_layers.1 â–â–†â–ˆ
wandb: grad_mean/lif_layers.10 â–â–†â–ˆ
wandb: grad_mean/lif_layers.11 â–â–†â–ˆ
wandb: grad_mean/lif_layers.12 â–â–†â–ˆ
wandb: grad_mean/lif_layers.13 â–â–†â–ˆ
wandb: grad_mean/lif_layers.14 â–â–†â–ˆ
wandb: grad_mean/lif_layers.15 â–â–†â–ˆ
wandb:  grad_mean/lif_layers.2 â–â–†â–ˆ
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 3e-05
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run royal-sweep-44 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/52nq6lhk
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_100222-52nq6lhk/logs
wandb: Agent Starting Run: r3ozb2i0 with config:
wandb: 	alpha: 2
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 8
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_101402-r3ozb2i0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-45
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/r3ozb2i0
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 1.1 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 9      | train
--------------------------------------------------------
1.1 K     Trainable params
0         Non-trainable params
1.1 K     Total params
0.005     Total estimated model params size (MB)
13        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 77-82, summary, console lines 38-38
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–ˆâ–ƒ
wandb:   grad_mean/lif_layers.1 â–â–ˆâ–‚
wandb:   grad_mean/output_layer â–â–ˆâ–‚
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–…â–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/output_layer 0.00148
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb:               train_loss 0.16247
wandb:      trainer/global_step 279
wandb:                 val_loss 0.16159
wandb: 
wandb: ğŸš€ View run celestial-sweep-45 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/r3ozb2i0
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_101402-r3ozb2i0/logs
wandb: Agent Starting Run: 3n76ljhq with config:
wandb: 	alpha: 2
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: setting up run 3n76ljhq
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_101556-3n76ljhq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-46
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/3n76ljhq
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 688    | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
707       Trainable params
0         Non-trainable params
707       Total params
0.003     Total estimated model params size (MB)
19        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 79-82, summary, console lines 38-38
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–ˆâ–…â–
wandb:   grad_mean/lif_layers.1 â–ˆâ–‡â–
wandb:   grad_mean/lif_layers.2 â–ˆâ–ƒâ–
wandb:   grad_mean/lif_layers.3 â–ˆâ–‚â–
wandb:   grad_mean/output_layer â–ˆâ–ƒâ–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_2 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/lif_layers.2 1e-05
wandb:   grad_mean/lif_layers.3 0.00013
wandb:   grad_mean/output_layer 0.07644
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb: spikes/train_avg_layer_2 0
wandb:                       +4 ...
wandb: 
wandb: ğŸš€ View run efficient-sweep-46 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/3n76ljhq
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_101556-3n76ljhq/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: psmeawrm with config:
wandb: 	alpha: 8
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_101831-psmeawrm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-47
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/psmeawrm
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 517 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
517 K     Trainable params
0         Non-trainable params
517 K     Total params
2.072     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 81-82, summary, console lines 38-38
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–†â–ˆâ–
wandb:  grad_mean/lif_layers.1 â–ˆâ–â–
wandb: grad_mean/lif_layers.10 â–ˆâ–â–
wandb: grad_mean/lif_layers.11 â–ˆâ–â–
wandb: grad_mean/lif_layers.12 â–ˆâ–â–
wandb: grad_mean/lif_layers.13 â–ˆâ–â–
wandb: grad_mean/lif_layers.14 â–ˆâ–â–
wandb: grad_mean/lif_layers.15 â–ˆâ–â–‚
wandb:  grad_mean/lif_layers.2 â–ˆâ–â–
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 0.0
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run grateful-sweep-47 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/psmeawrm
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_101831-psmeawrm/logs
wandb: Agent Starting Run: rce9st58 with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_112345-rce9st58
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-48
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/rce9st58
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 16.8 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
16.9 K    Trainable params
0         Non-trainable params
16.9 K    Total params
0.067     Total estimated model params size (MB)
31        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–â–ˆâ–†
wandb: grad_mean/lif_layers.1 â–â–‚â–ˆ
wandb: grad_mean/lif_layers.2 â–â–‚â–ˆ
wandb: grad_mean/lif_layers.3 â–â–‚â–ˆ
wandb: grad_mean/lif_layers.4 â–â–‚â–ˆ
wandb: grad_mean/lif_layers.5 â–â–‚â–ˆ
wandb: grad_mean/lif_layers.6 â–â–‚â–ˆ
wandb: grad_mean/lif_layers.7 â–â–‚â–ˆ
wandb: grad_mean/output_layer â–â–‚â–ˆ
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.0
wandb: grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.2 0.0
wandb: grad_mean/lif_layers.3 0.0
wandb: grad_mean/lif_layers.4 0.0
wandb: grad_mean/lif_layers.5 0.0
wandb: grad_mean/lif_layers.6 0.0
wandb: grad_mean/lif_layers.7 0.0
wandb: grad_mean/output_layer 0.00424
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run serene-sweep-48 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/rce9st58
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_112345-rce9st58/logs
wandb: Agent Starting Run: jqmprj09 with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: setting up run jqmprj09
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_113221-jqmprj09
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-49
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/jqmprj09
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 1.3 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
1.3 K     Trainable params
0         Non-trainable params
1.3 K     Total params
0.005     Total estimated model params size (MB)
31        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                  epoch â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: grad_mean/lif_layers.0 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.1 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.2 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.3 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.4 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.5 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.6 â–ˆâ–ƒâ–
wandb: grad_mean/lif_layers.7 â–ˆâ–„â–
wandb: grad_mean/output_layer â–ˆâ–„â–
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 39
wandb: grad_mean/lif_layers.0 0.0
wandb: grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.2 0.0
wandb: grad_mean/lif_layers.3 0.0
wandb: grad_mean/lif_layers.4 0.0
wandb: grad_mean/lif_layers.5 0.0
wandb: grad_mean/lif_layers.6 0.0
wandb: grad_mean/lif_layers.7 0.0
wandb: grad_mean/output_layer 3e-05
wandb:                    +12 ...
wandb: 
wandb: ğŸš€ View run snowy-sweep-49 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/jqmprj09
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_113221-jqmprj09/logs
wandb: Agent Starting Run: y22v10u8 with config:
wandb: 	alpha: 8
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 8
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_113649-y22v10u8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-50
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/05tr1yfx
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/y22v10u8
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 1.1 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 9      | train
--------------------------------------------------------
1.1 K     Trainable params
0         Non-trainable params
1.1 K     Total params
0.005     Total estimated model params size (MB)
13        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–ˆâ–ƒâ–
wandb:   grad_mean/lif_layers.1 â–ˆâ–ƒâ–
wandb:   grad_mean/output_layer â–ˆâ–ƒâ–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–‡â–†â–…â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–‡â–†â–…â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 4e-05
wandb:   grad_mean/output_layer 0.05086
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb:               train_loss 0.16502
wandb:      trainer/global_step 279
wandb:                 val_loss 0.1639
wandb: 
wandb: ğŸš€ View run hearty-sweep-50 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/y22v10u8
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251111_113649-y22v10u8/logs
