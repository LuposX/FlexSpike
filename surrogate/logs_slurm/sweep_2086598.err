/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
wandb: Agent Starting Run: 3uuv5oj6 with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Currently logged in as: lupos to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 3uuv5oj6
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_114150-3uuv5oj6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-1
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/i3bwt4c8
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/3uuv5oj6
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 517 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
517 K     Trainable params
0         Non-trainable params
517 K     Total params
2.072     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: uploading console lines 40-41
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–â–ƒ
wandb:  grad_mean/lif_layers.1 â–ˆâ–â–„
wandb: grad_mean/lif_layers.10 â–ˆâ–â–„
wandb: grad_mean/lif_layers.11 â–ˆâ–â–„
wandb: grad_mean/lif_layers.12 â–ˆâ–â–„
wandb: grad_mean/lif_layers.13 â–ˆâ–â–„
wandb: grad_mean/lif_layers.14 â–ˆâ–â–„
wandb: grad_mean/lif_layers.15 â–ˆâ–â–„
wandb:  grad_mean/lif_layers.2 â–ˆâ–â–„
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 0.0
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run glad-sweep-1 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/3uuv5oj6
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_114150-3uuv5oj6/logs
wandb: Agent Starting Run: wet5ngoc with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_124658-wet5ngoc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-2
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/i3bwt4c8
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/wet5ngoc
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 2.6 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
2.6 K     Trainable params
0         Non-trainable params
2.6 K     Total params
0.011     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–‚â–
wandb:  grad_mean/lif_layers.1 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.10 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.11 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.12 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.13 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.14 â–ƒâ–â–ˆ
wandb: grad_mean/lif_layers.15 â–ˆâ–‚â–
wandb:  grad_mean/lif_layers.2 â–ˆâ–‚â–
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 0.00033
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run wobbly-sweep-2 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/wet5ngoc
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_124658-wet5ngoc/logs
wandb: Agent Starting Run: wm73603f with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.8
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 32
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_125545-wm73603f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-3
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/i3bwt4c8
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/wm73603f
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 131 K  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 33     | train
--------------------------------------------------------
132 K     Trainable params
0         Non-trainable params
132 K     Total params
0.528     Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: 
wandb: Run history:
wandb:                   epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  grad_mean/lif_layers.0 â–ˆâ–‚â–
wandb:  grad_mean/lif_layers.1 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.10 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.11 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.12 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.13 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.14 â–ˆâ–‚â–
wandb: grad_mean/lif_layers.15 â–ˆâ–‚â–
wandb:  grad_mean/lif_layers.2 â–ˆâ–‚â–
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 39
wandb:  grad_mean/lif_layers.0 0.0
wandb:  grad_mean/lif_layers.1 0.0
wandb: grad_mean/lif_layers.10 0.0
wandb: grad_mean/lif_layers.11 0.0
wandb: grad_mean/lif_layers.12 0.0
wandb: grad_mean/lif_layers.13 0.0
wandb: grad_mean/lif_layers.14 0.0
wandb: grad_mean/lif_layers.15 0.0
wandb:  grad_mean/lif_layers.2 0.0
wandb:                     +28 ...
wandb: 
wandb: ğŸš€ View run daily-sweep-3 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/wm73603f
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_125545-wm73603f/logs
wandb: Agent Starting Run: 5tkzvdyt with config:
wandb: 	alpha: 4
wandb: 	batch_size: 2048
wandb: 	beta: 0.5
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: setting up run 5tkzvdyt
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_132343-5tkzvdyt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-4
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/i3bwt4c8
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/5tkzvdyt
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 52.0 K | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
52.0 K    Trainable params
0         Non-trainable params
52.0 K    Total params
0.208     Total estimated model params size (MB)
13        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=40` reached.
wandb: updating run metadata
wandb: uploading history steps 81-82, summary, console lines 38-38
wandb: 
wandb: Run history:
wandb:                    epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   grad_mean/lif_layers.0 â–â–†â–ˆ
wandb:   grad_mean/lif_layers.1 â–ˆâ–„â–
wandb:   grad_mean/output_layer â–ˆâ–„â–
wandb:                       lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_0 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: spikes/train_avg_layer_1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               train_loss â–ˆâ–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      trainer/global_step â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 val_loss â–ˆâ–†â–…â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                    epoch 39
wandb:   grad_mean/lif_layers.0 0.0
wandb:   grad_mean/lif_layers.1 0.0
wandb:   grad_mean/output_layer 0.00091
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0
wandb: spikes/train_avg_layer_1 0
wandb:               train_loss 0.16247
wandb:      trainer/global_step 279
wandb:                 val_loss 0.16158
wandb: 
wandb: ğŸš€ View run serene-sweep-4 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/5tkzvdyt
wandb: â­ï¸ View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_132343-5tkzvdyt/logs
wandb: Agent Starting Run: 0kna9yu8 with config:
wandb: 	alpha: 8
wandb: 	batch_size: 2048
wandb: 	beta: 0.3
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	dropout: 0
wandb: 	epochs: 40
wandb: 	layer_skip: 0
wandb: 	lr: 0.005
wandb: 	num_hidden: 256
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	use_slstm: True
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251110_133142-0kna9yu8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-5
wandb: â­ï¸ View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/i3bwt4c8
wandb: ğŸš€ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/0kna9yu8
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 8.2 M  | train
2 | temp_skip_projs  | ModuleList | 0      | train
3 | layer_skip_projs | ModuleList | 0      | train
4 | layer_bntt       | ModuleList | 0      | train
5 | layer_norms      | ModuleList | 0      | train
6 | output_layer     | Linear     | 257    | train
--------------------------------------------------------
8.2 M     Trainable params
0         Non-trainable params
8.2 M     Total params
32.667    Total estimated model params size (MB)
55        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_slstm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
/var/spool/slurmd/job2086598/slurm_script: line 25: 885674 Killed                  python -u 3_hyperparameter_search_rsnn.py --sweep-config sweep_hyperparameter_slstm.yaml --project SpikeSynth-Surrogate-Sweep --logging-directory /scratch/$USER/wandb_logs
slurmstepd: error: Detected 1 oom_kill event in StepId=2086598.batch. Some of the step tasks have been OOM Killed.
