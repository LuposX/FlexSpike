/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
wandb: Agent Starting Run: jmeu40cq with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.0015094222727522802
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Currently logged in as: lupos to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run jmeu40cq
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_101854-jmeu40cq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/jmeu40cq
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 50.6 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
50.7 K    Trainable params
0         Non-trainable params
50.7 K    Total params
0.203     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
wandb: updating run metadata
wandb: uploading history steps 104-105, summary, console lines 42-42
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:   grad_mean/lif_layers.0 ‚ñÅ‚ñÇ‚ñÑ‚ñá‚ñá‚ñà
wandb:   grad_mean/lif_layers.1 ‚ñÅ‚ñÇ‚ñÑ‚ñá‚ñá‚ñà
wandb:   grad_mean/lif_layers.2 ‚ñÅ‚ñÇ‚ñÑ‚ñá‚ñá‚ñà
wandb:   grad_mean/lif_layers.3 ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñà‚ñà
wandb:   grad_mean/output_layer ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_1 ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb: spikes/train_avg_layer_2 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 49
wandb:   grad_mean/lif_layers.0 0.37908
wandb:   grad_mean/lif_layers.1 0.02217
wandb:   grad_mean/lif_layers.2 0.0014
wandb:   grad_mean/lif_layers.3 0.00013
wandb:   grad_mean/output_layer 0.10614
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01189
wandb: spikes/train_avg_layer_1 0.02803
wandb: spikes/train_avg_layer_2 0.03157
wandb:                       +4 ...
wandb: 
wandb: üöÄ View run valiant-sweep-1 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/jmeu40cq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_101854-jmeu40cq/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: rwgo01b6 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -4
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 6.0983045412081094e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run rwgo01b6
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_105912-rwgo01b6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/rwgo01b6
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 400    | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
431       Trainable params
0         Non-trainable params
431       Total params
0.002     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: uploading console lines 44-44
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:   grad_mean/lif_layers.0 ‚ñà‚ñÅ
wandb:   grad_mean/lif_layers.1 ‚ñà‚ñÅ
wandb:   grad_mean/output_layer ‚ñà‚ñÅ
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ
wandb: spikes/train_avg_layer_1 ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÖ‚ñá
wandb:               train_loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:      trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:                 val_loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    epoch 21
wandb:   grad_mean/lif_layers.0 0.01685
wandb:   grad_mean/lif_layers.1 0.00785
wandb:   grad_mean/output_layer 0.38333
wandb:                       lr 4e-05
wandb: spikes/train_avg_layer_0 0.01058
wandb: spikes/train_avg_layer_1 0.00026
wandb:               train_loss 0.30911
wandb:      trainer/global_step 153
wandb:                 val_loss 0.3062
wandb: 
wandb: üöÄ View run lively-sweep-2 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/rwgo01b6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_105912-rwgo01b6/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 153, in run
    self.on_advance_end(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 398, in on_advance_end
    self.val_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py", line 179, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 145, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 437, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/RSNN.py", line 688, in validation_step
    outputs = self(X_batch)
              ^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/RSNN.py", line 574, in forward
    lif_out, _states = lif(input_seq)  # lif_out: (T, B, F_hidden)
                       ^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/src_cell.py", line 167, in forward
    x = self.rho * torch.tanh(i / self.rho)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
Exception

wandb: Agent Starting Run: 9t94c29p with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -2
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.003344037225741403
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 256
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run 9t94c29p
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_110010-9t94c29p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-sweep-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/9t94c29p
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 67.8 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 257    | train
--------------------------------------------------------
68.1 K    Trainable params
0         Non-trainable params
68.1 K    Total params
0.272     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Exception in thread QueueFeederThread:
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 291, in _feed
    queue_sem.release()
ValueError: semaphore or lock released too many times
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Exception ignored in: Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
<function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
        self._shutdown_workers()self._shutdown_workers()

  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^    ^if w.is_alive():^
^^ ^ ^ ^     ^if w.is_alive(): ^
 ^  
^   File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^ ^ ^ ^ ^ ^^^^^^^^^^^^
^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^
^^ ^ ^ ^ ^     ^assert self._parent_pid == os.getpid(), 'can only test a child process' ^
 ^  ^  ^  ^  ^^ ^^ ^^ ^^ ^^ ^^ ^^ 
^^AssertionError^^: ^^can only test a child process^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^AssertionError^: ^can only test a child process^
^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
<function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>Traceback (most recent call last):

  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
          if w.is_alive(): 
^^ ^ ^ ^ ^ ^ ^ ^^^^^^^^
^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
             assert self._parent_pid == os.getpid(), 'can only test a child process' 
 ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^AssertionError^: ^can only test a child process^
^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^    ^if w.is_alive():^
^^ ^ ^ ^ ^ 
   File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
 ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
         assert self._parent_pid == os.getpid(), 'can only test a child process' 
         ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^AssertionError^: ^can only test a child process^
^^^
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
           ^if w.is_alive():^
^^ ^ ^ ^ ^ ^ ^ ^^^^
^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
            assert self._parent_pid == os.getpid(), 'can only test a child process' 
   ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^AssertionError^: ^can only test a child process^
^^^^^^
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    Exception ignored in: assert self._parent_pid == os.getpid(), 'can only test a child process'
<function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0> 
 Traceback (most recent call last):
   File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    ^self._shutdown_workers()

AssertionError: can only test a child process  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers

    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    if w.is_alive():
       Exception ignored in: ^^^^<function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>^
^Traceback (most recent call last):
^  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    ^self._shutdown_workers()^
^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    assert self._parent_pid == os.getpid(), 'can only test a child process'
     if w.is_alive(): 
                   if w.is_alive(): ^
^^^ ^^ ^^ ^^ ^^ ^^ ^^ ^^^^^^^^^^^^
^^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^^^^^^^^^^^^^^^
^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^^^^^^^^^
AssertionError: can only test a child process
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           assert self._parent_pid == os.getpid(), 'can only test a child process' 
     ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^AssertionError^: ^can only test a child process^
^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
`Trainer.fit` stopped: `max_epochs=50` reached.
wandb: updating run metadata
wandb: uploading console lines 54-54
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   grad_mean/lif_layers.0 ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   grad_mean/lif_layers.1 ‚ñÅ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:   grad_mean/output_layer ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÑ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: spikes/train_avg_layer_1 ‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:               train_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:      trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                 val_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    epoch 49
wandb:   grad_mean/lif_layers.0 0.00142
wandb:   grad_mean/lif_layers.1 0.00011
wandb:   grad_mean/output_layer 0.00214
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01396
wandb: spikes/train_avg_layer_1 0.02281
wandb:               train_loss 0.14605
wandb:      trainer/global_step 349
wandb:                 val_loss 0.14486
wandb: 
wandb: üöÄ View run eternal-sweep-3 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/9t94c29p
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_110010-9t94c29p/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: q0bunu45 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 3.501306373311848e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 256
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_113806-q0bunu45
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-sweep-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/q0bunu45
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 462 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 257    | train
--------------------------------------------------------
462 K     Trainable params
0         Non-trainable params
462 K     Total params
1.851     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                  epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: grad_mean/lif_layers.0 ‚ñÅ
wandb: grad_mean/lif_layers.1 ‚ñÅ
wandb: grad_mean/lif_layers.2 ‚ñÅ
wandb: grad_mean/lif_layers.3 ‚ñÅ
wandb: grad_mean/lif_layers.4 ‚ñÅ
wandb: grad_mean/lif_layers.5 ‚ñÅ
wandb: grad_mean/lif_layers.6 ‚ñÅ
wandb: grad_mean/lif_layers.7 ‚ñÅ
wandb: grad_mean/output_layer ‚ñÅ
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 15
wandb: grad_mean/lif_layers.0 0.0073
wandb: grad_mean/lif_layers.1 0.00212
wandb: grad_mean/lif_layers.2 0.00104
wandb: grad_mean/lif_layers.3 0.00088
wandb: grad_mean/lif_layers.4 0.00088
wandb: grad_mean/lif_layers.5 0.00097
wandb: grad_mean/lif_layers.6 0.001
wandb: grad_mean/lif_layers.7 0.00127
wandb: grad_mean/output_layer 0.27324
wandb:                    +12 ...
wandb: 
wandb: üöÄ View run sandy-sweep-4 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/q0bunu45
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_113806-q0bunu45/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 300, in apply
    def apply(self, *args):
    
Exception

wandb: Agent Starting Run: q6cfffb1 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -2
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.015321332638101485
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 256
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run q6cfffb1
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_124311-q6cfffb1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/q6cfffb1
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 988 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 257    | train
--------------------------------------------------------
989 K     Trainable params
0         Non-trainable params
989 K     Total params
3.957     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
wandb: 
wandb: Run history:
wandb:                   epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:  grad_mean/lif_layers.0 ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  grad_mean/lif_layers.1 ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: grad_mean/lif_layers.10 ‚ñÉ‚ñÅ‚ñÉ‚ñà‚ñÑ‚ñÖ
wandb: grad_mean/lif_layers.11 ‚ñÅ‚ñÇ‚ñá‚ñà‚ñÜ‚ñÑ
wandb: grad_mean/lif_layers.12 ‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÅ
wandb: grad_mean/lif_layers.13 ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb: grad_mean/lif_layers.14 ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: grad_mean/lif_layers.15 ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:  grad_mean/lif_layers.2 ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 49
wandb:  grad_mean/lif_layers.0 578.32184
wandb:  grad_mean/lif_layers.1 63.01308
wandb: grad_mean/lif_layers.10 0.0056
wandb: grad_mean/lif_layers.11 0.00128
wandb: grad_mean/lif_layers.12 0.0001
wandb: grad_mean/lif_layers.13 1e-05
wandb: grad_mean/lif_layers.14 1e-05
wandb: grad_mean/lif_layers.15 1e-05
wandb:  grad_mean/lif_layers.2 10.08336
wandb:                     +28 ...
wandb: 
wandb: üöÄ View run effortless-sweep-5 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/q6cfffb1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_124311-q6cfffb1/logs
wandb: Agent Starting Run: l6t82qdk with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -4
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.0011132994184943651
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_194425-l6t82qdk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/l6t82qdk
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 52     | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
71        Trainable params
0         Non-trainable params
71        Total params
0.000     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:   grad_mean/lif_layers.0 ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ
wandb:   grad_mean/lif_layers.1 ‚ñÅ‚ñÖ‚ñà‚ñÖ‚ñá‚ñà
wandb:   grad_mean/output_layer ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÅ‚ñá
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: spikes/train_avg_layer_1 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:               train_loss ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:      trainer/global_step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                 val_loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    epoch 49
wandb:   grad_mean/lif_layers.0 0.00551
wandb:   grad_mean/lif_layers.1 0.00409
wandb:   grad_mean/output_layer 0.00444
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.02218
wandb: spikes/train_avg_layer_1 0.00548
wandb:               train_loss 0.15855
wandb:      trainer/global_step 349
wandb:                 val_loss 0.15767
wandb: 
wandb: üöÄ View run leafy-sweep-6 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/l6t82qdk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_194425-l6t82qdk/logs
wandb: Agent Starting Run: yzp31rnk with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -4
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.00019300965341297944
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 6
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run yzp31rnk
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_194559-yzp31rnk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-sweep-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/yzp31rnk
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 21.3 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
21.4 K    Trainable params
0         Non-trainable params
21.4 K    Total params
0.086     Total estimated model params size (MB)
20        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: uploading summary, console lines 44-44
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ
wandb:                       lr ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÅ
wandb: spikes/train_avg_layer_1 ‚ñÅ
wandb: spikes/train_avg_layer_2 ‚ñÅ
wandb: spikes/train_avg_layer_3 ‚ñÅ
wandb: spikes/train_avg_layer_4 ‚ñÅ
wandb: spikes/train_avg_layer_5 ‚ñÅ
wandb:               train_loss ‚ñÅ
wandb:      trainer/global_step ‚ñÅ‚ñÅ
wandb:                       +1 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 0
wandb:                       lr 0.00019
wandb: spikes/train_avg_layer_0 0.01712
wandb: spikes/train_avg_layer_1 0.00092
wandb: spikes/train_avg_layer_2 2e-05
wandb: spikes/train_avg_layer_3 0
wandb: spikes/train_avg_layer_4 0
wandb: spikes/train_avg_layer_5 0
wandb:               train_loss 0.24077
wandb:      trainer/global_step 6
wandb:                       +1 ...
wandb: 
wandb: üöÄ View run crisp-sweep-7 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/yzp31rnk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_194559-yzp31rnk/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 153, in run
    self.on_advance_end(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 398, in on_advance_end
    self.val_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py", line 179, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 145, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 437, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/RSNN.py", line 688, in validation_step
    outputs = self(X_batch)
              ^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/RSNN.py", line 574, in forward
    lif_out, _states = lif(input_seq)  # lif_out: (T, B, F_hidden)
                       ^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/src_cell.py", line 195, in forward
    sout = torch.stack(sout_seq, dim=0)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Exception

wandb: Agent Starting Run: 9znvv3n9 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.026967445022177676
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 256
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_194706-9znvv3n9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/9znvv3n9
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 462 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 257    | train
--------------------------------------------------------
462 K     Trainable params
0         Non-trainable params
462 K     Total params
1.851     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                  epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: grad_mean/lif_layers.0 ‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñÅ‚ñÅ
wandb: grad_mean/lif_layers.1 ‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñÅ‚ñÅ
wandb: grad_mean/lif_layers.2 ‚ñÇ‚ñÇ‚ñÑ‚ñà‚ñÅ‚ñÇ
wandb: grad_mean/lif_layers.3 ‚ñÇ‚ñÇ‚ñÑ‚ñà‚ñÅ‚ñÇ
wandb: grad_mean/lif_layers.4 ‚ñÅ‚ñÉ‚ñÖ‚ñà‚ñÅ‚ñÇ
wandb: grad_mean/lif_layers.5 ‚ñÅ‚ñá‚ñá‚ñà‚ñÉ‚ñÑ
wandb: grad_mean/lif_layers.6 ‚ñÅ‚ñà‚ñÖ‚ñÇ‚ñÜ‚ñÑ
wandb: grad_mean/lif_layers.7 ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñà‚ñá
wandb: grad_mean/output_layer ‚ñÑ‚ñà‚ñÜ‚ñà‚ñÉ‚ñÅ
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 49
wandb: grad_mean/lif_layers.0 70.15025
wandb: grad_mean/lif_layers.1 4.74168
wandb: grad_mean/lif_layers.2 0.15889
wandb: grad_mean/lif_layers.3 0.00689
wandb: grad_mean/lif_layers.4 0.00037
wandb: grad_mean/lif_layers.5 0.00012
wandb: grad_mean/lif_layers.6 6e-05
wandb: grad_mean/lif_layers.7 3e-05
wandb: grad_mean/output_layer 0.00074
wandb:                    +12 ...
wandb: 
wandb: üöÄ View run dandy-sweep-8 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/9znvv3n9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_194706-9znvv3n9/logs
wandb: Agent Starting Run: zhtnnsue with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -4
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.0013901880574695776
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 6
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run zhtnnsue
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_230626-zhtnnsue
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-9
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/zhtnnsue
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 83.6 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
83.7 K    Trainable params
0         Non-trainable params
83.7 K    Total params
0.335     Total estimated model params size (MB)
20        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà
wandb:                       lr ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñà
wandb: spikes/train_avg_layer_1 ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñà
wandb: spikes/train_avg_layer_2 ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñà
wandb: spikes/train_avg_layer_3 ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñà
wandb: spikes/train_avg_layer_4 ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb: spikes/train_avg_layer_5 ‚ñÅ‚ñÅ‚ñÑ‚ñÜ‚ñà
wandb:               train_loss ‚ñà‚ñá‚ñÖ‚ñÇ‚ñÅ
wandb:      trainer/global_step ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà
wandb:                       +1 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 4
wandb:                       lr 0.00136
wandb: spikes/train_avg_layer_0 0.02738
wandb: spikes/train_avg_layer_1 0.03595
wandb: spikes/train_avg_layer_2 0.03684
wandb: spikes/train_avg_layer_3 0.03372
wandb: spikes/train_avg_layer_4 0.03461
wandb: spikes/train_avg_layer_5 0.01573
wandb:               train_loss 0.19988
wandb:      trainer/global_step 34
wandb:                       +1 ...
wandb: 
wandb: üöÄ View run logical-sweep-9 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/zhtnnsue
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_230626-zhtnnsue/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
                  ^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/RSNN.py", line 634, in training_step
    outputs = self(X_batch)
              ^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/RSNN.py", line 574, in forward
    lif_out, _states = lif(input_seq)  # lif_out: (T, B, F_hidden)
                       ^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/src_cell.py", line 163, in forward
    delta = torch.matmul(sin_t, self.Ws.t())  # (batch, hidden)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Exception

wandb: Agent Starting Run: 6jpj6755 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -4
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 4.32669327651504e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_231410-6jpj6755
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/6jpj6755
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 116 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
116 K     Trainable params
0         Non-trainable params
116 K     Total params
0.467     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: uploading summary
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà
wandb:                       lr ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÅ
wandb: spikes/train_avg_layer_1 ‚ñà‚ñÜ‚ñÅ‚ñÖ‚ñÅ
wandb: spikes/train_avg_layer_2 ‚ñà‚ñÅ‚ñÖ‚ñÜ‚ñà
wandb: spikes/train_avg_layer_3 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_4 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_5 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_6 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_7 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                       +3 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 4
wandb:                       lr 0.0
wandb: spikes/train_avg_layer_0 0.01829
wandb: spikes/train_avg_layer_1 0.00092
wandb: spikes/train_avg_layer_2 1e-05
wandb: spikes/train_avg_layer_3 0
wandb: spikes/train_avg_layer_4 0
wandb: spikes/train_avg_layer_5 0
wandb: spikes/train_avg_layer_6 0
wandb: spikes/train_avg_layer_7 0
wandb:                       +3 ...
wandb: 
wandb: üöÄ View run vital-sweep-10 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/6jpj6755
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_231410-6jpj6755/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 300, in apply
    def apply(self, *args):
    
Exception

wandb: Agent Starting Run: m60m89bi with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -4
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.00020863290178667893
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 6
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run m60m89bi
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_232407-m60m89bi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run skilled-sweep-11
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/m60m89bi
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 1.5 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
1.5 K     Trainable params
0         Non-trainable params
1.5 K     Total params
0.006     Total estimated model params size (MB)
20        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà
wandb:                       lr ‚ñà‚ñà‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñà
wandb: spikes/train_avg_layer_1 ‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñà
wandb: spikes/train_avg_layer_2 ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÇ‚ñÅ
wandb: spikes/train_avg_layer_3 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÜ
wandb: spikes/train_avg_layer_4 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_5 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:               train_loss ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ
wandb:      trainer/global_step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà
wandb:                       +1 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 6
wandb:                       lr 0.0002
wandb: spikes/train_avg_layer_0 0.0165
wandb: spikes/train_avg_layer_1 0.00183
wandb: spikes/train_avg_layer_2 8e-05
wandb: spikes/train_avg_layer_3 1e-05
wandb: spikes/train_avg_layer_4 0
wandb: spikes/train_avg_layer_5 0
wandb:               train_loss 0.26762
wandb:      trainer/global_step 48
wandb:                       +1 ...
wandb: 
wandb: üöÄ View run skilled-sweep-11 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/m60m89bi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_232407-m60m89bi/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 315, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/src_cell.py", line 61, in backward
    grad_input = grad_output.clone()
                 ^^^^^^^^^^^^^^^^^^^
Exception

wandb: Agent Starting Run: jkd8prg6 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.0029665454544395924
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_232509-jkd8prg6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-12
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/jkd8prg6
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 62.9 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
63.0 K    Trainable params
0         Non-trainable params
63.0 K    Total params
0.252     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                     epoch ‚ñÅ‚ñÅ
wandb:                        lr ‚ñÅ
wandb:  spikes/train_avg_layer_0 ‚ñÅ
wandb:  spikes/train_avg_layer_1 ‚ñÅ
wandb: spikes/train_avg_layer_10 ‚ñÅ
wandb: spikes/train_avg_layer_11 ‚ñÅ
wandb: spikes/train_avg_layer_12 ‚ñÅ
wandb: spikes/train_avg_layer_13 ‚ñÅ
wandb: spikes/train_avg_layer_14 ‚ñÅ
wandb: spikes/train_avg_layer_15 ‚ñÅ
wandb:                       +11 ...
wandb: 
wandb: Run summary:
wandb:                     epoch 0
wandb:                        lr 0.00296
wandb:  spikes/train_avg_layer_0 0.0205
wandb:  spikes/train_avg_layer_1 0.00248
wandb: spikes/train_avg_layer_10 0
wandb: spikes/train_avg_layer_11 0
wandb: spikes/train_avg_layer_12 0
wandb: spikes/train_avg_layer_13 0
wandb: spikes/train_avg_layer_14 0
wandb: spikes/train_avg_layer_15 0
wandb:                       +11 ...
wandb: 
wandb: üöÄ View run iconic-sweep-12 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/jkd8prg6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_232509-jkd8prg6/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 300, in apply
    def apply(self, *args):
    
Exception

wandb: Agent Starting Run: 4xmpvuuv with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.0004575287328316178
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 256
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run 4xmpvuuv
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_232708-4xmpvuuv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-sweep-13
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/4xmpvuuv
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 462 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 257    | train
--------------------------------------------------------
462 K     Trainable params
0         Non-trainable params
462 K     Total params
1.851     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ
wandb:                       lr ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÅ
wandb: spikes/train_avg_layer_1 ‚ñÅ
wandb: spikes/train_avg_layer_2 ‚ñÅ
wandb: spikes/train_avg_layer_3 ‚ñÅ
wandb: spikes/train_avg_layer_4 ‚ñÅ
wandb: spikes/train_avg_layer_5 ‚ñÅ
wandb: spikes/train_avg_layer_6 ‚ñÅ
wandb: spikes/train_avg_layer_7 ‚ñÅ
wandb:                       +3 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 0
wandb:                       lr 0.00046
wandb: spikes/train_avg_layer_0 0.01854
wandb: spikes/train_avg_layer_1 0.00136
wandb: spikes/train_avg_layer_2 8e-05
wandb: spikes/train_avg_layer_3 0.0
wandb: spikes/train_avg_layer_4 0
wandb: spikes/train_avg_layer_5 0
wandb: spikes/train_avg_layer_6 0
wandb: spikes/train_avg_layer_7 0
wandb:                       +3 ...
wandb: 
wandb: üöÄ View run peach-sweep-13 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/4xmpvuuv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_232708-4xmpvuuv/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
                  ^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/RSNN.py", line 634, in training_step
    outputs = self(X_batch)
              ^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/RSNN.py", line 574, in forward
    lif_out, _states = lif(input_seq)  # lif_out: (T, B, F_hidden)
                       ^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/src_cell.py", line 185, in forward
    h_candidate = torch.tanh(preact)
                  ^^^^^^^^^^^^^^^^^^
Exception

wandb: Agent Starting Run: 1sdvw85k with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.01516736826587148
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 6
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_233207-1sdvw85k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-14
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/1sdvw85k
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 83.6 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
83.7 K    Trainable params
0         Non-trainable params
83.7 K    Total params
0.335     Total estimated model params size (MB)
20        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
wandb: uploading history steps 104-105, summary, console lines 45-47
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:   grad_mean/lif_layers.0 ‚ñÜ‚ñÑ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:   grad_mean/lif_layers.1 ‚ñá‚ñÑ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:   grad_mean/lif_layers.2 ‚ñÉ‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:   grad_mean/lif_layers.3 ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:   grad_mean/lif_layers.4 ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:   grad_mean/lif_layers.5 ‚ñÑ‚ñÇ‚ñà‚ñÇ‚ñÅ‚ñÅ
wandb:   grad_mean/output_layer ‚ñà‚ñÑ‚ñà‚ñÖ‚ñÅ‚ñÇ
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÖ‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:                       +8 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 49
wandb:   grad_mean/lif_layers.0 0.00027
wandb:   grad_mean/lif_layers.1 4e-05
wandb:   grad_mean/lif_layers.2 1e-05
wandb:   grad_mean/lif_layers.3 0.0
wandb:   grad_mean/lif_layers.4 0.0
wandb:   grad_mean/lif_layers.5 1e-05
wandb:   grad_mean/output_layer 0.00203
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01369
wandb:                       +8 ...
wandb: 
wandb: üöÄ View run fearless-sweep-14 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/1sdvw85k
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251127_233207-1sdvw85k/logs
wandb: Agent Starting Run: ouy4j4jh with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -2
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.017916829242191984
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 256
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run ouy4j4jh
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_003846-ouy4j4jh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/ouy4j4jh
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 462 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 257    | train
--------------------------------------------------------
462 K     Trainable params
0         Non-trainable params
462 K     Total params
1.851     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
wandb: uploading history steps 104-105, summary, console lines 42-42
wandb: 
wandb: Run history:
wandb:                  epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: grad_mean/lif_layers.0 ‚ñà‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÜ
wandb: grad_mean/lif_layers.1 ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: grad_mean/lif_layers.2 ‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÉ
wandb: grad_mean/lif_layers.3 ‚ñà‚ñÇ‚ñÅ‚ñÜ‚ñÖ‚ñÉ
wandb: grad_mean/lif_layers.4 ‚ñÇ‚ñÇ‚ñÅ‚ñà‚ñÑ‚ñÉ
wandb: grad_mean/lif_layers.5 ‚ñÉ‚ñÅ‚ñÅ‚ñà‚ñÉ‚ñÉ
wandb: grad_mean/lif_layers.6 ‚ñà‚ñÅ‚ñÅ‚ñà‚ñÑ‚ñÑ
wandb: grad_mean/lif_layers.7 ‚ñà‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÉ
wandb: grad_mean/output_layer ‚ñÉ‚ñÜ‚ñÅ‚ñÇ‚ñà‚ñÉ
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 49
wandb: grad_mean/lif_layers.0 0.04837
wandb: grad_mean/lif_layers.1 0.00727
wandb: grad_mean/lif_layers.2 0.00197
wandb: grad_mean/lif_layers.3 0.0004
wandb: grad_mean/lif_layers.4 0.00015
wandb: grad_mean/lif_layers.5 7e-05
wandb: grad_mean/lif_layers.6 2e-05
wandb: grad_mean/lif_layers.7 1e-05
wandb: grad_mean/output_layer 0.00624
wandb:                    +12 ...
wandb: 
wandb: üöÄ View run driven-sweep-15 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/ouy4j4jh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_003846-ouy4j4jh/logs
wandb: Agent Starting Run: u7x45y9d with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.00029484871687528245
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run u7x45y9d
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_035659-u7x45y9d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-16
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/u7x45y9d
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 400    | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
431       Trainable params
0         Non-trainable params
431       Total params
0.002     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: uploading summary, console lines 44-44
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   grad_mean/lif_layers.0 ‚ñÅ‚ñà
wandb:   grad_mean/lif_layers.1 ‚ñà‚ñÅ
wandb:   grad_mean/output_layer ‚ñà‚ñÅ
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà
wandb: spikes/train_avg_layer_1 ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà
wandb:               train_loss ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:      trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:                 val_loss ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    epoch 21
wandb:   grad_mean/lif_layers.0 0.01239
wandb:   grad_mean/lif_layers.1 0.0063
wandb:   grad_mean/output_layer 0.21941
wandb:                       lr 0.00018
wandb: spikes/train_avg_layer_0 0.02951
wandb: spikes/train_avg_layer_1 0.00616
wandb:               train_loss 0.20976
wandb:      trainer/global_step 149
wandb:                 val_loss 0.20715
wandb: 
wandb: üöÄ View run fluent-sweep-16 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/u7x45y9d
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_035659-u7x45y9d/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 153, in run
    self.on_advance_end(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 398, in on_advance_end
    self.val_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py", line 179, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 145, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 437, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/RSNN.py", line 688, in validation_step
    outputs = self(X_batch)
              ^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/RSNN.py", line 574, in forward
    lif_out, _states = lif(input_seq)  # lif_out: (T, B, F_hidden)
                       ^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/src_cell.py", line 189, in forward
    hs = zs * hs_prev + (1.0 - zs) * h_prev
                         ~~~~^~~~
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 38, in wrapped
    @functools.wraps(f)
    
Exception

wandb: Agent Starting Run: 480lotb1 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -4
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.002175374224656792
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 256
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_035806-480lotb1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-sweep-17
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/480lotb1
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 462 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 257    | train
--------------------------------------------------------
462 K     Trainable params
0         Non-trainable params
462 K     Total params
1.851     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    if w.is_alive():
       ^^^^^^^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^
^^ 
   File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^
^^
 AssertionError :  can only test a child process 
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Exception in thread QueueFeederThread:
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 291, in _feed
    queue_sem.release()
ValueError: semaphore or lock released too many times
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
Exception ignored in:   File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
<function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
      File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
self._shutdown_workers()
      File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
self._shutdown_workers()
Exception ignored in:   File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
<function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()    
if w.is_alive():
   File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
     if w.is_alive(): 
      Exception ignored in:  ^     <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>^self._shutdown_workers() 
^
 Traceback (most recent call last):
^   File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
^^  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
^^^    ^^if w.is_alive():^^
^^^ ^^ ^^ 
^   File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^ ^ ^ 
^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^^^^^^^    ^^if w.is_alive():^
^
   File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
      ^^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^    
^assert self._parent_pid == os.getpid(), 'can only test a child process'     ^
self._shutdown_workers() ^
      ^ assert self._parent_pid == os.getpid(), 'can only test a child process' ^  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
 
 ^   ^   
     File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
          ^  ^ ^^ ^^ ^^ ^^^^^^^^^^^^^^^^^        ^^^assert self._parent_pid == os.getpid(), 'can only test a child process'if w.is_alive():^^^

^^^^  ^^^  ^^^  ^^^  ^^^  ^^^  ^^^  ^^^ ^^^^ ^^^^ ^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^AssertionError^^

: ^^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
AssertionErrorcan only test a child process^^: 
^^can only test a child process
^
AssertionError^: ^can only test a child process^
^^^^^^^^^^^^^^^
AssertionError: can only test a child process    
assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
OSError: [Errno 9] Bad file descriptor
OSError: [Errno 9] Bad file descriptor
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
Traceback (most recent call last):
OSError: [Errno 9] Bad file descriptor
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1618, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 1136, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Exception: 
`Trainer.fit` stopped: `max_epochs=50` reached.
wandb: updating run metadata
wandb: uploading history steps 104-105, summary, console lines 45-46
wandb: 
wandb: Run history:
wandb:                  epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb: grad_mean/lif_layers.0 ‚ñà‚ñÜ‚ñÅ‚ñÇ‚ñÉ‚ñÇ
wandb: grad_mean/lif_layers.1 ‚ñà‚ñÜ‚ñÅ‚ñÇ‚ñÉ‚ñÉ
wandb: grad_mean/lif_layers.2 ‚ñà‚ñÜ‚ñÅ‚ñÇ‚ñÉ‚ñÇ
wandb: grad_mean/lif_layers.3 ‚ñà‚ñÜ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: grad_mean/lif_layers.4 ‚ñà‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb: grad_mean/lif_layers.5 ‚ñà‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb: grad_mean/lif_layers.6 ‚ñÅ‚ñà‚ñÜ‚ñá‚ñá‚ñÜ
wandb: grad_mean/lif_layers.7 ‚ñÅ‚ñà‚ñÜ‚ñá‚ñà‚ñÜ
wandb: grad_mean/output_layer ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:                    +12 ...
wandb: 
wandb: Run summary:
wandb:                  epoch 49
wandb: grad_mean/lif_layers.0 30484.13867
wandb: grad_mean/lif_layers.1 1598.09253
wandb: grad_mean/lif_layers.2 67.84391
wandb: grad_mean/lif_layers.3 2.5709
wandb: grad_mean/lif_layers.4 0.08174
wandb: grad_mean/lif_layers.5 0.00771
wandb: grad_mean/lif_layers.6 0.00075
wandb: grad_mean/lif_layers.7 7e-05
wandb: grad_mean/output_layer 0.01543
wandb:                    +12 ...
wandb: 
wandb: üöÄ View run zany-sweep-17 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/480lotb1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_035806-480lotb1/logs
wandb: Agent Starting Run: u3a6e8gy with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -4
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 4.7024707426965405e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_071712-u3a6e8gy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-18
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/u3a6e8gy
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 92     | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
111       Trainable params
0         Non-trainable params
111       Total params
0.000     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:   grad_mean/lif_layers.0 ‚ñÅ
wandb:   grad_mean/lif_layers.1 ‚ñÅ
wandb:   grad_mean/lif_layers.2 ‚ñÅ
wandb:   grad_mean/lif_layers.3 ‚ñÅ
wandb:   grad_mean/output_layer ‚ñÅ
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà
wandb: spikes/train_avg_layer_1 ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà
wandb: spikes/train_avg_layer_2 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 17
wandb:   grad_mean/lif_layers.0 0.0142
wandb:   grad_mean/lif_layers.1 0.04181
wandb:   grad_mean/lif_layers.2 0.04008
wandb:   grad_mean/lif_layers.3 0.03698
wandb:   grad_mean/output_layer 0.70261
wandb:                       lr 3e-05
wandb: spikes/train_avg_layer_0 0.01883
wandb: spikes/train_avg_layer_1 0.00157
wandb: spikes/train_avg_layer_2 3e-05
wandb:                       +4 ...
wandb: 
wandb: üöÄ View run iconic-sweep-18 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/u3a6e8gy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_071712-u3a6e8gy/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 153, in run
    self.on_advance_end(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 398, in on_advance_end
    self.val_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py", line 179, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 123, in run
    self.reset()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 259, in reset
    iter(data_fetcher)  # creates the iterator inside the fetcher
    ^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fetchers.py", line 105, in __iter__
    super().__iter__()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fetchers.py", line 52, in __iter__
    self.iterator = iter(self.combined_loader)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/utilities/combined_loader.py", line 351, in __iter__
    iter(iterator)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/utilities/combined_loader.py", line 155, in __iter__
    self._load_current_iterator()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/utilities/combined_loader.py", line 173, in _load_current_iterator
    self.iterators = [iter(self.iterables[self._iterator_idx])]
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 494, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 427, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1170, in __init__
    w.start()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 120, in start
    _cleanup()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 64, in _cleanup
    if (child_popen := p._popen) and child_popen.poll() is not None:
                                     ^^^^^^^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
Exception

Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1612, in _shutdown_workers
    if self._persistent_workers or self._workers_status[worker_id]:
                                   ^^^^^^^^^^^^^^^^^^^^
AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'
wandb: Agent Starting Run: u3a6e8gy with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -4
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 4.7024707426965405e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_071810-u3a6e8gy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-18
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/u3a6e8gy
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:751: Checkpoint directory /scratch/ka_rh5993/wandb_logs/checkpoints/SpikeSynth-Surrogate-Sweep/byr8b0jv/u3a6e8gy exists and is not empty.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 92     | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
111       Trainable params
0         Non-trainable params
111       Total params
0.000     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
      File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
      File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^
^^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^
^AssertionError^: ^can only test a child process^
^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception in thread QueueFeederThread:
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 291, in _feed
    queue_sem.release()
ValueError: semaphore or lock released too many times
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
Exception ignored in:   File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
<function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
    self._shutdown_workers()  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers

  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
        if w.is_alive():if w.is_alive():

              ^^^^^^^^^^^^    ^^if w.is_alive():^^
^^^ ^^ ^^ ^
 
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
   File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
  ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
     assert self._parent_pid == os.getpid(), 'can only test a child process' 
            assert self._parent_pid == os.getpid(), 'can only test a child process'  
          ^  ^  ^  ^^ ^^ ^^ ^^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^AssertionError^^: ^^can only test a child process
^
AssertionError^: ^can only test a child process^
^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^    ^if w.is_alive():^
^^ ^ ^ ^ ^ ^ 
   File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
              assert self._parent_pid == os.getpid(), 'can only test a child process' 
^^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^AssertionError^: ^can only test a child process^
^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
Traceback (most recent call last):
OSError: [Errno 9] Bad file descriptor
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0><function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0><function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>


Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       if w.is_alive(): 
     ^ ^ ^ ^ ^     ^if w.is_alive():^^
^^^ ^^ ^^ ^^ ^^ 
^   File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^ ^^^^^^
^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
      assert self._parent_pid == os.getpid(), 'can only test a child process' 
          assert self._parent_pid == os.getpid(), 'can only test a child process'  
         ^  ^  ^  ^  ^ ^^ ^^ ^^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^AssertionError^^: ^^can only test a child process^^
^
^AssertionError^: ^can only test a child process^

AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    Traceback (most recent call last):
if w.is_alive():
       ^^^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
          Traceback (most recent call last):
 ^^^Traceback (most recent call last):
^^^^^^^^^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
^^^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
^^^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
^^^^^OSError: [Errno 9] Bad file descriptor
^^^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
^^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
^^OSError: [Errno 9] Bad file descriptor

AssertionError: can only test a child process
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
           ^^if w.is_alive():^
^^ ^ ^ ^ ^ ^ ^ ^^
^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
       assert self._parent_pid == os.getpid(), 'can only test a child process' 
             ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^AssertionError^: ^can only test a child process^Exception ignored in: 
^Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
<function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
AssertionError
Traceback (most recent call last):
: Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
can only test a child process  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__

Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
    Traceback (most recent call last):
self._shutdown_workers()  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__

      File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
     self._shutdown_workers() 
      File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
     ^if w.is_alive():^
^    ^ if w.is_alive():^ 
^ ^  ^  ^  ^  ^ ^^     ^
 if w.is_alive():^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^
^^ ^^ ^^ ^^ ^^ ^^ ^^ ^^^^^^
^^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^^
^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^
^^ ^ ^ 
   File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
       ^^^^^^^^^^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^    
^assert self._parent_pid == os.getpid(), 'can only test a child process'^
 ^  ^      ^assert self._parent_pid == os.getpid(), 'can only test a child process'  ^
  ^   ^   ^   ^   ^   ^   ^  ^^^ ^^^ ^^^ ^^^ ^^^^^
^^^AssertionError^^^: ^^^can only test a child process^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

^AssertionErrorAssertionError^: : ^can only test a child processcan only test a child process^


AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
Traceback (most recent call last):
OSError: [Errno 9] Bad file descriptor
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1618, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 1136, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Exception: 
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Exception ignored in: Exception ignored in:     <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>self._shutdown_workers()<function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>


Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    self._shutdown_workers()
    if w.is_alive():  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers

           self._shutdown_workers()^
^^^^  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
^^^^^^^
      File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
if w.is_alive():
       ^^^^^^^^^^^^    
if w.is_alive():  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive

        assert self._parent_pid == os.getpid(), 'can only test a child process' 
   ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'
^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^^
^^^^ ^^ ^^ ^^ ^
 ^AssertionError ^:  ^can only test a child process ^
 ^ ^ ^^^^^^^^^^^^^^^^
^AssertionError^: ^can only test a child process^
^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
Traceback (most recent call last):
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
    self._shutdown_workers()  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers

  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
      self._shutdown_workers() 
      File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
^^^^^^^    ^if w.is_alive():^
^^ ^ 
   File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    if w.is_alive():
       assert self._parent_pid == os.getpid(), 'can only test a child process' 
    ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^
    ^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
assert self._parent_pid == os.getpid(), 'can only test a child process'^
^^^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^^
^^^
 ^AssertionError ^:  ^can only test a child process ^
 ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^
^AssertionError^: ^can only test a child process^
^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Traceback (most recent call last):
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
OSError: [Errno 9] Bad file descriptor
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Traceback (most recent call last):
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
`Trainer.fit` stopped: `max_epochs=50` reached.
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:   grad_mean/lif_layers.0 ‚ñà‚ñÖ‚ñÇ‚ñÖ‚ñÅ‚ñÉ
wandb:   grad_mean/lif_layers.1 ‚ñÖ‚ñÖ‚ñÅ‚ñà‚ñÑ‚ñÜ
wandb:   grad_mean/lif_layers.2 ‚ñÜ‚ñÖ‚ñÅ‚ñà‚ñÑ‚ñÜ
wandb:   grad_mean/lif_layers.3 ‚ñÜ‚ñÖ‚ñÅ‚ñà‚ñÉ‚ñÜ
wandb:   grad_mean/output_layer ‚ñÖ‚ñÜ‚ñÅ‚ñà‚ñÑ‚ñÜ
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_1 ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_2 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 49
wandb:   grad_mean/lif_layers.0 0.01372
wandb:   grad_mean/lif_layers.1 0.0049
wandb:   grad_mean/lif_layers.2 0.00765
wandb:   grad_mean/lif_layers.3 0.00496
wandb:   grad_mean/output_layer 0.2118
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.00723
wandb: spikes/train_avg_layer_1 7e-05
wandb: spikes/train_avg_layer_2 0
wandb:                       +4 ...
wandb: 
wandb: üöÄ View run iconic-sweep-18 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/u3a6e8gy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_071810-u3a6e8gy/logs
wandb: Agent Starting Run: 2zakt01s with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -2
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.023543893681701948
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 6
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run 2zakt01s
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_072227-2zakt01s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-19
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/2zakt01s
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 132    | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
151       Trainable params
0         Non-trainable params
151       Total params
0.001     Total estimated model params size (MB)
20        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
wandb: updating run metadata
wandb: uploading console lines 44-45
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:   grad_mean/lif_layers.0 ‚ñà‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ
wandb:   grad_mean/lif_layers.1 ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ
wandb:   grad_mean/lif_layers.2 ‚ñá‚ñà‚ñá‚ñÅ‚ñÇ‚ñÑ
wandb:   grad_mean/lif_layers.3 ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   grad_mean/lif_layers.4 ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:   grad_mean/lif_layers.5 ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   grad_mean/output_layer ‚ñà‚ñÖ‚ñÅ‚ñá‚ñÜ‚ñÉ
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                       +8 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 49
wandb:   grad_mean/lif_layers.0 0.08674
wandb:   grad_mean/lif_layers.1 0.01491
wandb:   grad_mean/lif_layers.2 0.00216
wandb:   grad_mean/lif_layers.3 0.00033
wandb:   grad_mean/lif_layers.4 0.00016
wandb:   grad_mean/lif_layers.5 0.00013
wandb:   grad_mean/output_layer 0.003
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01629
wandb:                       +8 ...
wandb: 
wandb: üöÄ View run electric-sweep-19 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/2zakt01s
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_072227-2zakt01s/logs
wandb: Agent Starting Run: axm0jph6 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -2
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.0034178079960247355
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 256
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_072452-axm0jph6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/axm0jph6
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 462 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 257    | train
--------------------------------------------------------
462 K     Trainable params
0         Non-trainable params
462 K     Total params
1.851     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ
wandb:                       lr ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÅ
wandb: spikes/train_avg_layer_1 ‚ñÅ
wandb: spikes/train_avg_layer_2 ‚ñÅ
wandb: spikes/train_avg_layer_3 ‚ñÅ
wandb: spikes/train_avg_layer_4 ‚ñÅ
wandb: spikes/train_avg_layer_5 ‚ñÅ
wandb: spikes/train_avg_layer_6 ‚ñÅ
wandb: spikes/train_avg_layer_7 ‚ñÅ
wandb:                       +3 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 0
wandb:                       lr 0.00341
wandb: spikes/train_avg_layer_0 0.02142
wandb: spikes/train_avg_layer_1 0.00904
wandb: spikes/train_avg_layer_2 0.00758
wandb: spikes/train_avg_layer_3 0.00617
wandb: spikes/train_avg_layer_4 0.005
wandb: spikes/train_avg_layer_5 0.00361
wandb: spikes/train_avg_layer_6 0.00241
wandb: spikes/train_avg_layer_7 0.0013
wandb:                       +3 ...
wandb: 
wandb: üöÄ View run glowing-sweep-20 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/axm0jph6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_072452-axm0jph6/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 300, in apply
    def apply(self, *args):
    
Exception

wandb: Agent Starting Run: vrtg40c7 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 2.5810248978879674e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_073007-vrtg40c7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-sweep-21
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/vrtg40c7
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 62.9 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
63.0 K    Trainable params
0         Non-trainable params
63.0 K    Total params
0.252     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: uploading summary, console lines 42-42
wandb: 
wandb: Run history:
wandb:                     epoch ‚ñÅ‚ñÅ
wandb:                        lr ‚ñÅ
wandb:  spikes/train_avg_layer_0 ‚ñÅ
wandb:  spikes/train_avg_layer_1 ‚ñÅ
wandb: spikes/train_avg_layer_10 ‚ñÅ
wandb: spikes/train_avg_layer_11 ‚ñÅ
wandb: spikes/train_avg_layer_12 ‚ñÅ
wandb: spikes/train_avg_layer_13 ‚ñÅ
wandb: spikes/train_avg_layer_14 ‚ñÅ
wandb: spikes/train_avg_layer_15 ‚ñÅ
wandb:                       +11 ...
wandb: 
wandb: Run summary:
wandb:                     epoch 0
wandb:                        lr 3e-05
wandb:  spikes/train_avg_layer_0 0.01618
wandb:  spikes/train_avg_layer_1 0.00105
wandb: spikes/train_avg_layer_10 0
wandb: spikes/train_avg_layer_11 0
wandb: spikes/train_avg_layer_12 0
wandb: spikes/train_avg_layer_13 0
wandb: spikes/train_avg_layer_14 0
wandb: spikes/train_avg_layer_15 0
wandb:                       +11 ...
wandb: 
wandb: üöÄ View run toasty-sweep-21 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/vrtg40c7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_073007-vrtg40c7/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 300, in apply
    def apply(self, *args):
    
Exception

wandb: Agent Starting Run: l8t2vpie with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -2
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 2.2603945455255457e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_073307-l8t2vpie
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-22
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/l8t2vpie
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 52     | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
71        Trainable params
0         Non-trainable params
71        Total params
0.000     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   grad_mean/lif_layers.0 ‚ñà‚ñÇ‚ñÅ
wandb:   grad_mean/lif_layers.1 ‚ñÅ‚ñà‚ñá
wandb:   grad_mean/output_layer ‚ñÅ‚ñà‚ñÖ
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÉ
wandb: spikes/train_avg_layer_1 ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ
wandb:               train_loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:      trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                 val_loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    epoch 30
wandb:   grad_mean/lif_layers.0 0.01814
wandb:   grad_mean/lif_layers.1 0.0092
wandb:   grad_mean/output_layer 0.34206
wandb:                       lr 1e-05
wandb: spikes/train_avg_layer_0 0.01933
wandb: spikes/train_avg_layer_1 0.00068
wandb:               train_loss 0.27566
wandb:      trainer/global_step 216
wandb:                 val_loss 0.27311
wandb: 
wandb: üöÄ View run avid-sweep-22 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/l8t2vpie
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_073307-l8t2vpie/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 153, in run
    self.on_advance_end(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 398, in on_advance_end
    self.val_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py", line 179, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 145, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 437, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/RSNN.py", line 688, in validation_step
    outputs = self(X_batch)
              ^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/RSNN.py", line 574, in forward
    lif_out, _states = lif(input_seq)  # lif_out: (T, B, F_hidden)
                       ^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/src_cell.py", line 163, in forward
    delta = torch.matmul(sin_t, self.Ws.t())  # (batch, hidden)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Exception

wandb: Agent Starting Run: 5w9ceu83 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -4
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 1.3397865870472438e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 256
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_073410-5w9ceu83
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-23
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/5w9ceu83
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 462 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 257    | train
--------------------------------------------------------
462 K     Trainable params
0         Non-trainable params
462 K     Total params
1.851     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ
wandb:                       lr ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÅ
wandb: spikes/train_avg_layer_1 ‚ñÅ
wandb: spikes/train_avg_layer_2 ‚ñÅ
wandb: spikes/train_avg_layer_3 ‚ñÅ
wandb: spikes/train_avg_layer_4 ‚ñÅ
wandb: spikes/train_avg_layer_5 ‚ñÅ
wandb: spikes/train_avg_layer_6 ‚ñÅ
wandb: spikes/train_avg_layer_7 ‚ñÅ
wandb:                       +3 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 0
wandb:                       lr 1e-05
wandb: spikes/train_avg_layer_0 0.02166
wandb: spikes/train_avg_layer_1 0.00133
wandb: spikes/train_avg_layer_2 4e-05
wandb: spikes/train_avg_layer_3 0
wandb: spikes/train_avg_layer_4 0
wandb: spikes/train_avg_layer_5 0
wandb: spikes/train_avg_layer_6 0
wandb: spikes/train_avg_layer_7 0
wandb:                       +3 ...
wandb: 
wandb: üöÄ View run zesty-sweep-23 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/5w9ceu83
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_073410-5w9ceu83/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 300, in apply
    def apply(self, *args):
    
Exception

wandb: Agent Starting Run: n8tehgx6 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -4
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 2.6409588835545736e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_073909-n8tehgx6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-24
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/n8tehgx6
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 62.9 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
63.0 K    Trainable params
0         Non-trainable params
63.0 K    Total params
0.252     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: uploading summary, console lines 42-42
wandb: 
wandb: Run history:
wandb:                     epoch ‚ñÅ‚ñÅ
wandb:                        lr ‚ñÅ
wandb:  spikes/train_avg_layer_0 ‚ñÅ
wandb:  spikes/train_avg_layer_1 ‚ñÅ
wandb: spikes/train_avg_layer_10 ‚ñÅ
wandb: spikes/train_avg_layer_11 ‚ñÅ
wandb: spikes/train_avg_layer_12 ‚ñÅ
wandb: spikes/train_avg_layer_13 ‚ñÅ
wandb: spikes/train_avg_layer_14 ‚ñÅ
wandb: spikes/train_avg_layer_15 ‚ñÅ
wandb:                       +11 ...
wandb: 
wandb: Run summary:
wandb:                     epoch 0
wandb:                        lr 3e-05
wandb:  spikes/train_avg_layer_0 0.01836
wandb:  spikes/train_avg_layer_1 0.00156
wandb: spikes/train_avg_layer_10 0
wandb: spikes/train_avg_layer_11 0
wandb: spikes/train_avg_layer_12 0
wandb: spikes/train_avg_layer_13 0
wandb: spikes/train_avg_layer_14 0
wandb: spikes/train_avg_layer_15 0
wandb:                       +11 ...
wandb: 
wandb: üöÄ View run icy-sweep-24 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/n8tehgx6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_073909-n8tehgx6/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 300, in apply
    def apply(self, *args):
    
Exception

wandb: Agent Starting Run: qd7j07rt with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -2
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 5.698722553990944e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run qd7j07rt
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_074210-qd7j07rt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-sweep-25
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/qd7j07rt
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 62.9 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
63.0 K    Trainable params
0         Non-trainable params
63.0 K    Total params
0.252     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Exception in thread QueueFeederThread:
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 291, in _feed
    queue_sem.release()
ValueError: semaphore or lock released too many times
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                     epoch ‚ñÅ‚ñÅ
wandb:                        lr ‚ñÅ
wandb:  spikes/train_avg_layer_0 ‚ñÅ
wandb:  spikes/train_avg_layer_1 ‚ñÅ
wandb: spikes/train_avg_layer_10 ‚ñÅ
wandb: spikes/train_avg_layer_11 ‚ñÅ
wandb: spikes/train_avg_layer_12 ‚ñÅ
wandb: spikes/train_avg_layer_13 ‚ñÅ
wandb: spikes/train_avg_layer_14 ‚ñÅ
wandb: spikes/train_avg_layer_15 ‚ñÅ
wandb:                       +11 ...
wandb: 
wandb: Run summary:
wandb:                     epoch 0
wandb:                        lr 1e-05
wandb:  spikes/train_avg_layer_0 0.02221
wandb:  spikes/train_avg_layer_1 0.00124
wandb: spikes/train_avg_layer_10 0
wandb: spikes/train_avg_layer_11 0
wandb: spikes/train_avg_layer_12 0
wandb: spikes/train_avg_layer_13 0
wandb: spikes/train_avg_layer_14 0
wandb: spikes/train_avg_layer_15 0
wandb:                       +11 ...
wandb: 
wandb: üöÄ View run vague-sweep-25 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/qd7j07rt
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_074210-qd7j07rt/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 300, in apply
    def apply(self, *args):
    
Exception

wandb: Agent Starting Run: jla4bfz1 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.0009351818110567044
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 256
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_074510-jla4bfz1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-sweep-26
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/jla4bfz1
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 199 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 257    | train
--------------------------------------------------------
199 K     Trainable params
0         Non-trainable params
199 K     Total params
0.799     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: uploading summary, console lines 44-44
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà
wandb:                       lr ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÅ‚ñÇ‚ñÑ‚ñá‚ñà
wandb: spikes/train_avg_layer_1 ‚ñÅ‚ñÇ‚ñÑ‚ñá‚ñà
wandb: spikes/train_avg_layer_2 ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñà
wandb: spikes/train_avg_layer_3 ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñà
wandb:               train_loss ‚ñà‚ñà‚ñÉ‚ñÇ‚ñÅ
wandb:      trainer/global_step ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà
wandb:                 val_loss ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    epoch 4
wandb:                       lr 0.00091
wandb: spikes/train_avg_layer_0 0.02301
wandb: spikes/train_avg_layer_1 0.03823
wandb: spikes/train_avg_layer_2 0.03387
wandb: spikes/train_avg_layer_3 0.0178
wandb:               train_loss 0.19246
wandb:      trainer/global_step 34
wandb:                 val_loss 0.18887
wandb: 
wandb: üöÄ View run hopeful-sweep-26 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/jla4bfz1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_074510-jla4bfz1/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 300, in apply
    def apply(self, *args):
    
Exception

wandb: Agent Starting Run: 6o8mddlb with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -2
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.03495112619916358
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 256
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run 6o8mddlb
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_075508-6o8mddlb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-27
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/6o8mddlb
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 199 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 257    | train
--------------------------------------------------------
199 K     Trainable params
0         Non-trainable params
199 K     Total params
0.799     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:   grad_mean/lif_layers.0 ‚ñà‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÉ
wandb:   grad_mean/lif_layers.1 ‚ñà‚ñÅ‚ñÑ‚ñÉ‚ñÅ‚ñÉ
wandb:   grad_mean/lif_layers.2 ‚ñá‚ñá‚ñà‚ñÅ‚ñÅ‚ñÇ
wandb:   grad_mean/lif_layers.3 ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:   grad_mean/output_layer ‚ñà‚ñÅ‚ñÜ‚ñÑ‚ñÉ‚ñÖ
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÖ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: spikes/train_avg_layer_1 ‚ñà‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: spikes/train_avg_layer_2 ‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 49
wandb:   grad_mean/lif_layers.0 0.00791
wandb:   grad_mean/lif_layers.1 0.00015
wandb:   grad_mean/lif_layers.2 2e-05
wandb:   grad_mean/lif_layers.3 2e-05
wandb:   grad_mean/output_layer 0.0049
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01875
wandb: spikes/train_avg_layer_1 0.0171
wandb: spikes/train_avg_layer_2 0.0359
wandb:                       +4 ...
wandb: 
wandb: üöÄ View run brisk-sweep-27 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/6o8mddlb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_075508-6o8mddlb/logs
wandb: Agent Starting Run: i6o6dv7t with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -2
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.0008510638573071721
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_092454-i6o6dv7t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-28
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/i6o6dv7t
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 52     | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
71        Trainable params
0         Non-trainable params
71        Total params
0.000     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1618, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 1136, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Exception: 
wandb: updating run metadata
wandb: uploading console lines 44-64
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:   grad_mean/lif_layers.0 ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÇ
wandb:   grad_mean/lif_layers.1 ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   grad_mean/output_layer ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: spikes/train_avg_layer_1 ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:               train_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:      trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                 val_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    epoch 49
wandb:   grad_mean/lif_layers.0 0.00447
wandb:   grad_mean/lif_layers.1 0.00108
wandb:   grad_mean/output_layer 0.02603
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01857
wandb: spikes/train_avg_layer_1 0.0031
wandb:               train_loss 0.16324
wandb:      trainer/global_step 349
wandb:                 val_loss 0.16245
wandb: 
wandb: üöÄ View run fearless-sweep-28 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/i6o6dv7t
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_092454-i6o6dv7t/logs
wandb: Agent Starting Run: bhxjdna0 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.006955355546072288
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run bhxjdna0
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_092606-bhxjdna0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-29
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/bhxjdna0
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 248 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
248 K     Trainable params
0         Non-trainable params
248 K     Total params
0.995     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
wandb: 
wandb: Run history:
wandb:                   epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:  grad_mean/lif_layers.0 ‚ñÖ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  grad_mean/lif_layers.1 ‚ñÖ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: grad_mean/lif_layers.10 ‚ñà‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: grad_mean/lif_layers.11 ‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: grad_mean/lif_layers.12 ‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: grad_mean/lif_layers.13 ‚ñà‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: grad_mean/lif_layers.14 ‚ñà‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: grad_mean/lif_layers.15 ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  grad_mean/lif_layers.2 ‚ñÑ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                     +28 ...
wandb: 
wandb: Run summary:
wandb:                   epoch 49
wandb:  grad_mean/lif_layers.0 840.22314
wandb:  grad_mean/lif_layers.1 158.37947
wandb: grad_mean/lif_layers.10 6e-05
wandb: grad_mean/lif_layers.11 7e-05
wandb: grad_mean/lif_layers.12 5e-05
wandb: grad_mean/lif_layers.13 3e-05
wandb: grad_mean/lif_layers.14 1e-05
wandb: grad_mean/lif_layers.15 1e-05
wandb:  grad_mean/lif_layers.2 96.52552
wandb:                     +28 ...
wandb: 
wandb: üöÄ View run amber-sweep-29 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/bhxjdna0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_092606-bhxjdna0/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ngeiez2o with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -2
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 3.89701036931387e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 6
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run ngeiez2o
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_124012-ngeiez2o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-30
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/ngeiez2o
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 83.6 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
83.7 K    Trainable params
0         Non-trainable params
83.7 K    Total params
0.335     Total estimated model params size (MB)
20        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: uploading summary, console lines 44-44
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ
wandb:                       lr ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÅ
wandb: spikes/train_avg_layer_1 ‚ñÅ
wandb: spikes/train_avg_layer_2 ‚ñÅ
wandb: spikes/train_avg_layer_3 ‚ñÅ
wandb: spikes/train_avg_layer_4 ‚ñÅ
wandb: spikes/train_avg_layer_5 ‚ñÅ
wandb:               train_loss ‚ñÅ
wandb:      trainer/global_step ‚ñÅ‚ñÅ
wandb:                       +1 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 0
wandb:                       lr 0.0
wandb: spikes/train_avg_layer_0 0.01848
wandb: spikes/train_avg_layer_1 0.00131
wandb: spikes/train_avg_layer_2 6e-05
wandb: spikes/train_avg_layer_3 0
wandb: spikes/train_avg_layer_4 0
wandb: spikes/train_avg_layer_5 0
wandb:               train_loss 0.23082
wandb:      trainer/global_step 6
wandb:                       +1 ...
wandb: 
wandb: üöÄ View run dutiful-sweep-30 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/ngeiez2o
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_124012-ngeiez2o/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 300, in apply
    def apply(self, *args):
    
Exception

wandb: Agent Starting Run: bvmlyq4m with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.00034622707032075226
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_124207-bvmlyq4m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-31
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/bvmlyq4m
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 17.5 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
17.7 K    Trainable params
0         Non-trainable params
17.7 K    Total params
0.071     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: uploading console lines 44-44
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñà‚ñà
wandb:                       lr ‚ñà‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÅ‚ñà
wandb: spikes/train_avg_layer_1 ‚ñÅ‚ñà
wandb:               train_loss ‚ñà‚ñÅ
wandb:      trainer/global_step ‚ñÅ‚ñÅ‚ñà‚ñà
wandb:                 val_loss ‚ñà‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    epoch 1
wandb:                       lr 0.00034
wandb: spikes/train_avg_layer_0 0.02217
wandb: spikes/train_avg_layer_1 0.00242
wandb:               train_loss 0.27468
wandb:      trainer/global_step 13
wandb:                 val_loss 0.26878
wandb: 
wandb: üöÄ View run soft-sweep-31 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/bvmlyq4m
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_124207-bvmlyq4m/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 300, in apply
    def apply(self, *args):
    
Exception

wandb: Agent Starting Run: b88ivao6 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -4
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.006257924701919629
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 6
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_124308-b88ivao6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-32
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/b88ivao6
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 21.3 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
21.4 K    Trainable params
0         Non-trainable params
21.4 K    Total params
0.086     Total estimated model params size (MB)
20        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
wandb: updating run metadata
wandb: uploading console lines 46-47
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:   grad_mean/lif_layers.0 ‚ñÖ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ
wandb:   grad_mean/lif_layers.1 ‚ñÖ‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÅ
wandb:   grad_mean/lif_layers.2 ‚ñÖ‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÅ
wandb:   grad_mean/lif_layers.3 ‚ñÜ‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÅ
wandb:   grad_mean/lif_layers.4 ‚ñà‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÅ
wandb:   grad_mean/lif_layers.5 ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:   grad_mean/output_layer ‚ñà‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÑ
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:                       +8 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 49
wandb:   grad_mean/lif_layers.0 0.43497
wandb:   grad_mean/lif_layers.1 0.0154
wandb:   grad_mean/lif_layers.2 0.0021
wandb:   grad_mean/lif_layers.3 0.00038
wandb:   grad_mean/lif_layers.4 7e-05
wandb:   grad_mean/lif_layers.5 8e-05
wandb:   grad_mean/output_layer 0.00381
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.0101
wandb:                       +8 ...
wandb: 
wandb: üöÄ View run peachy-sweep-32 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/b88ivao6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_124308-b88ivao6/logs
wandb: Agent Starting Run: 23q302jm with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -2
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.04849048122730497
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_131342-23q302jm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-33
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/23q302jm
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 248 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
248 K     Trainable params
0         Non-trainable params
248 K     Total params
0.995     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                     epoch ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà
wandb:                        lr ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÅ
wandb:  spikes/train_avg_layer_0 ‚ñÉ‚ñÅ‚ñÇ‚ñá‚ñà
wandb:  spikes/train_avg_layer_1 ‚ñÉ‚ñÅ‚ñÉ‚ñà‚ñà
wandb: spikes/train_avg_layer_10 ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñà
wandb: spikes/train_avg_layer_11 ‚ñÅ‚ñÖ‚ñÉ‚ñà‚ñá
wandb: spikes/train_avg_layer_12 ‚ñÅ‚ñà‚ñÇ‚ñÜ‚ñà
wandb: spikes/train_avg_layer_13 ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñà
wandb: spikes/train_avg_layer_14 ‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñà
wandb: spikes/train_avg_layer_15 ‚ñÇ‚ñÅ‚ñÑ‚ñá‚ñà
wandb:                       +11 ...
wandb: 
wandb: Run summary:
wandb:                     epoch 4
wandb:                        lr 0.0473
wandb:  spikes/train_avg_layer_0 0.05832
wandb:  spikes/train_avg_layer_1 0.05092
wandb: spikes/train_avg_layer_10 0.0763
wandb: spikes/train_avg_layer_11 0.01625
wandb: spikes/train_avg_layer_12 0.0136
wandb: spikes/train_avg_layer_13 0.03317
wandb: spikes/train_avg_layer_14 0.01961
wandb: spikes/train_avg_layer_15 0.00523
wandb:                       +11 ...
wandb: 
wandb: üöÄ View run cool-sweep-33 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/23q302jm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_131342-23q302jm/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 300, in apply
    def apply(self, *args):
    
Exception

wandb: Agent Starting Run: 81b6nry4 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 1.4829629418066974e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 256
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_133408-81b6nry4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-34
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/81b6nry4
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 199 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 257    | train
--------------------------------------------------------
199 K     Trainable params
0         Non-trainable params
199 K     Total params
0.799     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: uploading summary, console lines 42-42
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ
wandb:                       lr ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÅ
wandb: spikes/train_avg_layer_1 ‚ñÅ
wandb: spikes/train_avg_layer_2 ‚ñÅ
wandb: spikes/train_avg_layer_3 ‚ñÅ
wandb:               train_loss ‚ñÅ
wandb:      trainer/global_step ‚ñÅ‚ñÅ
wandb:                 val_loss ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    epoch 0
wandb:                       lr 0.0
wandb: spikes/train_avg_layer_0 0.02177
wandb: spikes/train_avg_layer_1 0.00177
wandb: spikes/train_avg_layer_2 8e-05
wandb: spikes/train_avg_layer_3 0
wandb:               train_loss 0.22019
wandb:      trainer/global_step 6
wandb:                 val_loss 0.21814
wandb: 
wandb: üöÄ View run rural-sweep-34 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/81b6nry4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_133408-81b6nry4/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 131, in closure
    step_output = self._step_fn()
                  ^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/RSNN.py", line 634, in training_step
    outputs = self(X_batch)
              ^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/RSNN.py", line 574, in forward
    lif_out, _states = lif(input_seq)  # lif_out: (T, B, F_hidden)
                       ^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/src_cell.py", line 171, in forward
    h_prev = h.detach()
             ^^^^^^^^^^
Exception

wandb: Agent Starting Run: d7qpox3f with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -2
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.0012469394159536574
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run d7qpox3f
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_133708-d7qpox3f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-35
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/d7qpox3f
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 29.6 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
29.7 K    Trainable params
0         Non-trainable params
29.7 K    Total params
0.119     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: uploading console lines 44-44
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ
wandb:                       lr ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÅ
wandb: spikes/train_avg_layer_1 ‚ñÅ
wandb: spikes/train_avg_layer_2 ‚ñÅ
wandb: spikes/train_avg_layer_3 ‚ñÅ
wandb: spikes/train_avg_layer_4 ‚ñÅ
wandb: spikes/train_avg_layer_5 ‚ñÅ
wandb: spikes/train_avg_layer_6 ‚ñÅ
wandb: spikes/train_avg_layer_7 ‚ñÅ
wandb:                       +3 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 0
wandb:                       lr 0.00125
wandb: spikes/train_avg_layer_0 0.02535
wandb: spikes/train_avg_layer_1 0.00209
wandb: spikes/train_avg_layer_2 0.00015
wandb: spikes/train_avg_layer_3 0
wandb: spikes/train_avg_layer_4 0
wandb: spikes/train_avg_layer_5 0
wandb: spikes/train_avg_layer_6 0
wandb: spikes/train_avg_layer_7 0
wandb:                       +3 ...
wandb: 
wandb: üöÄ View run celestial-sweep-35 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/d7qpox3f
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_133708-d7qpox3f/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 300, in apply
    def apply(self, *args):
    
Exception

wandb: Agent Starting Run: umvpselz with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -6
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.009882003197220088
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 128
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run umvpselz
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_133810-umvpselz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-sweep-36
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/umvpselz
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 17.5 K | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 129    | train
--------------------------------------------------------
17.7 K    Trainable params
0         Non-trainable params
17.7 K    Total params
0.071     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:   grad_mean/lif_layers.0 ‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb:   grad_mean/lif_layers.1 ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   grad_mean/output_layer ‚ñá‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÅ
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÖ‚ñà‚ñà‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: spikes/train_avg_layer_1 ‚ñÅ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:               train_loss ‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:      trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                 val_loss ‚ñà‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    epoch 49
wandb:   grad_mean/lif_layers.0 0.00085
wandb:   grad_mean/lif_layers.1 7e-05
wandb:   grad_mean/output_layer 0.00156
wandb:                       lr 0
wandb: spikes/train_avg_layer_0 0.01164
wandb: spikes/train_avg_layer_1 0.05036
wandb:               train_loss 0.14569
wandb:      trainer/global_step 349
wandb:                 val_loss 0.14476
wandb: 
wandb: üöÄ View run breezy-sweep-36 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/umvpselz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_133810-umvpselz/logs
wandb: Agent Starting Run: r6kza1id with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -2
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 7.884792995861402e-06
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 16
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_135521-r6kza1id
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-37
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/r6kza1id
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 400    | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 17     | train
--------------------------------------------------------
431       Trainable params
0         Non-trainable params
431       Total params
0.002     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: uploading console lines 44-44
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:   grad_mean/lif_layers.0 ‚ñÅ
wandb:   grad_mean/lif_layers.1 ‚ñÅ
wandb:   grad_mean/output_layer ‚ñÅ
wandb:                       lr ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ
wandb: spikes/train_avg_layer_1 ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà
wandb:               train_loss ‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:      trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                 val_loss ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    epoch 14
wandb:   grad_mean/lif_layers.0 0.01773
wandb:   grad_mean/lif_layers.1 0.01005
wandb:   grad_mean/output_layer 0.31688
wandb:                       lr 1e-05
wandb: spikes/train_avg_layer_0 0.01922
wandb: spikes/train_avg_layer_1 0.00184
wandb:               train_loss 0.2604
wandb:      trainer/global_step 104
wandb:                 val_loss 0.25808
wandb: 
wandb: üöÄ View run worthy-sweep-37 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/r6kza1id
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_135521-r6kza1id/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 315, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/src_cell.py", line 61, in backward
    grad_input = grad_output.clone()
                 ^^^^^^^^^^^^^^^^^^^
Exception

wandb: Agent Starting Run: jsl2mjsp with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -4
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: True
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 3.208667149320906e-05
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 4
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run jsl2mjsp
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_135607-jsl2mjsp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-sweep-38
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/jsl2mjsp
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 92     | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
111       Trainable params
0         Non-trainable params
111       Total params
0.000     Total estimated model params size (MB)
16        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:   grad_mean/lif_layers.0 ‚ñÅ‚ñà
wandb:   grad_mean/lif_layers.1 ‚ñà‚ñÅ
wandb:   grad_mean/lif_layers.2 ‚ñà‚ñÅ
wandb:   grad_mean/lif_layers.3 ‚ñà‚ñÅ
wandb:   grad_mean/output_layer ‚ñà‚ñÅ
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb: spikes/train_avg_layer_1 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_2 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                       +4 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 24
wandb:   grad_mean/lif_layers.0 0.06845
wandb:   grad_mean/lif_layers.1 0.01468
wandb:   grad_mean/lif_layers.2 0.03505
wandb:   grad_mean/lif_layers.3 0.03756
wandb:   grad_mean/output_layer 0.71207
wandb:                       lr 2e-05
wandb: spikes/train_avg_layer_0 0.00932
wandb: spikes/train_avg_layer_1 0
wandb: spikes/train_avg_layer_2 0
wandb:                       +4 ...
wandb: 
wandb: üöÄ View run hardy-sweep-38 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/jsl2mjsp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_135607-jsl2mjsp/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 300, in apply
    def apply(self, *args):
    
Exception

wandb: Agent Starting Run: h2kvji4p with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -2
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.0002347535868884015
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 64
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: setting up run h2kvji4p
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_135709-h2kvji4p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-39
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/h2kvji4p
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 4.7 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 65     | train
--------------------------------------------------------
4.8 K     Trainable params
0         Non-trainable params
4.8 K     Total params
0.019     Total estimated model params size (MB)
12        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: updating run metadata
wandb: uploading console lines 44-44
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà
wandb:                       lr ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñà
wandb: spikes/train_avg_layer_1 ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñà
wandb:               train_loss ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ
wandb:      trainer/global_step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà
wandb:                 val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    epoch 5
wandb:                       lr 0.00023
wandb: spikes/train_avg_layer_0 0.02128
wandb: spikes/train_avg_layer_1 0.00213
wandb:               train_loss 0.22157
wandb:      trainer/global_step 41
wandb:                 val_loss 0.21833
wandb: 
wandb: üöÄ View run sage-sweep-39 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/h2kvji4p
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_135709-h2kvji4p/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 153, in run
    self.on_advance_end(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 398, in on_advance_end
    self.val_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py", line 179, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 145, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 437, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/RSNN.py", line 688, in validation_step
    outputs = self(X_batch)
              ^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/RSNN.py", line 629, in forward
    out = self.output_layer(x_seq_b).squeeze()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Exception

wandb: Agent Starting Run: qonoggi4 with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -4
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.036039027947192846
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 4
wandb: 	num_hidden_layers: 8
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: True
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_135812-qonoggi4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-40
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/qonoggi4
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 172    | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 5      | train
--------------------------------------------------------
191       Trainable params
0         Non-trainable params
191       Total params
0.001     Total estimated model params size (MB)
24        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
        self._shutdown_workers()self._shutdown_workers()

  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
        if w.is_alive(): 
  ^ ^ ^ ^ ^ ^ ^ ^^^^    ^^if w.is_alive():^^
^^
^  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
^ ^ ^ ^^ ^ 
   File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
 ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
        assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'

        assert self._parent_pid == os.getpid(), 'can only test a child process'  
                    ^  ^^ ^^ ^^ ^^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^AssertionError
^: AssertionError^can only test a child process: ^
can only test a child process^
^^
AssertionError: can only test a child process
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Exception in thread QueueFeederThread:
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 291, in _feed
    queue_sem.release()
ValueError: semaphore or lock released too many times
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1496833f56c0>
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1637, in _shutdown_workers
    if w.is_alive():
       ^^^^^^^^^^^^
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/process.py", line 160, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: can only test a child process
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
Traceback (most recent call last):
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/queues.py", line 259, in _feed
    reader_close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 178, in close
    self._close()
  File "/opt/bwhpc/common/devel/miniforge/25.3.1-py3.12/lib/python3.12/multiprocessing/connection.py", line 377, in _close
    _close(self._handle)
OSError: [Errno 9] Bad file descriptor
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà
wandb:                       lr ‚ñà‚ñà‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÅ
wandb: spikes/train_avg_layer_0 ‚ñÅ‚ñÅ‚ñÖ‚ñà‚ñà‚ñà‚ñà
wandb: spikes/train_avg_layer_1 ‚ñÅ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà
wandb: spikes/train_avg_layer_2 ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñà
wandb: spikes/train_avg_layer_3 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñà
wandb: spikes/train_avg_layer_4 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñà
wandb: spikes/train_avg_layer_5 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÇ
wandb: spikes/train_avg_layer_6 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ
wandb: spikes/train_avg_layer_7 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ
wandb:                       +3 ...
wandb: 
wandb: Run summary:
wandb:                    epoch 6
wandb:                       lr 0.03432
wandb: spikes/train_avg_layer_0 0.04116
wandb: spikes/train_avg_layer_1 0.02176
wandb: spikes/train_avg_layer_2 3e-05
wandb: spikes/train_avg_layer_3 4e-05
wandb: spikes/train_avg_layer_4 3e-05
wandb: spikes/train_avg_layer_5 0.0
wandb: spikes/train_avg_layer_6 0.0
wandb: spikes/train_avg_layer_7 0
wandb:                       +3 ...
wandb: 
wandb: üöÄ View run exalted-sweep-40 at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/qonoggi4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_135812-qonoggi4/logs
Traceback (most recent call last):
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/3_hyperparameter_search_rsnn.py", line 334, in training_run
    trainer.fit(model)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 49, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
    self.advance(data_fetcher)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 348, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 192, in run
    self._optimizer_step(batch_idx, closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 270, in _optimizer_step
    call._call_lightning_module_hook(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 177, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1366, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py", line 154, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 123, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/optimizer.py", line 82, in _use_grad
    ret = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    loss = closure()
           ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 109, in _wrap_closure
    closure_result = closure()
                     ^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 140, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 241, in backward_fn
    call._call_strategy_hook(self.trainer, "backward", loss, optimizer)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py", line 213, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py", line 73, in backward
    model.backward(tensor, *args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/core/module.py", line 1135, in backward
    loss.backward(*args, **kwargs)
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/torch/autograd/function.py", line 315, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data6/home/ka/ka_itec/ka_rh5993/SpikeSynth/surrogate/utils/src_cell.py", line 61, in backward
    grad_input = grad_output.clone()
                 ^^^^^^^^^^^^^^^^^^^
Exception

wandb: Agent Starting Run: 3jz7fvaj with config:
wandb: 	alpha: 0.9
wandb: 	batch_size: 2048
wandb: 	beta: 0
wandb: 	bh_init: -4
wandb: 	bh_max: -4
wandb: 	bntt_time_steps: 100
wandb: 	data_path: ./data/small_dataset.ds
wandb: 	detach_rec: False
wandb: 	dropout: 0
wandb: 	epochs: 50
wandb: 	layer_skip: 0
wandb: 	loss_fn: mse
wandb: 	loss_kwargs: 
wandb: 	lr: 0.0427443160192248
wandb: 	neuron_type: SRC
wandb: 	num_hidden: 256
wandb: 	num_hidden_layers: 16
wandb: 	optimizer_class: AdamW
wandb: 	r: 2
wandb: 	relu_bypass: False
wandb: 	rho: 3
wandb: 	rs: -7
wandb: 	scheduler_class: cosine
wandb: 	scheduler_kwargs: 
wandb: 	surrogate_gradient: atan
wandb: 	temporal_skip: -1
wandb: 	use_bntt: False
wandb: 	use_layernorm: False
wandb: 	z: 0
wandb: 	zdep_s: 0
wandb: 	zhyp_s: 0.9
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /scratch/ka_rh5993/wandb_logs/wandb/run-20251128_135909-3jz7fvaj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-41
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep
wandb: üßπ View sweep at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/sweeps/byr8b0jv
wandb: üöÄ View run at https://wandb.ai/lupos/SpikeSynth-Surrogate-Sweep/runs/3jz7fvaj
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

  | Name             | Type       | Params | Mode 
--------------------------------------------------------
0 | norm             | LayerNorm  | 14     | train
1 | lif_layers       | ModuleList | 988 K  | train
2 | leaky_linears    | ModuleList | 0      | train
3 | temp_skip_projs  | ModuleList | 0      | train
4 | layer_skip_projs | ModuleList | 0      | train
5 | layer_bntt       | ModuleList | 0      | train
6 | layer_norms      | ModuleList | 0      | train
7 | output_layer     | Linear     | 257    | train
--------------------------------------------------------
989 K     Trainable params
0         Non-trainable params
989 K     Total params
3.957     Total estimated model params size (MB)
40        Modules in train mode
0         Modules in eval mode
wandb: WARNING Config item 'num_hidden_layers' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'num_hidden' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'beta' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'optimizer_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'lr' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'batch_size' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'dropout' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'temporal_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'layer_skip' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_bntt' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_class' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'use_layernorm' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'neuron_type' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'scheduler_kwargs' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'bntt_time_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_fn' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'loss_kwargs' was locked by 'sweep' (ignored update).
SLURM auto-requeueing enabled. Setting signal handlers.
/home/ka/ka_itec/ka_rh5993/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
wandb: Network error (SSLError), entering retry loop.
slurmstepd: error: *** JOB 2331893 ON uc3n057 CANCELLED AT 2025-11-28T16:18:33 DUE TO TIME LIMIT ***
